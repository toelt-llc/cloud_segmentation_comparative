{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!chmod -R 777 ../\n",
    "\n",
    "# conda install -c conda-forge cudatoolkit=11.8.0\n",
    "# python3 -m pip install nvidia-cudnn-cu11==8.6.0.163 tensorflow==2.13.*\n",
    "# mkdir -p $CONDA_PREFIX/etc/conda/activate.d\n",
    "# echo 'CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n",
    "# echo 'export LD_LIBRARY_PATH=$CONDA_PREFIX/lib/:$CUDNN_PATH/lib:$LD_LIBRARY_PATH' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n",
    "# source $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n",
    "# # Verify install:\n",
    "# python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n",
    "\n",
    "# conda install ipykernel\n",
    "\n",
    "# !pip install pandas seaborn numpy matplotlib scikit-learn tifffile scikit-image tqdm opencv-python rasterio ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# System and path operations\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# Imported local modules\n",
    "from src.config import *\n",
    "from src.utils import *\n",
    "from src.models_arch import *\n",
    "from src.models_utils import *\n",
    "\n",
    "# Basic python data handling and visualization libraries\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Libraries for image processing\n",
    "from PIL import Image\n",
    "import skimage\n",
    "from skimage.io import imread, imsave\n",
    "import tifffile as tiff\n",
    "\n",
    "# Libraries for model evaluation and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, jaccard_score\n",
    "\n",
    "\n",
    "# TensorFlow and Keras libraries for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import (ModelCheckpoint, TensorBoard, ReduceLROnPlateau, \n",
    "                             CSVLogger, EarlyStopping)\n",
    "from keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting GPU's number\n",
    "physical_devices=tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.config.set_visible_devices(physical_devices[:],'GPU')\n",
    "tf.config.set_visible_devices(physical_devices[2],'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "#   try:\n",
    "#     tf.config.set_logical_device_configuration(\n",
    "#         gpus[0],\n",
    "#         [tf.config.LogicalDeviceConfiguration(memory_limit=10240)])\n",
    "#     logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Virtual devices must be set before GPUs have been initialized\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "# !rm -rf ../logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CloudXNet_SPARCS_epochs500_batch16'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name = 'UNet'\n",
    "# model_name = 'UNetPlusPlus'\n",
    "model_name = 'CloudXNet'\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "train_with_RGB = False\n",
    "\n",
    "# learning rate\n",
    "# LR = 0.0005\n",
    "# LR = 0.0001\n",
    "LR = 1e-4\n",
    "\n",
    "NUM_EPOCHS=500\n",
    "\n",
    "model_name = f'{model_name}_SPARCS_epochs{NUM_EPOCHS}_batch{batch_size}'\n",
    "\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 07:49:10.066612: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 07:49:10.577755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22311 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c1:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=LR)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=LR)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=LR)\n",
    "\n",
    "metrics = ['binary_crossentropy', jaccard_coef_loss, jaccard_coef, jaccard_coef_thresholded, 'accuracy', tf.keras.metrics.AUC()]\n",
    "metrics = [jacc_coef, tf.keras.metrics.AUC()]\n",
    "\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "loss = jacc_coef\n",
    "# loss = tf.keras.losses.BinaryFocalCrossentropy()\n",
    "# loss = bce_dice_loss\n",
    "\n",
    "model = create_model(model_name='CXNet', IMG_HEIGHT=256 , IMG_WIDTH=256, IMG_CHANNELS=3)\n",
    "# model = create_model(model_name='unet')\n",
    "# model = create_model(model_name='rs_net')\n",
    "# model = create_model(model_name='cloud_net')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gen 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_names = sorted(os.listdir(Path(sparcs_train_dir, \"images_p/\")))\n",
    "valid_image_names = sorted(os.listdir(Path(sparcs_valid_dir, \"images_p/\")))\n",
    "steps_per_epoch = math.ceil(len(train_image_names) / batch_size)\n",
    "validation_steps = math.ceil(len(valid_image_names) / batch_size)\n",
    "steps_per_epoch, validation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_names = sorted(os.listdir(Path(sparcs_test_dir, \"images_p/\")))\n",
    "total_test_samples = math.ceil(len(test_image_names) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_SPARCS_generator('train', batch_size=batch_size, shuffle=True, only_rgb=train_with_RGB)\n",
    "valid = get_SPARCS_generator('valid', batch_size=batch_size, only_rgb=train_with_RGB)\n",
    "test = get_SPARCS_generator('test', batch_size=batch_size, only_rgb=train_with_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_SPARCS('test', only_rgb=train_with_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Step 1: Create and initialize the generator (example for the training set)\n",
    "# train_generator = get_SPARCS_generator(sets='train', batch_size=32, shuffle=True)\n",
    "\n",
    "# # Step 2: Get a batch of data from the generator\n",
    "# batch_images, batch_masks = next(train_generator)\n",
    "\n",
    "# # Step 3: Print the shapes of the images and masks in the batch\n",
    "# print(\"Batch of Images shape:\", batch_images.shape)\n",
    "# print(\"Batch of Masks shape:\", batch_masks.shape)\n",
    "\n",
    "# # Step 4: Display some example images along with their masks\n",
    "# num_examples_to_display = 4\n",
    "# for i in range(num_examples_to_display):\n",
    "#     plt.subplot(2, num_examples_to_display, i + 1)\n",
    "#     plt.imshow(batch_images[i])\n",
    "#     plt.title(\"Image\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(2, num_examples_to_display, i + num_examples_to_display + 1)\n",
    "#     plt.imshow(batch_masks[i][:, :, 0], cmap=\"gray\")\n",
    "#     plt.title(\"Mask\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_image_size = len(sorted(os.listdir(Path(sparcs_train_dir, \"images_p/\"))))\n",
    "valid_image_size = len(sorted(os.listdir(Path(sparcs_valid_dir, \"images_p/\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.from_tensor_slices(get_SPARCS(only_rgb=train_with_RGB))\n",
    "valid = tf.data.Dataset.from_tensor_slices(get_SPARCS('valid', only_rgb=train_with_RGB))\n",
    "test = tf.data.Dataset.from_tensor_slices(get_SPARCS('test', only_rgb=train_with_RGB))\n",
    "\n",
    "train = train.shuffle(train_image_size, seed=seed_value).batch(batch_size)\n",
    "valid = valid.shuffle(valid_image_size, seed=seed_value).batch(batch_size)\n",
    "test = test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test, y_test = get_SPARCS('test', only_rgb=train_with_RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_SPARCS('train', only_rgb=train_with_RGB)\n",
    "X_valid, y_valid = get_SPARCS('valid', only_rgb=train_with_RGB)\n",
    "X_test, y_test = get_SPARCS('test', only_rgb=train_with_RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Include the epoch in the file name (uses `str.format`)\n",
    "ckpt_path = str(checkpoint_path / f\"{model_name}/\" / \"cp-{epoch:04d}.ckpt\")\n",
    "ckpt_dir = os.path.dirname(ckpt_path)\n",
    "\n",
    "# Checkpoint callbacks definition\n",
    "\n",
    "# Save the checkpoint only if the validation AUC is the best\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=ckpt_path, \n",
    "#     verbose=1, \n",
    "#     save_best_only=True,\n",
    "#     monitor='val_auc',\n",
    "#     save_weights_only=True,\n",
    "#     save_freq='epoch')\n",
    "\n",
    "# Save the checkpoint only if the validation loss is minimum\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=ckpt_path, \n",
    "    verbose=1, \n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_callback = TimeHistory()\n",
    "\n",
    "log_dir = project_root_path / \"logs/fit/\" / model_name / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "callbacks_list = [cp_callback, time_callback, tensorboard_callback]\n",
    "callbacks_list = [time_callback, tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the weights using the `checkpoint_path` format\n",
    "model.save_weights(ckpt_path.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from checkpoint\n",
    "\n",
    "# # Create a new model instance\n",
    "# model = create_model(model_name='unet_plus_plus')\n",
    "\n",
    "# # Try to load the latest checkpoint\n",
    "# try:\n",
    "#     latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "#     latest_epoch = int(latest.split(\"-\")[-1].split(\".\")[0])  # extract epoch number from the filename\n",
    "#     model.load_weights(latest)\n",
    "#     print(f\"Loaded weights from {latest}\")\n",
    "# except Exception as e:\n",
    "#     print(\"No checkpoint was found, starting training from scratch.\")\n",
    "#     latest_epoch = 0  # if no checkpoint was found, start from epoch 0\n",
    "\n",
    "# # Now compile your model with optimizer and loss\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')  \n",
    "\n",
    "# # Continue training the model\n",
    "# # Here, epochs should be the final epoch number you want to reach and initial_epoch should be set to the epoch number where training last left off\n",
    "# model.fit(train_dataset, epochs=total_epochs, initial_epoch=latest_epoch, callbacks=callbacks_list, validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = compile_model(model, optimizer=optimizer, metrics=metrics, loss=loss)\n",
    "# model = compile_model(model, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 70ms/step - loss: 0.6482 - val_loss: 0.5185\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.6178 - val_loss: 0.4556\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.5224 - val_loss: 0.3962\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4740 - val_loss: 0.3584\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.4166 - val_loss: 0.3594\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.4050 - val_loss: 0.3123\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3820 - val_loss: 0.3228\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.3535 - val_loss: 0.2995\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3367 - val_loss: 0.3304\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.3401 - val_loss: 0.2950\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3138 - val_loss: 0.2893\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2984 - val_loss: 0.3066\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2945 - val_loss: 0.2859\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3243 - val_loss: 0.3287\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3153 - val_loss: 0.2770\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2962 - val_loss: 0.3019\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2932 - val_loss: 0.2654\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.2887 - val_loss: 0.2799\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2778 - val_loss: 0.2636\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.2713 - val_loss: 0.2799\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2686 - val_loss: 0.2443\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2633 - val_loss: 0.2600\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2527 - val_loss: 0.2602\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.2617 - val_loss: 0.2683\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2406 - val_loss: 0.2482\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2365 - val_loss: 0.2900\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2425 - val_loss: 0.2972\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2331 - val_loss: 0.2337\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2244 - val_loss: 0.3584\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.2084 - val_loss: 0.3512\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1999 - val_loss: 0.3398\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2036 - val_loss: 0.2869\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1950 - val_loss: 0.3316\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1863 - val_loss: 0.3484\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1891 - val_loss: 0.3844\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1805 - val_loss: 0.4114\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1771 - val_loss: 0.5529\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 67ms/step - loss: 0.1768 - val_loss: 0.3513\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1678 - val_loss: 0.4399\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1686 - val_loss: 0.3402\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1687 - val_loss: 0.3165\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1669 - val_loss: 0.3964\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2377 - val_loss: 0.4358\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1875 - val_loss: 0.3284\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1792 - val_loss: 0.2660\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1607 - val_loss: 0.3548\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1612 - val_loss: 0.3330\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1618 - val_loss: 0.3465\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1669 - val_loss: 0.2825\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1490 - val_loss: 0.4613\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1489 - val_loss: 0.2942\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1447 - val_loss: 0.3177\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1404 - val_loss: 0.3770\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1400 - val_loss: 0.3523\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1458 - val_loss: 0.3285\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1366 - val_loss: 0.3887\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1401 - val_loss: 0.3076\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1353 - val_loss: 0.2729\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1329 - val_loss: 0.3431\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1322 - val_loss: 0.4166\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1293 - val_loss: 0.3102\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1421 - val_loss: 0.2425\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1437 - val_loss: 0.2391\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1325 - val_loss: 0.3332\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1286 - val_loss: 0.3232\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1231 - val_loss: 0.3000\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1240 - val_loss: 0.2975\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1288 - val_loss: 0.3142\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1220 - val_loss: 0.3376\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1201 - val_loss: 0.3297\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1206 - val_loss: 0.3375\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1159 - val_loss: 0.3042\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1216 - val_loss: 0.2208\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1194 - val_loss: 0.2566\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1144 - val_loss: 0.2648\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1222 - val_loss: 0.2608\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1130 - val_loss: 0.3124\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1162 - val_loss: 0.2988\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1118 - val_loss: 0.2272\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1104 - val_loss: 0.2639\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1128 - val_loss: 0.2320\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1138 - val_loss: 0.3531\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1349 - val_loss: 0.3181\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1152 - val_loss: 0.2880\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1067 - val_loss: 0.2781\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1036 - val_loss: 0.3168\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1100 - val_loss: 0.2408\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1046 - val_loss: 0.2286\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1026 - val_loss: 0.2408\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1052 - val_loss: 0.2331\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1054 - val_loss: 0.2992\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1007 - val_loss: 0.2615\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1004 - val_loss: 0.2477\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1026 - val_loss: 0.2719\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1012 - val_loss: 0.2617\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0979 - val_loss: 0.2409\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0983 - val_loss: 0.2803\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0966 - val_loss: 0.2288\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0967 - val_loss: 0.2348\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1066 - val_loss: 0.2352\n",
      "8/8 [==============================] - 1s 27ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 4s 71ms/step - loss: 0.6555 - val_loss: 0.5868\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5979 - val_loss: 0.5667\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5231 - val_loss: 0.4830\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4780 - val_loss: 0.5117\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4764 - val_loss: 0.4717\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.4238 - val_loss: 0.4803\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4080 - val_loss: 0.4459\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3907 - val_loss: 0.4679\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3736 - val_loss: 0.3796\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3401 - val_loss: 0.4741\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3224 - val_loss: 0.4474\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3339 - val_loss: 0.5256\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3068 - val_loss: 0.5476\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3277 - val_loss: 0.4119\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3178 - val_loss: 0.4204\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3198 - val_loss: 0.4673\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3033 - val_loss: 0.4152\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2786 - val_loss: 0.5729\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2883 - val_loss: 0.4711\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2776 - val_loss: 0.5376\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2634 - val_loss: 0.4011\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2780 - val_loss: 0.4533\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2521 - val_loss: 0.4382\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2483 - val_loss: 0.4638\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2678 - val_loss: 0.3908\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2550 - val_loss: 0.4983\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2419 - val_loss: 0.4863\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2265 - val_loss: 0.4933\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2195 - val_loss: 0.4093\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2149 - val_loss: 0.3877\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2168 - val_loss: 0.4022\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2146 - val_loss: 0.4376\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2277 - val_loss: 0.4560\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2000 - val_loss: 0.5554\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2031 - val_loss: 0.5420\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1920 - val_loss: 0.4615\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1975 - val_loss: 0.4203\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1775 - val_loss: 0.6284\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1922 - val_loss: 0.4212\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1820 - val_loss: 0.4525\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1712 - val_loss: 0.4522\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1835 - val_loss: 0.3077\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1890 - val_loss: 0.4370\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1678 - val_loss: 0.5744\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1597 - val_loss: 0.5348\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1540 - val_loss: 0.4623\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1544 - val_loss: 0.6020\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1691 - val_loss: 0.4389\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1528 - val_loss: 0.5754\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1568 - val_loss: 0.5002\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1515 - val_loss: 0.3718\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1444 - val_loss: 0.5714\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1503 - val_loss: 0.5234\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1430 - val_loss: 0.6759\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1486 - val_loss: 0.5503\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1410 - val_loss: 0.4947\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1356 - val_loss: 0.4538\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1374 - val_loss: 0.3926\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1390 - val_loss: 0.5186\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1367 - val_loss: 0.3768\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1328 - val_loss: 0.3737\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1354 - val_loss: 0.5368\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1326 - val_loss: 0.3883\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1319 - val_loss: 0.4281\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1354 - val_loss: 0.5266\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1274 - val_loss: 0.4162\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1227 - val_loss: 0.5066\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1250 - val_loss: 0.4205\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1260 - val_loss: 0.4243\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1253 - val_loss: 0.4554\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1202 - val_loss: 0.3530\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1203 - val_loss: 0.4097\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1174 - val_loss: 0.4050\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1225 - val_loss: 0.5641\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1150 - val_loss: 0.4255\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1186 - val_loss: 0.3886\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1147 - val_loss: 0.3656\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1126 - val_loss: 0.4755\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1173 - val_loss: 0.4392\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1117 - val_loss: 0.4581\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1126 - val_loss: 0.4310\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1105 - val_loss: 0.3799\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1091 - val_loss: 0.5113\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1081 - val_loss: 0.3796\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1094 - val_loss: 0.3740\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1083 - val_loss: 0.3918\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1073 - val_loss: 0.3715\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1060 - val_loss: 0.3744\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1172 - val_loss: 0.5088\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1096 - val_loss: 0.3320\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1046 - val_loss: 0.5836\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1007 - val_loss: 0.3520\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1003 - val_loss: 0.4243\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1004 - val_loss: 0.3802\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1089 - val_loss: 0.5060\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1088 - val_loss: 0.4358\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0989 - val_loss: 0.3738\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1006 - val_loss: 0.3692\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0991 - val_loss: 0.3967\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0953 - val_loss: 0.4209\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0974 - val_loss: 0.3940\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1002 - val_loss: 0.3982\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0960 - val_loss: 0.3287\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0972 - val_loss: 0.3330\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0955 - val_loss: 0.3636\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0922 - val_loss: 0.4052\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0913 - val_loss: 0.4607\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1018 - val_loss: 0.2724\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0916 - val_loss: 0.3091\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0886 - val_loss: 0.3106\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0893 - val_loss: 0.3157\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0910 - val_loss: 0.2546\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0903 - val_loss: 0.3333\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0855 - val_loss: 0.3822\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0858 - val_loss: 0.3490\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0848 - val_loss: 0.3461\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0838 - val_loss: 0.3895\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0852 - val_loss: 0.3247\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0853 - val_loss: 0.3522\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0832 - val_loss: 0.3071\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0846 - val_loss: 0.4340\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0833 - val_loss: 0.4092\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0820 - val_loss: 0.4138\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0824 - val_loss: 0.4043\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0964 - val_loss: 0.3415\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0855 - val_loss: 0.2726\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0833 - val_loss: 0.3518\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0816 - val_loss: 0.4412\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0816 - val_loss: 0.3458\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0810 - val_loss: 0.3586\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0858 - val_loss: 0.3387\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0830 - val_loss: 0.3500\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0758 - val_loss: 0.3944\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0757 - val_loss: 0.3126\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0761 - val_loss: 0.4897\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0761 - val_loss: 0.3620\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0765 - val_loss: 0.3822\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0742 - val_loss: 0.3152\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0754 - val_loss: 0.3174\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0769 - val_loss: 0.3649\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0740 - val_loss: 0.3398\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0748 - val_loss: 0.3172\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0732 - val_loss: 0.3000\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0726 - val_loss: 0.3515\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0705 - val_loss: 0.3445\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0721 - val_loss: 0.3608\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0716 - val_loss: 0.2983\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0707 - val_loss: 0.3092\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0704 - val_loss: 0.3388\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0712 - val_loss: 0.3522\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0711 - val_loss: 0.3689\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0703 - val_loss: 0.3609\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0680 - val_loss: 0.3718\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0692 - val_loss: 0.3787\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0743 - val_loss: 0.3410\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0911 - val_loss: 0.2964\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1379 - val_loss: 0.3403\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0954 - val_loss: 0.3707\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0797 - val_loss: 0.3110\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0707 - val_loss: 0.2858\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0685 - val_loss: 0.2938\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0680 - val_loss: 0.3181\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0681 - val_loss: 0.3635\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0657 - val_loss: 0.3301\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0649 - val_loss: 0.3055\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0646 - val_loss: 0.3005\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0647 - val_loss: 0.3096\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0678 - val_loss: 0.3385\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0652 - val_loss: 0.3085\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0637 - val_loss: 0.3233\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0628 - val_loss: 0.2705\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0638 - val_loss: 0.3004\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0640 - val_loss: 0.2988\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0686 - val_loss: 0.3640\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0639 - val_loss: 0.3546\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0653 - val_loss: 0.2567\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0643 - val_loss: 0.2855\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0645 - val_loss: 0.3608\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0634 - val_loss: 0.3419\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0644 - val_loss: 0.2947\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0614 - val_loss: 0.3166\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0617 - val_loss: 0.2947\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0608 - val_loss: 0.3253\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0603 - val_loss: 0.3611\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0608 - val_loss: 0.3438\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0610 - val_loss: 0.3048\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0631 - val_loss: 0.3052\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0598 - val_loss: 0.2597\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0734 - val_loss: 0.3278\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0639 - val_loss: 0.2912\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0602 - val_loss: 0.2973\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0587 - val_loss: 0.3050\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0601 - val_loss: 0.3306\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0813 - val_loss: 0.2885\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0745 - val_loss: 0.3184\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0618 - val_loss: 0.2903\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0595 - val_loss: 0.3089\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0582 - val_loss: 0.2954\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0572 - val_loss: 0.3078\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0570 - val_loss: 0.2954\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 4s 68ms/step - loss: 0.6963 - val_loss: 0.5705\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.6192 - val_loss: 0.5151\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5188 - val_loss: 0.4070\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4512 - val_loss: 0.4008\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4245 - val_loss: 0.3822\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4048 - val_loss: 0.3668\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3630 - val_loss: 0.2926\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3433 - val_loss: 0.2682\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3188 - val_loss: 0.2562\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3170 - val_loss: 0.2501\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2984 - val_loss: 0.2446\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3032 - val_loss: 0.2417\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2948 - val_loss: 0.2373\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2871 - val_loss: 0.2324\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2716 - val_loss: 0.2297\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2637 - val_loss: 0.2174\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2570 - val_loss: 0.2168\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2595 - val_loss: 0.2168\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2494 - val_loss: 0.2072\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2389 - val_loss: 0.1979\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2276 - val_loss: 0.1989\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2270 - val_loss: 0.2429\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2165 - val_loss: 0.2482\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2180 - val_loss: 0.1857\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2098 - val_loss: 0.2092\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1981 - val_loss: 0.2144\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2018 - val_loss: 0.2280\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1995 - val_loss: 0.2435\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1830 - val_loss: 0.2504\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1811 - val_loss: 0.2305\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1802 - val_loss: 0.2451\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1731 - val_loss: 0.2471\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1676 - val_loss: 0.3029\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1687 - val_loss: 0.2472\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1644 - val_loss: 0.2963\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1660 - val_loss: 0.2212\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2033 - val_loss: 0.3188\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1873 - val_loss: 0.2061\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1636 - val_loss: 0.2499\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1634 - val_loss: 0.2075\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1535 - val_loss: 0.2742\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1437 - val_loss: 0.2946\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1483 - val_loss: 0.2510\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1454 - val_loss: 0.2873\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1501 - val_loss: 0.3125\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1427 - val_loss: 0.2296\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1439 - val_loss: 0.2603\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1361 - val_loss: 0.3124\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1333 - val_loss: 0.2711\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1307 - val_loss: 0.2347\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1292 - val_loss: 0.2798\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1345 - val_loss: 0.3072\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1295 - val_loss: 0.2536\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1264 - val_loss: 0.3033\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1267 - val_loss: 0.2408\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1248 - val_loss: 0.2814\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1273 - val_loss: 0.2261\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1197 - val_loss: 0.2479\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1235 - val_loss: 0.3056\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1241 - val_loss: 0.2114\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1233 - val_loss: 0.2079\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1193 - val_loss: 0.2206\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1184 - val_loss: 0.2238\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1162 - val_loss: 0.2298\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1101 - val_loss: 0.2583\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1103 - val_loss: 0.1997\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1119 - val_loss: 0.2852\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1090 - val_loss: 0.1880\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1117 - val_loss: 0.2092\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1070 - val_loss: 0.2291\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1082 - val_loss: 0.1998\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1076 - val_loss: 0.1747\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1071 - val_loss: 0.1955\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1056 - val_loss: 0.2301\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1053 - val_loss: 0.1637\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1057 - val_loss: 0.2521\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1026 - val_loss: 0.1998\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1071 - val_loss: 0.2098\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1337 - val_loss: 0.1824\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1038 - val_loss: 0.2057\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0965 - val_loss: 0.2074\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0971 - val_loss: 0.2027\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1005 - val_loss: 0.2015\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1015 - val_loss: 0.1902\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0996 - val_loss: 0.1672\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0971 - val_loss: 0.1785\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0954 - val_loss: 0.1918\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0985 - val_loss: 0.2007\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0954 - val_loss: 0.1752\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0912 - val_loss: 0.1807\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0896 - val_loss: 0.1789\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0901 - val_loss: 0.1786\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0933 - val_loss: 0.1875\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0879 - val_loss: 0.2090\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0897 - val_loss: 0.1828\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0869 - val_loss: 0.2253\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0920 - val_loss: 0.1866\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0879 - val_loss: 0.1699\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0858 - val_loss: 0.1986\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0872 - val_loss: 0.2060\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0837 - val_loss: 0.1850\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0836 - val_loss: 0.1838\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0825 - val_loss: 0.1797\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0825 - val_loss: 0.1879\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0827 - val_loss: 0.2058\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0809 - val_loss: 0.1971\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0802 - val_loss: 0.1798\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0775 - val_loss: 0.1730\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0785 - val_loss: 0.1935\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0799 - val_loss: 0.1882\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0864 - val_loss: 0.2231\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0812 - val_loss: 0.1908\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0790 - val_loss: 0.1985\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0759 - val_loss: 0.1972\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0751 - val_loss: 0.1747\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0766 - val_loss: 0.1792\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0815 - val_loss: 0.1561\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0795 - val_loss: 0.2152\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0789 - val_loss: 0.1926\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0752 - val_loss: 0.1824\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0747 - val_loss: 0.1846\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0774 - val_loss: 0.1671\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0746 - val_loss: 0.1835\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0734 - val_loss: 0.2088\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0716 - val_loss: 0.1949\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0745 - val_loss: 0.1969\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0709 - val_loss: 0.2285\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0710 - val_loss: 0.2032\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0703 - val_loss: 0.1998\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0685 - val_loss: 0.1788\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0793 - val_loss: 0.1791\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0936 - val_loss: 0.1670\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0722 - val_loss: 0.1657\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0703 - val_loss: 0.1693\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0681 - val_loss: 0.1696\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0679 - val_loss: 0.1624\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0660 - val_loss: 0.1648\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0655 - val_loss: 0.1565\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0660 - val_loss: 0.1738\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0665 - val_loss: 0.1735\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0651 - val_loss: 0.1604\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0672 - val_loss: 0.1626\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0659 - val_loss: 0.1534\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0695 - val_loss: 0.1680\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0659 - val_loss: 0.1846\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0653 - val_loss: 0.2000\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0632 - val_loss: 0.1938\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0617 - val_loss: 0.1864\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0620 - val_loss: 0.1719\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0658 - val_loss: 0.1660\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0670 - val_loss: 0.1790\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0653 - val_loss: 0.1699\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0620 - val_loss: 0.1646\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0603 - val_loss: 0.1891\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0637 - val_loss: 0.1578\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0696 - val_loss: 0.1992\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0687 - val_loss: 0.1944\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0626 - val_loss: 0.1701\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0601 - val_loss: 0.1687\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0603 - val_loss: 0.1831\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0607 - val_loss: 0.1645\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0598 - val_loss: 0.1831\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0588 - val_loss: 0.1818\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0601 - val_loss: 0.1815\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0611 - val_loss: 0.1762\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0593 - val_loss: 0.1785\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0586 - val_loss: 0.1695\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0575 - val_loss: 0.1784\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0615 - val_loss: 0.1681\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0718 - val_loss: 0.1701\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0592 - val_loss: 0.1853\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0572 - val_loss: 0.1681\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0573 - val_loss: 0.1785\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0572 - val_loss: 0.1778\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0556 - val_loss: 0.1921\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0576 - val_loss: 0.1718\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0615 - val_loss: 0.1783\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0571 - val_loss: 0.1671\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0559 - val_loss: 0.1727\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0560 - val_loss: 0.1660\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0545 - val_loss: 0.1814\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0545 - val_loss: 0.1735\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0556 - val_loss: 0.1659\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0568 - val_loss: 0.1743\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0548 - val_loss: 0.1745\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0544 - val_loss: 0.1736\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0542 - val_loss: 0.1822\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0546 - val_loss: 0.1752\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0541 - val_loss: 0.1732\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0543 - val_loss: 0.1809\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0532 - val_loss: 0.1840\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0525 - val_loss: 0.1996\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0548 - val_loss: 0.2018\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0566 - val_loss: 0.1814\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0558 - val_loss: 0.2014\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0529 - val_loss: 0.1931\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0525 - val_loss: 0.1893\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0536 - val_loss: 0.1821\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0517 - val_loss: 0.2133\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0526 - val_loss: 0.1824\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0576 - val_loss: 0.2026\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0521 - val_loss: 0.1810\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0514 - val_loss: 0.1890\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0614 - val_loss: 0.1802\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1574 - val_loss: 0.1860\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1129 - val_loss: 0.1638\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0828 - val_loss: 0.1932\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0623 - val_loss: 0.1894\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0565 - val_loss: 0.1748\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0547 - val_loss: 0.1954\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0537 - val_loss: 0.1779\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0528 - val_loss: 0.1762\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0519 - val_loss: 0.1814\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0516 - val_loss: 0.1878\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0510 - val_loss: 0.1926\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0506 - val_loss: 0.1768\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0507 - val_loss: 0.1803\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0504 - val_loss: 0.2058\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0501 - val_loss: 0.1822\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0500 - val_loss: 0.1869\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0504 - val_loss: 0.1867\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0496 - val_loss: 0.1822\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0492 - val_loss: 0.1768\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0493 - val_loss: 0.1875\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0495 - val_loss: 0.1939\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0492 - val_loss: 0.1774\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0493 - val_loss: 0.1767\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0490 - val_loss: 0.1758\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0486 - val_loss: 0.1897\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0483 - val_loss: 0.1932\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0499 - val_loss: 0.2016\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0596 - val_loss: 0.1791\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0498 - val_loss: 0.1830\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0488 - val_loss: 0.2013\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0482 - val_loss: 0.1811\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0486 - val_loss: 0.1655\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0490 - val_loss: 0.1883\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0476 - val_loss: 0.1852\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0476 - val_loss: 0.1846\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0477 - val_loss: 0.1872\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0472 - val_loss: 0.1831\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0475 - val_loss: 0.1819\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0475 - val_loss: 0.1634\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0484 - val_loss: 0.1719\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0472 - val_loss: 0.2028\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0465 - val_loss: 0.1890\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0465 - val_loss: 0.1783\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0474 - val_loss: 0.1823\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0483 - val_loss: 0.1887\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0479 - val_loss: 0.1794\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0484 - val_loss: 0.1977\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0464 - val_loss: 0.1834\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0461 - val_loss: 0.1883\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0459 - val_loss: 0.2012\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0462 - val_loss: 0.1946\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0456 - val_loss: 0.1962\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0456 - val_loss: 0.1888\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0459 - val_loss: 0.1874\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0686 - val_loss: 0.1431\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1202 - val_loss: 0.1720\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0688 - val_loss: 0.1534\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0613 - val_loss: 0.1403\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0507 - val_loss: 0.1670\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0490 - val_loss: 0.1682\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0474 - val_loss: 0.1641\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0463 - val_loss: 0.1666\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0458 - val_loss: 0.1610\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0456 - val_loss: 0.1559\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0459 - val_loss: 0.1698\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0453 - val_loss: 0.1635\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0449 - val_loss: 0.1695\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0453 - val_loss: 0.1652\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0453 - val_loss: 0.1622\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0451 - val_loss: 0.1799\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0448 - val_loss: 0.1711\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0448 - val_loss: 0.1624\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0444 - val_loss: 0.1734\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0443 - val_loss: 0.1677\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0443 - val_loss: 0.1572\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0443 - val_loss: 0.1698\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0439 - val_loss: 0.1712\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0440 - val_loss: 0.1758\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0447 - val_loss: 0.1681\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0476 - val_loss: 0.1780\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0443 - val_loss: 0.1734\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0437 - val_loss: 0.1728\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0436 - val_loss: 0.1677\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0433 - val_loss: 0.1771\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0439 - val_loss: 0.1760\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0429 - val_loss: 0.1855\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0431 - val_loss: 0.1694\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0431 - val_loss: 0.1663\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0433 - val_loss: 0.1772\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0432 - val_loss: 0.1830\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0431 - val_loss: 0.1799\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0428 - val_loss: 0.1788\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0431 - val_loss: 0.1664\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0452 - val_loss: 0.1807\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0468 - val_loss: 0.1718\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0446 - val_loss: 0.2179\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0438 - val_loss: 0.1720\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0433 - val_loss: 0.1772\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0426 - val_loss: 0.1712\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0429 - val_loss: 0.1768\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0510 - val_loss: 0.2035\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1302 - val_loss: 0.1755\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0844 - val_loss: 0.1719\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0546 - val_loss: 0.1866\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0475 - val_loss: 0.2041\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0457 - val_loss: 0.1950\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0443 - val_loss: 0.1947\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0436 - val_loss: 0.1949\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0433 - val_loss: 0.1878\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0428 - val_loss: 0.1902\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0425 - val_loss: 0.1843\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0427 - val_loss: 0.2114\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0423 - val_loss: 0.2004\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0423 - val_loss: 0.1924\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0420 - val_loss: 0.1963\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0419 - val_loss: 0.1966\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0418 - val_loss: 0.1945\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0419 - val_loss: 0.1920\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0416 - val_loss: 0.1920\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0417 - val_loss: 0.1998\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0416 - val_loss: 0.2027\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0413 - val_loss: 0.1892\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0415 - val_loss: 0.1937\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0416 - val_loss: 0.2088\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0412 - val_loss: 0.2008\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0413 - val_loss: 0.1873\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0411 - val_loss: 0.1974\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0412 - val_loss: 0.1946\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0412 - val_loss: 0.2029\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0408 - val_loss: 0.1962\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0412 - val_loss: 0.1940\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0411 - val_loss: 0.1919\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0408 - val_loss: 0.1888\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0413 - val_loss: 0.2012\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0409 - val_loss: 0.1874\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0409 - val_loss: 0.1886\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0407 - val_loss: 0.1925\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0404 - val_loss: 0.2059\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0409 - val_loss: 0.1819\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0406 - val_loss: 0.1771\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0402 - val_loss: 0.2038\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0406 - val_loss: 0.1900\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0403 - val_loss: 0.1818\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0406 - val_loss: 0.1927\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0403 - val_loss: 0.1956\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0405 - val_loss: 0.1871\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0403 - val_loss: 0.2154\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0414 - val_loss: 0.1942\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0947 - val_loss: 0.1579\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1558 - val_loss: 0.1420\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1141 - val_loss: 0.1279\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0782 - val_loss: 0.1406\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0653 - val_loss: 0.1666\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0544 - val_loss: 0.1586\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0496 - val_loss: 0.1668\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0473 - val_loss: 0.1583\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0450 - val_loss: 0.1689\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0442 - val_loss: 0.1696\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0436 - val_loss: 0.1750\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0425 - val_loss: 0.1731\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0424 - val_loss: 0.1827\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0419 - val_loss: 0.1888\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0416 - val_loss: 0.1855\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0414 - val_loss: 0.1829\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0413 - val_loss: 0.1831\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0409 - val_loss: 0.1840\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0406 - val_loss: 0.1890\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0407 - val_loss: 0.1841\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0403 - val_loss: 0.1807\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0401 - val_loss: 0.1978\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0401 - val_loss: 0.1924\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0401 - val_loss: 0.1886\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0399 - val_loss: 0.1899\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0401 - val_loss: 0.1876\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0402 - val_loss: 0.1861\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0397 - val_loss: 0.1861\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0395 - val_loss: 0.1909\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0395 - val_loss: 0.1893\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0394 - val_loss: 0.2111\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0393 - val_loss: 0.1874\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0393 - val_loss: 0.1944\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0395 - val_loss: 0.1891\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0396 - val_loss: 0.1961\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0397 - val_loss: 0.1836\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0389 - val_loss: 0.1902\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0391 - val_loss: 0.1967\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0393 - val_loss: 0.1915\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0388 - val_loss: 0.2065\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0385 - val_loss: 0.1927\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0387 - val_loss: 0.2187\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0390 - val_loss: 0.1999\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0388 - val_loss: 0.1892\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0386 - val_loss: 0.1867\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0391 - val_loss: 0.1937\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0390 - val_loss: 0.1924\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0387 - val_loss: 0.1830\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0387 - val_loss: 0.1984\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0388 - val_loss: 0.1988\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0384 - val_loss: 0.2008\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0381 - val_loss: 0.2060\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0381 - val_loss: 0.2053\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0384 - val_loss: 0.2026\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0382 - val_loss: 0.1968\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0384 - val_loss: 0.2112\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0382 - val_loss: 0.1960\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0388 - val_loss: 0.2076\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0388 - val_loss: 0.2291\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0394 - val_loss: 0.2047\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0386 - val_loss: 0.1932\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0381 - val_loss: 0.2046\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0379 - val_loss: 0.2164\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0385 - val_loss: 0.1931\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.2100\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0382 - val_loss: 0.2055\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0376 - val_loss: 0.2202\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0376 - val_loss: 0.2085\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0374 - val_loss: 0.2068\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0373 - val_loss: 0.2150\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0373 - val_loss: 0.2146\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0373 - val_loss: 0.1910\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0371 - val_loss: 0.2097\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.2068\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0375 - val_loss: 0.2068\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0373 - val_loss: 0.2095\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0372 - val_loss: 0.2175\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0373 - val_loss: 0.2279\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0376 - val_loss: 0.2050\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0383 - val_loss: 0.2018\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0382 - val_loss: 0.2131\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0379 - val_loss: 0.2087\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0372 - val_loss: 0.2062\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0370 - val_loss: 0.2178\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0380 - val_loss: 0.2089\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0372 - val_loss: 0.1952\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0370 - val_loss: 0.2366\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0367 - val_loss: 0.1968\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0366 - val_loss: 0.2193\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0365 - val_loss: 0.1985\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0367 - val_loss: 0.2189\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0367 - val_loss: 0.1966\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0389 - val_loss: 0.2294\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0375 - val_loss: 0.2255\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0446 - val_loss: 0.2264\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1077 - val_loss: 0.2074\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0586 - val_loss: 0.1673\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0606 - val_loss: 0.1789\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0424 - val_loss: 0.1615\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0389 - val_loss: 0.1697\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0379 - val_loss: 0.1799\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0372 - val_loss: 0.1847\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0369 - val_loss: 0.1813\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0369 - val_loss: 0.1814\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0368 - val_loss: 0.1841\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0363 - val_loss: 0.1814\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0361 - val_loss: 0.1949\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0362 - val_loss: 0.1946\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0360 - val_loss: 0.1911\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0358 - val_loss: 0.2005\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0359 - val_loss: 0.2016\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0358 - val_loss: 0.1939\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0359 - val_loss: 0.1915\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0356 - val_loss: 0.1964\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0355 - val_loss: 0.1921\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0355 - val_loss: 0.1998\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0354 - val_loss: 0.2058\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0353 - val_loss: 0.2102\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0354 - val_loss: 0.1920\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0353 - val_loss: 0.2037\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0358 - val_loss: 0.1981\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0357 - val_loss: 0.1975\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0356 - val_loss: 0.1996\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0353 - val_loss: 0.2065\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0352 - val_loss: 0.2019\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0351 - val_loss: 0.1976\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0354 - val_loss: 0.1918\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0354 - val_loss: 0.2144\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0354 - val_loss: 0.2184\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0354 - val_loss: 0.1968\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0362 - val_loss: 0.2080\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0351 - val_loss: 0.2051\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0349 - val_loss: 0.2048\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0348 - val_loss: 0.2004\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0352 - val_loss: 0.1805\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0352 - val_loss: 0.2067\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0349 - val_loss: 0.1991\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0349 - val_loss: 0.1969\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0349 - val_loss: 0.2087\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0351 - val_loss: 0.2185\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0352 - val_loss: 0.2083\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0494 - val_loss: 0.3488\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1131 - val_loss: 0.1963\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0714 - val_loss: 0.1964\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0499 - val_loss: 0.1581\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0417 - val_loss: 0.1666\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.1716\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0365 - val_loss: 0.1737\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 66ms/step - loss: 0.6751 - val_loss: 0.6438\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5944 - val_loss: 0.5037\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4918 - val_loss: 0.3568\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4464 - val_loss: 0.3721\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4081 - val_loss: 0.3215\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3820 - val_loss: 0.3157\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3673 - val_loss: 0.3034\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3395 - val_loss: 0.2754\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3402 - val_loss: 0.2661\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3296 - val_loss: 0.2612\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3199 - val_loss: 0.2726\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3255 - val_loss: 0.2485\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3148 - val_loss: 0.2400\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3068 - val_loss: 0.2389\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2985 - val_loss: 0.2302\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2969 - val_loss: 0.2283\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3080 - val_loss: 0.2421\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3028 - val_loss: 0.2259\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2864 - val_loss: 0.2207\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2805 - val_loss: 0.2209\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2744 - val_loss: 0.2296\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2726 - val_loss: 0.2228\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2683 - val_loss: 0.2274\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2584 - val_loss: 0.2404\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2663 - val_loss: 0.2134\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2554 - val_loss: 0.2186\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2467 - val_loss: 0.2721\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2492 - val_loss: 0.2133\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2454 - val_loss: 0.2302\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2334 - val_loss: 0.2426\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2384 - val_loss: 0.2141\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2334 - val_loss: 0.2393\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2303 - val_loss: 0.2117\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2337 - val_loss: 0.2866\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2118 - val_loss: 0.4073\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2200 - val_loss: 0.3454\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2184 - val_loss: 0.3492\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1991 - val_loss: 0.4169\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1994 - val_loss: 0.7310\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1923 - val_loss: 0.4141\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1956 - val_loss: 0.4304\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1878 - val_loss: 0.4368\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1791 - val_loss: 0.7025\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1810 - val_loss: 0.7362\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1749 - val_loss: 0.8042\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1739 - val_loss: 0.5548\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1677 - val_loss: 1.5198\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1831 - val_loss: 0.6376\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1604 - val_loss: 0.7871\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1667 - val_loss: 1.0270\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1625 - val_loss: 0.7953\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1623 - val_loss: 0.5958\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1577 - val_loss: 0.8244\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1482 - val_loss: 0.9314\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1492 - val_loss: 1.1106\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1478 - val_loss: 0.5968\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1444 - val_loss: 0.7728\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1416 - val_loss: 0.9507\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1374 - val_loss: 0.7994\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1404 - val_loss: 1.0919\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1391 - val_loss: 0.7503\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1520 - val_loss: 0.2823\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1433 - val_loss: 0.6098\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1386 - val_loss: 0.6220\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1336 - val_loss: 0.4880\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1335 - val_loss: 0.8119\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1303 - val_loss: 0.4855\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1328 - val_loss: 0.7006\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1307 - val_loss: 0.7824\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1259 - val_loss: 0.4636\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1262 - val_loss: 0.8465\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1261 - val_loss: 0.5787\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1219 - val_loss: 0.9989\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1251 - val_loss: 0.7620\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1224 - val_loss: 1.1291\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1212 - val_loss: 0.6803\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1165 - val_loss: 0.7277\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1170 - val_loss: 0.5181\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1137 - val_loss: 0.7208\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1165 - val_loss: 0.6294\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1234 - val_loss: 0.5995\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1241 - val_loss: 0.4134\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1201 - val_loss: 0.4539\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1124 - val_loss: 0.9904\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1175 - val_loss: 0.7203\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1118 - val_loss: 0.4823\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1092 - val_loss: 0.5361\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1090 - val_loss: 0.5799\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1072 - val_loss: 0.4826\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1034 - val_loss: 0.7873\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1070 - val_loss: 0.5350\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1059 - val_loss: 0.4632\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1048 - val_loss: 0.4228\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1037 - val_loss: 0.4481\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1014 - val_loss: 0.5984\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1012 - val_loss: 0.5212\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1011 - val_loss: 0.4010\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1005 - val_loss: 0.5828\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0996 - val_loss: 0.2515\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1222 - val_loss: 0.2297\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 5s 67ms/step - loss: 0.6314 - val_loss: 0.5713\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5568 - val_loss: 0.4585\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5133 - val_loss: 0.4585\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4955 - val_loss: 0.4640\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4767 - val_loss: 0.4644\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4717 - val_loss: 0.4454\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4617 - val_loss: 0.4256\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4490 - val_loss: 0.3851\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4065 - val_loss: 0.4127\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3852 - val_loss: 0.3950\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3840 - val_loss: 0.4413\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3641 - val_loss: 0.4584\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3387 - val_loss: 0.4884\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3258 - val_loss: 0.4118\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2971 - val_loss: 0.3537\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2922 - val_loss: 0.3801\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2823 - val_loss: 0.3985\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2801 - val_loss: 0.4132\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2794 - val_loss: 0.3729\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2833 - val_loss: 0.3371\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2674 - val_loss: 0.2947\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2596 - val_loss: 0.3425\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2615 - val_loss: 0.3063\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2469 - val_loss: 0.2860\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2303 - val_loss: 0.2850\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2361 - val_loss: 0.2668\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2637 - val_loss: 0.3090\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2353 - val_loss: 0.2629\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2153 - val_loss: 0.2382\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2034 - val_loss: 0.2239\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1971 - val_loss: 0.2305\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1938 - val_loss: 0.2323\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1975 - val_loss: 0.2411\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1966 - val_loss: 0.2042\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1930 - val_loss: 0.2109\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1820 - val_loss: 0.2113\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1849 - val_loss: 0.2294\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1897 - val_loss: 0.1982\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1745 - val_loss: 0.2054\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1608 - val_loss: 0.2245\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1672 - val_loss: 0.2208\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1672 - val_loss: 0.2198\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1595 - val_loss: 0.2112\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1597 - val_loss: 0.2050\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1528 - val_loss: 0.2082\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1501 - val_loss: 0.2204\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1480 - val_loss: 0.2357\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1523 - val_loss: 0.2112\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1451 - val_loss: 0.1952\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1530 - val_loss: 0.2143\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1456 - val_loss: 0.2029\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1494 - val_loss: 0.2235\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1379 - val_loss: 0.2105\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1434 - val_loss: 0.2079\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1349 - val_loss: 0.2348\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1420 - val_loss: 0.2339\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1390 - val_loss: 0.2021\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1362 - val_loss: 0.2385\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1380 - val_loss: 0.1915\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1380 - val_loss: 0.2301\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1321 - val_loss: 0.2203\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1292 - val_loss: 0.1937\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1302 - val_loss: 0.2232\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1269 - val_loss: 0.2245\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1276 - val_loss: 0.2010\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1294 - val_loss: 0.1993\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1245 - val_loss: 0.2250\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1268 - val_loss: 0.2059\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1222 - val_loss: 0.2022\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1203 - val_loss: 0.2226\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1235 - val_loss: 0.2700\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1217 - val_loss: 0.2251\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1214 - val_loss: 0.2122\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1171 - val_loss: 0.2252\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1143 - val_loss: 0.2056\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1177 - val_loss: 0.2170\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1181 - val_loss: 0.2081\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1118 - val_loss: 0.2041\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1087 - val_loss: 0.2238\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1193 - val_loss: 0.2234\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1208 - val_loss: 0.2325\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1112 - val_loss: 0.2227\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1073 - val_loss: 0.1903\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1083 - val_loss: 0.2674\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1171 - val_loss: 0.2074\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1056 - val_loss: 0.2398\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1143 - val_loss: 0.2346\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1036 - val_loss: 0.2562\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1018 - val_loss: 0.2344\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1004 - val_loss: 0.2107\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1032 - val_loss: 0.2656\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1224 - val_loss: 0.2060\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1136 - val_loss: 0.1965\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1054 - val_loss: 0.2106\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0979 - val_loss: 0.2213\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0984 - val_loss: 0.2274\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0946 - val_loss: 0.2288\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0941 - val_loss: 0.2308\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0952 - val_loss: 0.2280\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0946 - val_loss: 0.2080\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0921 - val_loss: 0.2304\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0925 - val_loss: 0.2256\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0932 - val_loss: 0.2206\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0914 - val_loss: 0.2337\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0922 - val_loss: 0.2166\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0919 - val_loss: 0.2371\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0914 - val_loss: 0.2392\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0888 - val_loss: 0.2271\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0878 - val_loss: 0.2416\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0855 - val_loss: 0.2276\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0864 - val_loss: 0.2682\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0898 - val_loss: 0.2284\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0904 - val_loss: 0.2414\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0983 - val_loss: 0.2014\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1036 - val_loss: 0.2209\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0878 - val_loss: 0.1845\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0856 - val_loss: 0.2067\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0830 - val_loss: 0.2157\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0841 - val_loss: 0.2314\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.0823 - val_loss: 0.2203\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.1494 - val_loss: 0.4037\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 67ms/step - loss: 0.1864 - val_loss: 0.2012\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1334 - val_loss: 0.2486\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1101 - val_loss: 0.2075\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0945 - val_loss: 0.2009\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.0897 - val_loss: 0.1987\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0854 - val_loss: 0.2190\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0863 - val_loss: 0.1959\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0829 - val_loss: 0.2045\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0838 - val_loss: 0.2113\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0817 - val_loss: 0.2175\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0805 - val_loss: 0.2221\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0793 - val_loss: 0.2202\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0781 - val_loss: 0.2128\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0775 - val_loss: 0.2205\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0775 - val_loss: 0.2519\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0793 - val_loss: 0.2443\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0765 - val_loss: 0.2145\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0759 - val_loss: 0.2244\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0745 - val_loss: 0.2111\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0745 - val_loss: 0.2235\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0739 - val_loss: 0.2115\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0727 - val_loss: 0.2145\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0726 - val_loss: 0.2333\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0720 - val_loss: 0.2485\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0747 - val_loss: 0.2254\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0773 - val_loss: 0.2253\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0713 - val_loss: 0.2021\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0699 - val_loss: 0.2253\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0719 - val_loss: 0.2143\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0754 - val_loss: 0.2292\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0696 - val_loss: 0.2392\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0701 - val_loss: 0.2050\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0694 - val_loss: 0.2407\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0700 - val_loss: 0.2154\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0705 - val_loss: 0.2318\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0696 - val_loss: 0.2316\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0672 - val_loss: 0.2112\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0675 - val_loss: 0.2286\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0671 - val_loss: 0.2364\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0682 - val_loss: 0.2233\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0664 - val_loss: 0.2087\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0656 - val_loss: 0.2331\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0664 - val_loss: 0.2091\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0659 - val_loss: 0.2033\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0650 - val_loss: 0.2324\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0651 - val_loss: 0.1855\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0640 - val_loss: 0.2336\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0654 - val_loss: 0.1997\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0647 - val_loss: 0.2118\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0745 - val_loss: 0.3454\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0704 - val_loss: 0.2064\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0647 - val_loss: 0.1997\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0652 - val_loss: 0.2085\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1213 - val_loss: 0.2299\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1002 - val_loss: 0.1995\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1166 - val_loss: 0.1963\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0946 - val_loss: 0.2444\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0758 - val_loss: 0.2394\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0703 - val_loss: 0.2209\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0680 - val_loss: 0.2532\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0660 - val_loss: 0.2505\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0652 - val_loss: 0.2533\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0645 - val_loss: 0.2237\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0638 - val_loss: 0.2351\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0620 - val_loss: 0.2322\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0623 - val_loss: 0.2393\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0612 - val_loss: 0.2264\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0614 - val_loss: 0.2271\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0600 - val_loss: 0.2315\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0605 - val_loss: 0.2426\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0602 - val_loss: 0.2331\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0600 - val_loss: 0.2207\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0634 - val_loss: 0.2530\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0660 - val_loss: 0.2307\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0616 - val_loss: 0.2106\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0595 - val_loss: 0.2176\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0584 - val_loss: 0.2001\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0578 - val_loss: 0.2239\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0579 - val_loss: 0.2203\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 4s 68ms/step - loss: 0.6416 - val_loss: 0.5049\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5510 - val_loss: 0.4523\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5092 - val_loss: 0.4132\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4917 - val_loss: 0.4035\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4703 - val_loss: 0.3637\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4748 - val_loss: 0.4010\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4398 - val_loss: 0.3511\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4026 - val_loss: 0.3298\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3827 - val_loss: 0.3183\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3869 - val_loss: 0.2811\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3583 - val_loss: 0.2791\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3426 - val_loss: 0.2706\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3390 - val_loss: 0.2857\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3132 - val_loss: 0.2647\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.3288 - val_loss: 0.2925\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3281 - val_loss: 0.2757\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3108 - val_loss: 0.2429\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3260 - val_loss: 0.2523\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3028 - val_loss: 0.2467\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2984 - val_loss: 0.2893\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2999 - val_loss: 0.2302\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2801 - val_loss: 0.2387\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2730 - val_loss: 0.2321\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2695 - val_loss: 0.2754\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3001 - val_loss: 0.2587\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3019 - val_loss: 0.2562\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2668 - val_loss: 0.2273\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2691 - val_loss: 0.2383\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2527 - val_loss: 0.2198\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2583 - val_loss: 0.2217\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2568 - val_loss: 0.2230\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2502 - val_loss: 0.2187\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2355 - val_loss: 0.2168\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2279 - val_loss: 0.2260\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2276 - val_loss: 0.2316\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2203 - val_loss: 0.2177\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2164 - val_loss: 0.2282\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2221 - val_loss: 0.2159\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2082 - val_loss: 0.2170\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2325 - val_loss: 0.2280\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2078 - val_loss: 0.2370\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1987 - val_loss: 0.2419\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2126 - val_loss: 0.2450\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1940 - val_loss: 0.2305\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1873 - val_loss: 0.2358\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2104 - val_loss: 0.2652\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1920 - val_loss: 0.2146\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1850 - val_loss: 0.2076\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1818 - val_loss: 0.2428\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1766 - val_loss: 0.2193\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1783 - val_loss: 0.2589\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1689 - val_loss: 0.2202\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1731 - val_loss: 0.2109\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1671 - val_loss: 0.2363\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1808 - val_loss: 0.2117\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1635 - val_loss: 0.2241\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1588 - val_loss: 0.2214\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1518 - val_loss: 0.2213\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1505 - val_loss: 0.2264\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1605 - val_loss: 0.2101\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1509 - val_loss: 0.2257\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1802 - val_loss: 0.2157\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1524 - val_loss: 0.2331\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1528 - val_loss: 0.1855\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1530 - val_loss: 0.2521\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1426 - val_loss: 0.2625\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1381 - val_loss: 0.2676\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1336 - val_loss: 0.2113\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1308 - val_loss: 0.2580\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1382 - val_loss: 0.2992\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1350 - val_loss: 0.2524\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1488 - val_loss: 0.1953\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1479 - val_loss: 0.2167\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1438 - val_loss: 0.1788\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1329 - val_loss: 0.2418\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1262 - val_loss: 0.2299\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1260 - val_loss: 0.1999\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1367 - val_loss: 0.2431\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1361 - val_loss: 0.2090\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1258 - val_loss: 0.2144\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1212 - val_loss: 0.2271\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1218 - val_loss: 0.2577\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1174 - val_loss: 0.2304\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1188 - val_loss: 0.2785\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1342 - val_loss: 0.1939\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1267 - val_loss: 0.2126\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1148 - val_loss: 0.2384\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1136 - val_loss: 0.3083\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1164 - val_loss: 0.2790\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1153 - val_loss: 0.2164\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1118 - val_loss: 0.3275\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1091 - val_loss: 0.2612\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1107 - val_loss: 0.2907\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1102 - val_loss: 0.2626\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1107 - val_loss: 0.2723\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1689 - val_loss: 0.1878\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1822 - val_loss: 0.2593\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1437 - val_loss: 0.2149\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1276 - val_loss: 0.2151\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1167 - val_loss: 0.2125\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1179 - val_loss: 0.2267\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1141 - val_loss: 0.2110\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1115 - val_loss: 0.2522\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1073 - val_loss: 0.2902\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1065 - val_loss: 0.2484\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1045 - val_loss: 0.2962\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1073 - val_loss: 0.2722\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1018 - val_loss: 0.2589\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1028 - val_loss: 0.3074\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1060 - val_loss: 0.2356\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0997 - val_loss: 0.3352\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0990 - val_loss: 0.2487\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0999 - val_loss: 0.2523\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0973 - val_loss: 0.3427\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0978 - val_loss: 0.3006\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0953 - val_loss: 0.2382\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1012 - val_loss: 0.2341\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0993 - val_loss: 0.2364\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0966 - val_loss: 0.2932\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1011 - val_loss: 0.2723\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0976 - val_loss: 0.2059\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0985 - val_loss: 0.2282\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0995 - val_loss: 0.2079\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0935 - val_loss: 0.2426\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0939 - val_loss: 0.2144\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0998 - val_loss: 0.4734\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1038 - val_loss: 0.2004\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0912 - val_loss: 0.2314\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0887 - val_loss: 0.3260\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0903 - val_loss: 0.2454\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1135 - val_loss: 0.1862\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1095 - val_loss: 0.1994\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0964 - val_loss: 0.4456\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1050 - val_loss: 0.1919\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0938 - val_loss: 0.2832\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0874 - val_loss: 0.2041\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0884 - val_loss: 0.2755\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0855 - val_loss: 0.2852\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0836 - val_loss: 0.2992\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0841 - val_loss: 0.4038\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0823 - val_loss: 0.3560\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0824 - val_loss: 0.3633\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0815 - val_loss: 0.3231\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0905 - val_loss: 0.3331\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0839 - val_loss: 0.2715\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0834 - val_loss: 0.2476\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0815 - val_loss: 0.2979\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0810 - val_loss: 0.3493\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0873 - val_loss: 0.2153\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0827 - val_loss: 0.3889\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1000 - val_loss: 0.3306\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1202 - val_loss: 0.2623\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0868 - val_loss: 0.2758\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0796 - val_loss: 0.2698\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0786 - val_loss: 0.2315\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0765 - val_loss: 0.2828\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0756 - val_loss: 0.2853\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0766 - val_loss: 0.2432\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0759 - val_loss: 0.3592\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0771 - val_loss: 0.2611\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0778 - val_loss: 0.2176\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0743 - val_loss: 0.2227\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0761 - val_loss: 0.2417\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0811 - val_loss: 0.1959\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1057 - val_loss: 0.2163\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0924 - val_loss: 0.2039\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0876 - val_loss: 0.2137\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0775 - val_loss: 0.2209\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0742 - val_loss: 0.2109\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0712 - val_loss: 0.2565\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0708 - val_loss: 0.2597\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0723 - val_loss: 0.2135\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0709 - val_loss: 0.2426\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0705 - val_loss: 0.2324\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0693 - val_loss: 0.2385\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0694 - val_loss: 0.2496\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0715 - val_loss: 0.3532\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0712 - val_loss: 0.2636\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0712 - val_loss: 0.3072\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0833 - val_loss: 0.3097\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0792 - val_loss: 0.2160\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0761 - val_loss: 0.2069\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0702 - val_loss: 0.3038\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0765 - val_loss: 0.2554\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0714 - val_loss: 0.2557\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0676 - val_loss: 0.2331\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0690 - val_loss: 0.2409\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0674 - val_loss: 0.2496\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0651 - val_loss: 0.2308\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0659 - val_loss: 0.2667\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0649 - val_loss: 0.2179\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0658 - val_loss: 0.2328\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0665 - val_loss: 0.2281\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0666 - val_loss: 0.2430\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0660 - val_loss: 0.3350\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0673 - val_loss: 0.2279\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0645 - val_loss: 0.2718\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0640 - val_loss: 0.2863\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0647 - val_loss: 0.2889\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0630 - val_loss: 0.2915\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0639 - val_loss: 0.2607\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0650 - val_loss: 0.3048\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0637 - val_loss: 0.2769\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0641 - val_loss: 0.2464\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0634 - val_loss: 0.2300\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0624 - val_loss: 0.2490\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0615 - val_loss: 0.3000\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0611 - val_loss: 0.2668\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0622 - val_loss: 0.2434\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0639 - val_loss: 0.3905\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0670 - val_loss: 0.2311\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0621 - val_loss: 0.2623\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0620 - val_loss: 0.2332\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0609 - val_loss: 0.3149\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0602 - val_loss: 0.2701\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0607 - val_loss: 0.2390\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0617 - val_loss: 0.2317\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0647 - val_loss: 0.1983\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0618 - val_loss: 0.2606\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0640 - val_loss: 0.2111\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0620 - val_loss: 0.2507\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0593 - val_loss: 0.2813\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0582 - val_loss: 0.3105\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0586 - val_loss: 0.3399\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0591 - val_loss: 0.2432\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0593 - val_loss: 0.2586\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0588 - val_loss: 0.2581\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0578 - val_loss: 0.3277\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0593 - val_loss: 0.2749\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0603 - val_loss: 0.2634\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0571 - val_loss: 0.2745\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0575 - val_loss: 0.2742\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0560 - val_loss: 0.2567\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0566 - val_loss: 0.2142\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0558 - val_loss: 0.2994\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0557 - val_loss: 0.2970\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0551 - val_loss: 0.3127\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0554 - val_loss: 0.2768\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0559 - val_loss: 0.2858\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0562 - val_loss: 0.2320\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0555 - val_loss: 0.2924\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0547 - val_loss: 0.2639\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0547 - val_loss: 0.2586\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0559 - val_loss: 0.2502\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0562 - val_loss: 0.2963\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0564 - val_loss: 0.2796\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0551 - val_loss: 0.2382\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0551 - val_loss: 0.2703\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0556 - val_loss: 0.3004\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0670 - val_loss: 0.3403\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0891 - val_loss: 0.2645\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0761 - val_loss: 0.2862\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1613 - val_loss: 0.2123\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1135 - val_loss: 0.1901\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0791 - val_loss: 0.2005\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0641 - val_loss: 0.2112\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0591 - val_loss: 0.2827\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0571 - val_loss: 0.2892\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0562 - val_loss: 0.2396\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0556 - val_loss: 0.2260\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0551 - val_loss: 0.2445\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0538 - val_loss: 0.3122\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0539 - val_loss: 0.2878\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0536 - val_loss: 0.2665\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0531 - val_loss: 0.2551\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0526 - val_loss: 0.2734\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0522 - val_loss: 0.2700\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0528 - val_loss: 0.2741\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0526 - val_loss: 0.3106\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0523 - val_loss: 0.2952\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0519 - val_loss: 0.2910\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0512 - val_loss: 0.3031\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0512 - val_loss: 0.3522\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0511 - val_loss: 0.3217\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0506 - val_loss: 0.3112\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0506 - val_loss: 0.3075\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0504 - val_loss: 0.3321\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0505 - val_loss: 0.2661\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0509 - val_loss: 0.2828\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0511 - val_loss: 0.3278\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0510 - val_loss: 0.3773\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0507 - val_loss: 0.2873\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0501 - val_loss: 0.2788\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0504 - val_loss: 0.3043\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0510 - val_loss: 0.3526\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0501 - val_loss: 0.2569\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0501 - val_loss: 0.2622\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0501 - val_loss: 0.2604\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0509 - val_loss: 0.2706\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0497 - val_loss: 0.2828\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0498 - val_loss: 0.2483\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0509 - val_loss: 0.3422\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0508 - val_loss: 0.3617\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0496 - val_loss: 0.2730\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0484 - val_loss: 0.2712\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0486 - val_loss: 0.2812\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0490 - val_loss: 0.2539\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0493 - val_loss: 0.3439\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0489 - val_loss: 0.2575\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0492 - val_loss: 0.2588\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0486 - val_loss: 0.2530\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0506 - val_loss: 0.4430\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0542 - val_loss: 0.2044\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0499 - val_loss: 0.2166\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0493 - val_loss: 0.2361\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0478 - val_loss: 0.2516\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0475 - val_loss: 0.2690\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0474 - val_loss: 0.2626\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0473 - val_loss: 0.2301\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0481 - val_loss: 0.2230\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0473 - val_loss: 0.2430\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0484 - val_loss: 0.2416\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0473 - val_loss: 0.2274\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0470 - val_loss: 0.2266\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0468 - val_loss: 0.2367\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0465 - val_loss: 0.2280\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0478 - val_loss: 0.2145\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0482 - val_loss: 0.2878\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0493 - val_loss: 0.2324\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.0482 - val_loss: 0.2098\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0471 - val_loss: 0.2454\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0464 - val_loss: 0.2166\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0469 - val_loss: 0.2404\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0468 - val_loss: 0.3466\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0467 - val_loss: 0.2226\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0474 - val_loss: 0.2665\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0491 - val_loss: 0.2180\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0543 - val_loss: 0.5614\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0888 - val_loss: 0.3953\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0559 - val_loss: 0.2418\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0483 - val_loss: 0.2481\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0464 - val_loss: 0.2575\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0459 - val_loss: 0.2706\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0454 - val_loss: 0.2876\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0453 - val_loss: 0.2701\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0458 - val_loss: 0.2656\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0459 - val_loss: 0.2833\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0455 - val_loss: 0.2597\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0448 - val_loss: 0.2614\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0443 - val_loss: 0.3041\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0442 - val_loss: 0.2683\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0445 - val_loss: 0.3582\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0444 - val_loss: 0.2820\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0442 - val_loss: 0.2693\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0449 - val_loss: 0.2553\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0455 - val_loss: 0.2483\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0444 - val_loss: 0.2752\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0436 - val_loss: 0.2664\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0437 - val_loss: 0.2543\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0436 - val_loss: 0.2853\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0438 - val_loss: 0.2792\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0441 - val_loss: 0.3336\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0436 - val_loss: 0.2351\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0434 - val_loss: 0.2482\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0433 - val_loss: 0.2410\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0433 - val_loss: 0.2509\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0434 - val_loss: 0.2403\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0442 - val_loss: 0.2333\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0444 - val_loss: 0.3729\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0441 - val_loss: 0.2875\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0437 - val_loss: 0.2467\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0437 - val_loss: 0.2964\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0433 - val_loss: 0.2543\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0435 - val_loss: 0.2760\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0436 - val_loss: 0.2386\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0431 - val_loss: 0.2659\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0430 - val_loss: 0.3016\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0427 - val_loss: 0.2696\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0429 - val_loss: 0.2422\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0429 - val_loss: 0.2841\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0427 - val_loss: 0.2544\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0430 - val_loss: 0.2278\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0432 - val_loss: 0.2607\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0427 - val_loss: 0.2474\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0423 - val_loss: 0.2593\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0423 - val_loss: 0.2844\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0437 - val_loss: 0.2805\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0456 - val_loss: 0.2841\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0434 - val_loss: 0.2578\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0424 - val_loss: 0.2294\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0424 - val_loss: 0.2457\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0418 - val_loss: 0.2289\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0424 - val_loss: 0.2781\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0418 - val_loss: 0.2259\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0415 - val_loss: 0.2782\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0414 - val_loss: 0.2737\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0416 - val_loss: 0.3002\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0425 - val_loss: 0.2562\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0432 - val_loss: 0.2240\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0435 - val_loss: 0.2831\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0425 - val_loss: 0.2452\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0415 - val_loss: 0.2549\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0413 - val_loss: 0.2704\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0419 - val_loss: 0.2103\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1423 - val_loss: 0.1549\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0985 - val_loss: 0.1892\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0677 - val_loss: 0.1867\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0495 - val_loss: 0.2215\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0460 - val_loss: 0.2114\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0441 - val_loss: 0.2219\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0432 - val_loss: 0.2293\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0424 - val_loss: 0.2290\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0420 - val_loss: 0.2190\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0418 - val_loss: 0.2283\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0413 - val_loss: 0.2309\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0413 - val_loss: 0.2508\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0412 - val_loss: 0.2303\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0409 - val_loss: 0.2349\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0407 - val_loss: 0.2261\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0407 - val_loss: 0.2307\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0406 - val_loss: 0.2356\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0406 - val_loss: 0.2436\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0407 - val_loss: 0.2458\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0403 - val_loss: 0.2426\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0402 - val_loss: 0.2580\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0400 - val_loss: 0.2493\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0399 - val_loss: 0.2380\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0399 - val_loss: 0.2626\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0398 - val_loss: 0.2304\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0398 - val_loss: 0.2465\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0397 - val_loss: 0.2410\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0396 - val_loss: 0.2265\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0397 - val_loss: 0.2336\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0395 - val_loss: 0.2418\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0396 - val_loss: 0.2488\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0396 - val_loss: 0.2523\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0394 - val_loss: 0.2411\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0393 - val_loss: 0.2392\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0395 - val_loss: 0.2280\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0395 - val_loss: 0.2436\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0392 - val_loss: 0.2444\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0394 - val_loss: 0.2140\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0391 - val_loss: 0.2511\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0394 - val_loss: 0.2419\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0392 - val_loss: 0.2395\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0392 - val_loss: 0.2578\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0393 - val_loss: 0.2406\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0391 - val_loss: 0.2396\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0390 - val_loss: 0.2423\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0390 - val_loss: 0.2258\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0398 - val_loss: 0.2293\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0390 - val_loss: 0.2431\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0388 - val_loss: 0.2517\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0387 - val_loss: 0.2458\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0387 - val_loss: 0.2442\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0386 - val_loss: 0.2719\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0391 - val_loss: 0.2704\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0394 - val_loss: 0.2513\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0394 - val_loss: 0.2506\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0389 - val_loss: 0.2190\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0386 - val_loss: 0.2362\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0389 - val_loss: 0.2608\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0387 - val_loss: 0.2400\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0387 - val_loss: 0.2645\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0387 - val_loss: 0.2421\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0386 - val_loss: 0.2470\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0381 - val_loss: 0.2465\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0383 - val_loss: 0.2461\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0380 - val_loss: 0.2734\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0380 - val_loss: 0.2354\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0379 - val_loss: 0.2267\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0379 - val_loss: 0.2328\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0380 - val_loss: 0.2457\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.2699\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0382 - val_loss: 0.2722\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0379 - val_loss: 0.2339\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0383 - val_loss: 0.2461\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0382 - val_loss: 0.2554\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0387 - val_loss: 0.2144\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0381 - val_loss: 0.2547\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0381 - val_loss: 0.2222\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0380 - val_loss: 0.2477\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0375 - val_loss: 0.2558\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0373 - val_loss: 0.2296\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0373 - val_loss: 0.2330\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0373 - val_loss: 0.2346\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0372 - val_loss: 0.2349\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0376 - val_loss: 0.2441\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.2575\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0373 - val_loss: 0.2051\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0373 - val_loss: 0.2763\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0372 - val_loss: 0.2384\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0373 - val_loss: 0.2570\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0379 - val_loss: 0.2570\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0377 - val_loss: 0.2263\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0377 - val_loss: 0.2193\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0373 - val_loss: 0.2267\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0371 - val_loss: 0.2754\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0373 - val_loss: 0.2422\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0369 - val_loss: 0.2703\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0368 - val_loss: 0.2544\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0369 - val_loss: 0.2372\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0374 - val_loss: 0.2420\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0379 - val_loss: 0.2670\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0379 - val_loss: 0.2179\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0367 - val_loss: 0.2309\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0364 - val_loss: 0.2417\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0365 - val_loss: 0.2634\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0368 - val_loss: 0.2563\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0362 - val_loss: 0.2533\n",
      "8/8 [==============================] - 0s 29ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 66ms/step - loss: 0.6363 - val_loss: 0.6028\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5689 - val_loss: 0.5215\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4991 - val_loss: 0.5239\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4723 - val_loss: 0.5848\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4335 - val_loss: 0.5557\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3910 - val_loss: 0.6088\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3898 - val_loss: 0.6240\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3760 - val_loss: 0.6245\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3545 - val_loss: 0.7072\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3331 - val_loss: 0.8262\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3357 - val_loss: 0.7869\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3182 - val_loss: 1.0213\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3043 - val_loss: 0.7934\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3135 - val_loss: 1.0387\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3050 - val_loss: 0.8556\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2944 - val_loss: 0.8367\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3081 - val_loss: 0.8229\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2843 - val_loss: 0.9027\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2885 - val_loss: 0.9308\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2680 - val_loss: 0.9404\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2937 - val_loss: 0.8855\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2875 - val_loss: 1.0520\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2748 - val_loss: 0.9020\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2644 - val_loss: 0.9251\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2574 - val_loss: 0.9987\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2440 - val_loss: 0.6682\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2488 - val_loss: 0.8486\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2342 - val_loss: 1.2315\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2250 - val_loss: 0.8274\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2296 - val_loss: 1.1122\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2162 - val_loss: 0.9468\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2137 - val_loss: 0.8786\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2353 - val_loss: 0.6989\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2087 - val_loss: 0.7210\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1972 - val_loss: 1.2407\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1902 - val_loss: 0.7884\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1922 - val_loss: 0.6684\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1847 - val_loss: 0.9083\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1748 - val_loss: 0.9136\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1752 - val_loss: 1.0651\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1787 - val_loss: 0.9195\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1758 - val_loss: 0.7233\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1669 - val_loss: 0.7505\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1690 - val_loss: 0.6262\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2431 - val_loss: 0.3305\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1961 - val_loss: 0.4936\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1718 - val_loss: 0.3993\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1625 - val_loss: 0.3837\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1588 - val_loss: 0.3192\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1564 - val_loss: 0.4511\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1501 - val_loss: 0.5161\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1561 - val_loss: 0.4592\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1558 - val_loss: 0.3061\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1542 - val_loss: 0.5970\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1445 - val_loss: 0.4340\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1471 - val_loss: 0.6205\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1445 - val_loss: 0.6258\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1520 - val_loss: 0.4558\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1374 - val_loss: 0.3317\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1345 - val_loss: 0.4091\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1395 - val_loss: 0.4523\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1354 - val_loss: 0.4825\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1339 - val_loss: 0.4167\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1336 - val_loss: 0.4726\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1269 - val_loss: 0.4015\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1305 - val_loss: 0.4106\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1288 - val_loss: 0.2885\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1246 - val_loss: 0.3503\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1236 - val_loss: 0.3400\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1219 - val_loss: 0.3832\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1202 - val_loss: 0.3938\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1149 - val_loss: 0.3676\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1183 - val_loss: 0.4079\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1184 - val_loss: 0.4493\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1166 - val_loss: 0.5626\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1234 - val_loss: 0.4666\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1126 - val_loss: 0.3515\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1106 - val_loss: 0.4469\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1121 - val_loss: 0.3190\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1142 - val_loss: 0.3944\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1169 - val_loss: 0.3304\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1202 - val_loss: 0.3372\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1080 - val_loss: 0.4256\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1091 - val_loss: 0.3255\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1090 - val_loss: 0.4435\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1055 - val_loss: 0.3473\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1158 - val_loss: 0.4997\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1078 - val_loss: 0.3533\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1046 - val_loss: 0.4273\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1010 - val_loss: 0.3607\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1039 - val_loss: 0.2663\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1011 - val_loss: 0.3099\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1023 - val_loss: 0.2818\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1015 - val_loss: 0.3385\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0990 - val_loss: 0.3911\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0985 - val_loss: 0.3076\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0967 - val_loss: 0.3089\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1015 - val_loss: 0.2796\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0964 - val_loss: 0.2993\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0941 - val_loss: 0.3338\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 4s 68ms/step - loss: 0.6545 - val_loss: 0.5401\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.6241 - val_loss: 0.5038\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5979 - val_loss: 0.4442\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4906 - val_loss: 0.3700\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4755 - val_loss: 0.3732\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4242 - val_loss: 0.3216\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4025 - val_loss: 0.3190\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3979 - val_loss: 0.3179\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3823 - val_loss: 0.3388\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3810 - val_loss: 0.3010\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3736 - val_loss: 0.2992\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.3674 - val_loss: 0.3065\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3682 - val_loss: 0.3398\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3753 - val_loss: 0.3399\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3662 - val_loss: 0.3210\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3472 - val_loss: 0.3248\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3546 - val_loss: 0.3308\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3339 - val_loss: 0.3148\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3456 - val_loss: 0.3371\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3332 - val_loss: 0.3705\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3107 - val_loss: 0.3435\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3220 - val_loss: 0.3789\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3185 - val_loss: 0.4307\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3299 - val_loss: 0.4060\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2934 - val_loss: 0.4854\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3135 - val_loss: 0.4294\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2746 - val_loss: 0.4883\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2706 - val_loss: 0.4879\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2812 - val_loss: 0.4109\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2530 - val_loss: 0.4737\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2416 - val_loss: 0.5190\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2471 - val_loss: 0.4252\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2731 - val_loss: 0.3069\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2570 - val_loss: 0.3418\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2324 - val_loss: 0.4271\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2230 - val_loss: 0.3529\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2161 - val_loss: 0.4207\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2189 - val_loss: 0.4949\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2080 - val_loss: 0.4263\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2123 - val_loss: 0.4202\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1988 - val_loss: 0.4372\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1937 - val_loss: 0.4097\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2234 - val_loss: 0.3294\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1998 - val_loss: 0.3786\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1972 - val_loss: 0.3908\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1904 - val_loss: 0.3715\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1814 - val_loss: 0.4522\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1860 - val_loss: 0.4102\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1785 - val_loss: 0.3385\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1784 - val_loss: 0.2574\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1781 - val_loss: 0.3676\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1774 - val_loss: 0.5195\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1764 - val_loss: 0.4010\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1714 - val_loss: 0.4203\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1799 - val_loss: 0.4207\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1713 - val_loss: 0.2592\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1612 - val_loss: 0.3263\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1633 - val_loss: 0.4144\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1594 - val_loss: 0.3031\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1563 - val_loss: 0.3297\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1630 - val_loss: 0.3398\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1598 - val_loss: 0.3323\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1517 - val_loss: 0.3360\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1988 - val_loss: 0.2578\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1635 - val_loss: 0.2995\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1724 - val_loss: 0.2672\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1845 - val_loss: 0.2166\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1496 - val_loss: 0.2169\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1416 - val_loss: 0.2571\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1493 - val_loss: 0.2433\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1405 - val_loss: 0.2202\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1440 - val_loss: 0.2357\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1431 - val_loss: 0.1951\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1401 - val_loss: 0.2167\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1333 - val_loss: 0.2224\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1333 - val_loss: 0.2606\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1634 - val_loss: 0.2399\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1417 - val_loss: 0.2108\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1404 - val_loss: 0.1934\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1344 - val_loss: 0.2991\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1388 - val_loss: 0.2151\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1368 - val_loss: 0.2032\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1346 - val_loss: 0.2201\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1289 - val_loss: 0.1997\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1309 - val_loss: 0.1997\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1303 - val_loss: 0.1878\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1277 - val_loss: 0.2109\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1227 - val_loss: 0.2504\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1206 - val_loss: 0.1979\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1309 - val_loss: 0.2122\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1183 - val_loss: 0.2854\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1170 - val_loss: 0.2145\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1208 - val_loss: 0.2028\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1169 - val_loss: 0.1835\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1177 - val_loss: 0.2321\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1206 - val_loss: 0.1958\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1190 - val_loss: 0.2233\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1233 - val_loss: 0.1982\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1393 - val_loss: 0.1937\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1335 - val_loss: 0.1813\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1150 - val_loss: 0.1958\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1091 - val_loss: 0.1968\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1073 - val_loss: 0.2048\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1061 - val_loss: 0.1959\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1081 - val_loss: 0.1857\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1412 - val_loss: 0.1759\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1232 - val_loss: 0.1748\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1089 - val_loss: 0.1825\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1080 - val_loss: 0.1693\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1050 - val_loss: 0.1928\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1016 - val_loss: 0.1898\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1009 - val_loss: 0.1905\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1003 - val_loss: 0.1916\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1003 - val_loss: 0.1954\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1034 - val_loss: 0.1796\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1065 - val_loss: 0.1977\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0969 - val_loss: 0.2053\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0956 - val_loss: 0.2334\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0961 - val_loss: 0.2060\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0943 - val_loss: 0.1909\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0954 - val_loss: 0.1984\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0952 - val_loss: 0.2092\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0906 - val_loss: 0.2058\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1028 - val_loss: 0.2226\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1010 - val_loss: 0.2179\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0935 - val_loss: 0.2310\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0912 - val_loss: 0.2197\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0893 - val_loss: 0.2083\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0905 - val_loss: 0.2203\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0887 - val_loss: 0.2156\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1642 - val_loss: 0.2740\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1838 - val_loss: 0.1915\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1271 - val_loss: 0.2234\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1124 - val_loss: 0.2129\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1044 - val_loss: 0.2160\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1010 - val_loss: 0.2599\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0999 - val_loss: 0.2083\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0960 - val_loss: 0.2624\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0968 - val_loss: 0.2279\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0994 - val_loss: 0.2214\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0929 - val_loss: 0.2492\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0929 - val_loss: 0.2526\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0883 - val_loss: 0.2240\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0843 - val_loss: 0.2475\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0900 - val_loss: 0.2360\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0892 - val_loss: 0.1931\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0864 - val_loss: 0.1894\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0822 - val_loss: 0.2145\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0815 - val_loss: 0.2228\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0802 - val_loss: 0.2346\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0796 - val_loss: 0.2317\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0780 - val_loss: 0.2571\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0788 - val_loss: 0.2265\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0780 - val_loss: 0.2345\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0780 - val_loss: 0.2166\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0789 - val_loss: 0.2363\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0773 - val_loss: 0.2304\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0751 - val_loss: 0.2280\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0746 - val_loss: 0.2287\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0774 - val_loss: 0.2604\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0855 - val_loss: 0.2341\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0763 - val_loss: 0.2228\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0772 - val_loss: 0.2088\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0737 - val_loss: 0.2089\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0733 - val_loss: 0.2012\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0749 - val_loss: 0.2419\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0732 - val_loss: 0.2307\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0726 - val_loss: 0.2159\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0725 - val_loss: 0.2174\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0720 - val_loss: 0.2058\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0710 - val_loss: 0.2323\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0711 - val_loss: 0.2373\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0705 - val_loss: 0.2278\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0715 - val_loss: 0.2179\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0703 - val_loss: 0.2351\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0710 - val_loss: 0.2324\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0723 - val_loss: 0.2585\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0794 - val_loss: 0.2691\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0795 - val_loss: 0.1953\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0730 - val_loss: 0.2457\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0724 - val_loss: 0.2234\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0692 - val_loss: 0.2346\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0670 - val_loss: 0.2331\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0669 - val_loss: 0.2313\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0666 - val_loss: 0.2319\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0658 - val_loss: 0.2621\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0652 - val_loss: 0.2445\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0659 - val_loss: 0.2314\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0684 - val_loss: 0.2698\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0713 - val_loss: 0.2100\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0717 - val_loss: 0.3043\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0667 - val_loss: 0.2803\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0662 - val_loss: 0.2482\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0654 - val_loss: 0.2521\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0646 - val_loss: 0.2485\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0638 - val_loss: 0.2560\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0640 - val_loss: 0.2260\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0638 - val_loss: 0.2445\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0645 - val_loss: 0.2335\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0624 - val_loss: 0.2795\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 5s 77ms/step - loss: 0.6251 - val_loss: 0.5738\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.5364 - val_loss: 0.5676\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4873 - val_loss: 0.5535\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4400 - val_loss: 0.5986\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4156 - val_loss: 0.5289\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3891 - val_loss: 0.5629\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3493 - val_loss: 0.8294\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3553 - val_loss: 0.6934\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3364 - val_loss: 0.8835\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3271 - val_loss: 0.8484\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3168 - val_loss: 0.7764\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3136 - val_loss: 0.6623\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3177 - val_loss: 0.6942\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3106 - val_loss: 0.6335\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2979 - val_loss: 0.7530\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3026 - val_loss: 0.5872\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3033 - val_loss: 0.6553\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2913 - val_loss: 0.7248\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2819 - val_loss: 0.5947\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2790 - val_loss: 0.5392\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2916 - val_loss: 0.5784\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2791 - val_loss: 0.4522\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2704 - val_loss: 0.4443\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2575 - val_loss: 0.4843\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2551 - val_loss: 0.4290\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2511 - val_loss: 0.5642\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2370 - val_loss: 0.3648\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2379 - val_loss: 0.4570\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2354 - val_loss: 0.4359\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2287 - val_loss: 0.3567\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2362 - val_loss: 0.2785\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2118 - val_loss: 0.3182\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2051 - val_loss: 0.3872\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2184 - val_loss: 0.3647\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1948 - val_loss: 0.2736\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1966 - val_loss: 0.2881\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2000 - val_loss: 0.2889\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1892 - val_loss: 0.2454\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1862 - val_loss: 0.2994\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1856 - val_loss: 0.3147\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1822 - val_loss: 0.2376\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1727 - val_loss: 0.2285\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1689 - val_loss: 0.2727\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1693 - val_loss: 0.2847\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1661 - val_loss: 0.2617\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1654 - val_loss: 0.2532\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1567 - val_loss: 0.2732\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1556 - val_loss: 0.2377\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1615 - val_loss: 0.2506\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1572 - val_loss: 0.2776\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1519 - val_loss: 0.2478\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1505 - val_loss: 0.2708\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1491 - val_loss: 0.2908\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1444 - val_loss: 0.2522\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1475 - val_loss: 0.2816\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1523 - val_loss: 0.2257\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1451 - val_loss: 0.2541\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1418 - val_loss: 0.3481\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1381 - val_loss: 0.3591\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1409 - val_loss: 0.2246\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1394 - val_loss: 0.2784\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1346 - val_loss: 0.2945\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1314 - val_loss: 0.2414\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1326 - val_loss: 0.2935\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1344 - val_loss: 0.2542\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1271 - val_loss: 0.3074\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1310 - val_loss: 0.2893\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1343 - val_loss: 0.2865\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1252 - val_loss: 0.3059\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1210 - val_loss: 0.3607\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1226 - val_loss: 0.2953\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1250 - val_loss: 0.2701\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1236 - val_loss: 0.2654\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1221 - val_loss: 0.2985\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1205 - val_loss: 0.2755\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1205 - val_loss: 0.2944\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1189 - val_loss: 0.2427\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1154 - val_loss: 0.3145\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1116 - val_loss: 0.3177\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1156 - val_loss: 0.2535\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1145 - val_loss: 0.2667\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1086 - val_loss: 0.2745\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1165 - val_loss: 0.3113\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1088 - val_loss: 0.2776\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1090 - val_loss: 0.3933\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1050 - val_loss: 0.3522\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1145 - val_loss: 0.3829\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1097 - val_loss: 0.3836\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1060 - val_loss: 0.3466\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1030 - val_loss: 0.2926\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1022 - val_loss: 0.2859\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1018 - val_loss: 0.3651\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1062 - val_loss: 0.3744\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1037 - val_loss: 0.3217\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1023 - val_loss: 0.2961\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0993 - val_loss: 0.3317\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1009 - val_loss: 0.3738\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0985 - val_loss: 0.3231\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1024 - val_loss: 0.2873\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0980 - val_loss: 0.3189\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0954 - val_loss: 0.3213\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0991 - val_loss: 0.3143\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0953 - val_loss: 0.3686\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0991 - val_loss: 0.3244\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0999 - val_loss: 0.2844\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0989 - val_loss: 0.3401\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0971 - val_loss: 0.4233\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0899 - val_loss: 0.3883\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1021 - val_loss: 0.2392\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1463 - val_loss: 0.3197\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1356 - val_loss: 0.2964\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0984 - val_loss: 0.3671\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0922 - val_loss: 0.3023\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0919 - val_loss: 0.3463\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0886 - val_loss: 0.3580\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0898 - val_loss: 0.3196\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0875 - val_loss: 0.3506\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0868 - val_loss: 0.3230\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0905 - val_loss: 0.3730\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0897 - val_loss: 0.3393\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0865 - val_loss: 0.3279\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0847 - val_loss: 0.3742\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0829 - val_loss: 0.3299\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0839 - val_loss: 0.3151\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0822 - val_loss: 0.3194\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0809 - val_loss: 0.3261\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0804 - val_loss: 0.3751\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0846 - val_loss: 0.3575\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0794 - val_loss: 0.3052\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0811 - val_loss: 0.2854\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0808 - val_loss: 0.3235\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0801 - val_loss: 0.3125\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0781 - val_loss: 0.2964\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0805 - val_loss: 0.3220\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0788 - val_loss: 0.3859\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0775 - val_loss: 0.2841\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0780 - val_loss: 0.2754\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0879 - val_loss: 0.2788\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1004 - val_loss: 0.2845\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0854 - val_loss: 0.2990\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0787 - val_loss: 0.2763\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0746 - val_loss: 0.2803\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0745 - val_loss: 0.2971\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0737 - val_loss: 0.3057\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0727 - val_loss: 0.3055\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0771 - val_loss: 0.2987\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0720 - val_loss: 0.3024\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0710 - val_loss: 0.3283\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0738 - val_loss: 0.2932\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0718 - val_loss: 0.2883\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0712 - val_loss: 0.3240\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0703 - val_loss: 0.2950\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0709 - val_loss: 0.2901\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0705 - val_loss: 0.3319\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0685 - val_loss: 0.2831\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0686 - val_loss: 0.3253\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0697 - val_loss: 0.2746\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0704 - val_loss: 0.2821\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0686 - val_loss: 0.2784\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0666 - val_loss: 0.3064\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0683 - val_loss: 0.2984\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0678 - val_loss: 0.3227\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0659 - val_loss: 0.2818\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0659 - val_loss: 0.2949\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0670 - val_loss: 0.2842\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0702 - val_loss: 0.2723\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0662 - val_loss: 0.3096\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0655 - val_loss: 0.3017\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0642 - val_loss: 0.2894\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0642 - val_loss: 0.2972\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0802 - val_loss: 0.3009\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1468 - val_loss: 0.2677\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0886 - val_loss: 0.2619\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0750 - val_loss: 0.2723\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0683 - val_loss: 0.3061\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0658 - val_loss: 0.3539\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0666 - val_loss: 0.2924\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0653 - val_loss: 0.2775\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0633 - val_loss: 0.2699\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0627 - val_loss: 0.3054\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0626 - val_loss: 0.3073\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0635 - val_loss: 0.3215\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0632 - val_loss: 0.2807\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0617 - val_loss: 0.2942\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0610 - val_loss: 0.3257\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0614 - val_loss: 0.2924\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0606 - val_loss: 0.2858\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0605 - val_loss: 0.3226\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0615 - val_loss: 0.3005\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0601 - val_loss: 0.2917\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0589 - val_loss: 0.2983\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0625 - val_loss: 0.2713\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0605 - val_loss: 0.2666\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0605 - val_loss: 0.2832\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0600 - val_loss: 0.2942\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0593 - val_loss: 0.2740\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0619 - val_loss: 0.2734\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0593 - val_loss: 0.2976\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0606 - val_loss: 0.2789\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0599 - val_loss: 0.2820\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0577 - val_loss: 0.2957\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0576 - val_loss: 0.2863\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0575 - val_loss: 0.3204\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0572 - val_loss: 0.3095\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0576 - val_loss: 0.3250\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0587 - val_loss: 0.3308\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0578 - val_loss: 0.2707\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0580 - val_loss: 0.2788\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0578 - val_loss: 0.3375\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0570 - val_loss: 0.3080\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0571 - val_loss: 0.2848\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0564 - val_loss: 0.3124\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0557 - val_loss: 0.2872\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0562 - val_loss: 0.2836\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0560 - val_loss: 0.3030\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0564 - val_loss: 0.3026\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0557 - val_loss: 0.2968\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0547 - val_loss: 0.2899\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0544 - val_loss: 0.3380\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0550 - val_loss: 0.3133\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0549 - val_loss: 0.2966\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0540 - val_loss: 0.3138\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0542 - val_loss: 0.2533\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0549 - val_loss: 0.2966\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0536 - val_loss: 0.2570\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0568 - val_loss: 0.2369\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0551 - val_loss: 0.2725\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0537 - val_loss: 0.2838\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0538 - val_loss: 0.3184\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0548 - val_loss: 0.2781\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0528 - val_loss: 0.2762\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0529 - val_loss: 0.2786\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0521 - val_loss: 0.3282\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0539 - val_loss: 0.2813\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0529 - val_loss: 0.3061\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0545 - val_loss: 0.2511\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0521 - val_loss: 0.3084\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0524 - val_loss: 0.2681\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0569 - val_loss: 0.2459\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0746 - val_loss: 0.2622\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1453 - val_loss: 0.2349\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0799 - val_loss: 0.2743\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0599 - val_loss: 0.3100\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0560 - val_loss: 0.2972\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0540 - val_loss: 0.3037\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0530 - val_loss: 0.2923\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0523 - val_loss: 0.2803\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0518 - val_loss: 0.3293\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0517 - val_loss: 0.3331\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0518 - val_loss: 0.2854\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0510 - val_loss: 0.3123\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0513 - val_loss: 0.2632\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0509 - val_loss: 0.2933\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0505 - val_loss: 0.2650\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0506 - val_loss: 0.2851\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0500 - val_loss: 0.3007\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0502 - val_loss: 0.2941\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0496 - val_loss: 0.2939\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0501 - val_loss: 0.2809\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0494 - val_loss: 0.2872\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0502 - val_loss: 0.2965\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0502 - val_loss: 0.2663\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0509 - val_loss: 0.2595\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0508 - val_loss: 0.2623\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0506 - val_loss: 0.2452\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0503 - val_loss: 0.3095\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0499 - val_loss: 0.2606\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0499 - val_loss: 0.2580\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0489 - val_loss: 0.2677\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0490 - val_loss: 0.2680\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0486 - val_loss: 0.2819\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0483 - val_loss: 0.2832\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0479 - val_loss: 0.2806\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0482 - val_loss: 0.3102\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0485 - val_loss: 0.2756\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0477 - val_loss: 0.2818\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0480 - val_loss: 0.2367\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0474 - val_loss: 0.2702\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0469 - val_loss: 0.2652\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0479 - val_loss: 0.2958\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0478 - val_loss: 0.2771\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0480 - val_loss: 0.2554\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0476 - val_loss: 0.2784\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0473 - val_loss: 0.2574\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0481 - val_loss: 0.2249\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0484 - val_loss: 0.2571\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0480 - val_loss: 0.2513\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0474 - val_loss: 0.3000\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0466 - val_loss: 0.2786\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0480 - val_loss: 0.2734\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0571 - val_loss: 0.2882\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0570 - val_loss: 0.2468\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0496 - val_loss: 0.2853\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0466 - val_loss: 0.2753\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0463 - val_loss: 0.2892\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0464 - val_loss: 0.3029\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0461 - val_loss: 0.3100\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0465 - val_loss: 0.2616\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0458 - val_loss: 0.2516\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0458 - val_loss: 0.2619\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0473 - val_loss: 0.2520\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0468 - val_loss: 0.2733\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0463 - val_loss: 0.2497\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0449 - val_loss: 0.2511\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0447 - val_loss: 0.2657\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0451 - val_loss: 0.2853\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0452 - val_loss: 0.2744\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0448 - val_loss: 0.2723\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0446 - val_loss: 0.2526\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0470 - val_loss: 0.2759\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0758 - val_loss: 0.1692\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0925 - val_loss: 0.2568\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0661 - val_loss: 0.2460\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0505 - val_loss: 0.2516\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0474 - val_loss: 0.2263\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0464 - val_loss: 0.2619\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0451 - val_loss: 0.2531\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0446 - val_loss: 0.2613\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0442 - val_loss: 0.2655\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0443 - val_loss: 0.2634\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0440 - val_loss: 0.2676\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0438 - val_loss: 0.2817\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0439 - val_loss: 0.2606\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0437 - val_loss: 0.2603\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0436 - val_loss: 0.2734\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0440 - val_loss: 0.2562\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0437 - val_loss: 0.2912\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0433 - val_loss: 0.2975\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0436 - val_loss: 0.2613\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0435 - val_loss: 0.2554\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0433 - val_loss: 0.2801\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0431 - val_loss: 0.2848\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0431 - val_loss: 0.2589\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0429 - val_loss: 0.2526\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0429 - val_loss: 0.2887\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0431 - val_loss: 0.2408\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0428 - val_loss: 0.2509\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0424 - val_loss: 0.2914\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0428 - val_loss: 0.2752\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0425 - val_loss: 0.2431\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0426 - val_loss: 0.2867\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0421 - val_loss: 0.2463\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0425 - val_loss: 0.2780\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0436 - val_loss: 0.2484\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0423 - val_loss: 0.2598\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0423 - val_loss: 0.2429\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0419 - val_loss: 0.2954\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0427 - val_loss: 0.2814\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0423 - val_loss: 0.2693\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0419 - val_loss: 0.2708\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0422 - val_loss: 0.2677\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0426 - val_loss: 0.2771\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0422 - val_loss: 0.2759\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0419 - val_loss: 0.2598\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0421 - val_loss: 0.2645\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0420 - val_loss: 0.2666\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0416 - val_loss: 0.2847\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0415 - val_loss: 0.2468\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0418 - val_loss: 0.2321\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0418 - val_loss: 0.3166\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0420 - val_loss: 0.2675\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0417 - val_loss: 0.2459\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0412 - val_loss: 0.2723\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0416 - val_loss: 0.2492\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0421 - val_loss: 0.2922\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0432 - val_loss: 0.2480\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0465 - val_loss: 0.2393\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0732 - val_loss: 0.2933\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0637 - val_loss: 0.2077\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0592 - val_loss: 0.2270\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0448 - val_loss: 0.2445\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0419 - val_loss: 0.2271\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0412 - val_loss: 0.2323\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0411 - val_loss: 0.2453\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0405 - val_loss: 0.2366\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0403 - val_loss: 0.2467\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0403 - val_loss: 0.2426\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0404 - val_loss: 0.2420\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0400 - val_loss: 0.2420\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0400 - val_loss: 0.2527\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0397 - val_loss: 0.2474\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0398 - val_loss: 0.2444\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0396 - val_loss: 0.2432\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0395 - val_loss: 0.2318\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0395 - val_loss: 0.2587\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0394 - val_loss: 0.2453\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0396 - val_loss: 0.2495\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0395 - val_loss: 0.2530\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0395 - val_loss: 0.2593\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0397 - val_loss: 0.2345\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0396 - val_loss: 0.2453\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0393 - val_loss: 0.2462\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0389 - val_loss: 0.2489\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0391 - val_loss: 0.2564\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0400 - val_loss: 0.2377\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0399 - val_loss: 0.2683\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0395 - val_loss: 0.2374\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0392 - val_loss: 0.2399\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0391 - val_loss: 0.2539\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0395 - val_loss: 0.2619\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0396 - val_loss: 0.2383\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0400 - val_loss: 0.2179\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0393 - val_loss: 0.2455\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0391 - val_loss: 0.2469\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0388 - val_loss: 0.2502\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0387 - val_loss: 0.2396\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0391 - val_loss: 0.2507\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0387 - val_loss: 0.2504\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0399 - val_loss: 0.2438\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0395 - val_loss: 0.2216\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1589 - val_loss: 0.2243\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1414 - val_loss: 0.2227\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0981 - val_loss: 0.1609\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0714 - val_loss: 0.1985\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0605 - val_loss: 0.1973\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0560 - val_loss: 0.2159\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0513 - val_loss: 0.2088\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0482 - val_loss: 0.1984\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0471 - val_loss: 0.2073\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0460 - val_loss: 0.2139\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0455 - val_loss: 0.2105\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0441 - val_loss: 0.2140\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0434 - val_loss: 0.2231\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0428 - val_loss: 0.2123\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0424 - val_loss: 0.2197\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0419 - val_loss: 0.2216\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0417 - val_loss: 0.2224\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0410 - val_loss: 0.2148\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0410 - val_loss: 0.2249\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0410 - val_loss: 0.2297\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0410 - val_loss: 0.2163\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0405 - val_loss: 0.2250\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0406 - val_loss: 0.2238\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0401 - val_loss: 0.2223\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0401 - val_loss: 0.2301\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0400 - val_loss: 0.2339\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0398 - val_loss: 0.2394\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0398 - val_loss: 0.2221\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0394 - val_loss: 0.2221\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0392 - val_loss: 0.2134\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0394 - val_loss: 0.2198\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0390 - val_loss: 0.2212\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0387 - val_loss: 0.2449\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0391 - val_loss: 0.2321\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0388 - val_loss: 0.2256\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0386 - val_loss: 0.2403\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0387 - val_loss: 0.2402\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0385 - val_loss: 0.2511\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0384 - val_loss: 0.2127\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0386 - val_loss: 0.2227\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0385 - val_loss: 0.2392\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0388 - val_loss: 0.2303\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0385 - val_loss: 0.2423\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0382 - val_loss: 0.2266\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0380 - val_loss: 0.2414\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.2527\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0377 - val_loss: 0.2334\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0377 - val_loss: 0.2366\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0376 - val_loss: 0.2350\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0379 - val_loss: 0.2557\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.2331\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.2410\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0379 - val_loss: 0.2341\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0376 - val_loss: 0.2437\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0373 - val_loss: 0.2612\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0371 - val_loss: 0.2563\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0372 - val_loss: 0.2731\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0376 - val_loss: 0.2363\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0380 - val_loss: 0.2437\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0376 - val_loss: 0.2387\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0371 - val_loss: 0.2586\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0376 - val_loss: 0.2633\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0375 - val_loss: 0.2457\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0370 - val_loss: 0.2596\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0373 - val_loss: 0.2416\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0373 - val_loss: 0.2476\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0368 - val_loss: 0.2424\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0367 - val_loss: 0.2686\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0368 - val_loss: 0.2690\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0371 - val_loss: 0.2527\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0366 - val_loss: 0.2650\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0365 - val_loss: 0.2642\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0365 - val_loss: 0.2707\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0371 - val_loss: 0.2429\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0374 - val_loss: 0.3089\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0377 - val_loss: 0.2568\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0377 - val_loss: 0.2739\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0367 - val_loss: 0.2435\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0364 - val_loss: 0.2476\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0361 - val_loss: 0.2786\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0362 - val_loss: 0.2764\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0403 - val_loss: 0.2459\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0400 - val_loss: 0.2883\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0385 - val_loss: 0.2677\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0363 - val_loss: 0.2613\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0359 - val_loss: 0.2774\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0358 - val_loss: 0.2488\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0357 - val_loss: 0.3177\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0359 - val_loss: 0.2654\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0359 - val_loss: 0.2773\n",
      "8/8 [==============================] - 0s 29ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 66ms/step - loss: 0.5675 - val_loss: 0.3921\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4516 - val_loss: 0.3294\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4021 - val_loss: 0.2916\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3901 - val_loss: 0.2785\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4081 - val_loss: 0.3009\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3590 - val_loss: 0.2381\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3766 - val_loss: 0.3902\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3534 - val_loss: 0.2212\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3079 - val_loss: 0.2148\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3094 - val_loss: 0.2848\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2811 - val_loss: 0.1919\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3406 - val_loss: 0.2494\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2853 - val_loss: 0.2061\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2817 - val_loss: 0.2006\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2623 - val_loss: 0.2349\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2499 - val_loss: 0.2243\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2442 - val_loss: 0.2780\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2829 - val_loss: 0.1948\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2658 - val_loss: 0.1910\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2351 - val_loss: 0.1790\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2258 - val_loss: 0.2023\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2297 - val_loss: 0.1979\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2333 - val_loss: 0.1996\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2500 - val_loss: 0.2098\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2252 - val_loss: 0.1783\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2035 - val_loss: 0.2303\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2011 - val_loss: 0.2374\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2092 - val_loss: 0.2122\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1902 - val_loss: 0.1789\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1828 - val_loss: 0.2316\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1874 - val_loss: 0.4012\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1924 - val_loss: 0.2072\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1686 - val_loss: 0.2286\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1886 - val_loss: 0.2013\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2321 - val_loss: 0.2128\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1684 - val_loss: 0.1684\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1644 - val_loss: 0.2183\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1547 - val_loss: 0.1922\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1457 - val_loss: 0.1805\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1407 - val_loss: 0.1757\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1528 - val_loss: 0.1590\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1585 - val_loss: 0.2835\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1689 - val_loss: 0.1866\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1387 - val_loss: 0.2195\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1413 - val_loss: 0.2324\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1269 - val_loss: 0.1756\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1222 - val_loss: 0.1778\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1233 - val_loss: 0.1631\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1277 - val_loss: 0.1543\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1685 - val_loss: 0.1855\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1665 - val_loss: 0.1688\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1281 - val_loss: 0.1690\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1196 - val_loss: 0.1350\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1254 - val_loss: 0.2346\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1220 - val_loss: 0.1584\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1109 - val_loss: 0.1490\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1136 - val_loss: 0.1591\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1983 - val_loss: 0.1920\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1696 - val_loss: 0.1771\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1569 - val_loss: 0.1983\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1303 - val_loss: 0.1595\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1322 - val_loss: 0.1514\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1189 - val_loss: 0.1954\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1500 - val_loss: 0.2108\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1770 - val_loss: 0.2118\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1431 - val_loss: 0.1501\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1211 - val_loss: 0.1354\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1111 - val_loss: 0.1408\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1169 - val_loss: 0.1517\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1202 - val_loss: 0.1976\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1201 - val_loss: 0.1609\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1044 - val_loss: 0.1502\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1052 - val_loss: 0.1532\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1058 - val_loss: 0.1318\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1014 - val_loss: 0.1920\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0958 - val_loss: 0.1371\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0960 - val_loss: 0.1682\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0904 - val_loss: 0.1595\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1034 - val_loss: 0.1812\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0978 - val_loss: 0.1536\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0928 - val_loss: 0.1544\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0869 - val_loss: 0.1677\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0898 - val_loss: 0.1542\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1016 - val_loss: 0.1744\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0893 - val_loss: 0.1455\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0854 - val_loss: 0.1674\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0817 - val_loss: 0.1522\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0760 - val_loss: 0.1417\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0784 - val_loss: 0.1426\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3373 - val_loss: 0.5019\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4041 - val_loss: 0.2813\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3421 - val_loss: 0.3164\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3195 - val_loss: 0.2651\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3410 - val_loss: 0.2766\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3123 - val_loss: 0.3148\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2886 - val_loss: 0.3237\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2651 - val_loss: 0.3109\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1965 - val_loss: 0.2215\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1771 - val_loss: 0.1608\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1759 - val_loss: 0.1545\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 4s 66ms/step - loss: 0.6452 - val_loss: 0.4186\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4664 - val_loss: 0.3765\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3927 - val_loss: 0.4005\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3780 - val_loss: 0.4335\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3453 - val_loss: 0.3969\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3210 - val_loss: 0.3519\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3226 - val_loss: 0.3264\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3035 - val_loss: 0.3069\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2839 - val_loss: 0.3302\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2757 - val_loss: 0.2435\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2838 - val_loss: 0.2780\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2784 - val_loss: 0.2280\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2600 - val_loss: 0.2419\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2615 - val_loss: 0.2834\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2686 - val_loss: 0.2753\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2645 - val_loss: 0.2806\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2548 - val_loss: 0.3141\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2591 - val_loss: 0.3074\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2426 - val_loss: 0.3146\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2490 - val_loss: 0.2702\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2384 - val_loss: 0.2787\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2352 - val_loss: 0.2869\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2427 - val_loss: 0.3388\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2371 - val_loss: 0.3971\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2269 - val_loss: 0.3027\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2433 - val_loss: 0.4775\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2317 - val_loss: 0.2907\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2304 - val_loss: 0.4682\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2170 - val_loss: 0.3068\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2211 - val_loss: 0.2581\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2261 - val_loss: 0.3018\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2193 - val_loss: 0.4246\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2172 - val_loss: 0.4218\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2236 - val_loss: 0.4393\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2090 - val_loss: 0.3005\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2103 - val_loss: 0.2860\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2112 - val_loss: 0.4256\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2274 - val_loss: 0.3181\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2126 - val_loss: 0.4418\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2033 - val_loss: 0.3798\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2008 - val_loss: 0.3667\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1991 - val_loss: 0.4433\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3171 - val_loss: 0.2173\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2833 - val_loss: 0.2066\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2367 - val_loss: 0.2230\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2343 - val_loss: 0.3322\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2356 - val_loss: 0.2385\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2346 - val_loss: 0.2422\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2039 - val_loss: 0.3155\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2022 - val_loss: 0.2056\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1963 - val_loss: 0.2626\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1874 - val_loss: 0.2992\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1844 - val_loss: 0.1603\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1881 - val_loss: 0.1944\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2020 - val_loss: 0.2001\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1982 - val_loss: 0.3766\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1757 - val_loss: 0.3722\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1623 - val_loss: 0.3545\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1585 - val_loss: 0.2740\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1452 - val_loss: 0.3932\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1499 - val_loss: 0.1946\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1458 - val_loss: 0.1992\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1439 - val_loss: 0.1476\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1435 - val_loss: 0.1664\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1414 - val_loss: 0.2719\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1365 - val_loss: 0.1742\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1417 - val_loss: 0.2977\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1322 - val_loss: 0.2363\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1296 - val_loss: 0.3525\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1265 - val_loss: 0.2548\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1289 - val_loss: 0.2135\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1536 - val_loss: 0.3411\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3503 - val_loss: 0.3345\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2417 - val_loss: 0.2271\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2288 - val_loss: 0.2939\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3018 - val_loss: 0.5196\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2214 - val_loss: 0.1846\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2180 - val_loss: 0.2264\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1554 - val_loss: 0.2044\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1425 - val_loss: 0.1407\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1293 - val_loss: 0.1408\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1331 - val_loss: 0.1535\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1549 - val_loss: 0.2164\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1390 - val_loss: 0.1931\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1472 - val_loss: 0.2937\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1521 - val_loss: 0.1308\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1396 - val_loss: 0.1276\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1184 - val_loss: 0.1387\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1122 - val_loss: 0.1401\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1086 - val_loss: 0.1474\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1058 - val_loss: 0.2077\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1063 - val_loss: 0.1477\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1291 - val_loss: 0.1503\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1249 - val_loss: 0.2166\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1062 - val_loss: 0.1386\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1032 - val_loss: 0.1329\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1027 - val_loss: 0.1237\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1056 - val_loss: 0.1229\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1034 - val_loss: 0.1363\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0961 - val_loss: 0.1560\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0965 - val_loss: 0.1389\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0957 - val_loss: 0.1101\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0943 - val_loss: 0.1365\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0985 - val_loss: 0.1264\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1016 - val_loss: 0.1264\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0950 - val_loss: 0.1284\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0878 - val_loss: 0.1237\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0927 - val_loss: 0.1134\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0971 - val_loss: 0.2058\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0876 - val_loss: 0.1082\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0928 - val_loss: 0.1865\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1286 - val_loss: 0.1219\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1006 - val_loss: 0.2510\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0875 - val_loss: 0.1250\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0903 - val_loss: 0.2044\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0849 - val_loss: 0.1495\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0818 - val_loss: 0.1617\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0807 - val_loss: 0.1908\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0979 - val_loss: 0.2066\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1630 - val_loss: 0.1095\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1142 - val_loss: 0.1791\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0944 - val_loss: 0.2543\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0968 - val_loss: 0.2447\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1053 - val_loss: 0.1263\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0893 - val_loss: 0.3522\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0831 - val_loss: 0.1882\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0800 - val_loss: 0.2414\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0764 - val_loss: 0.1571\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0815 - val_loss: 0.1064\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0762 - val_loss: 0.1695\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0807 - val_loss: 0.1074\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0829 - val_loss: 0.1555\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0778 - val_loss: 0.2107\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0731 - val_loss: 0.1323\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0903 - val_loss: 0.1173\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1275 - val_loss: 0.2593\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0854 - val_loss: 0.1355\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2274 - val_loss: 0.3391\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3809 - val_loss: 0.3219\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2931 - val_loss: 0.4005\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2691 - val_loss: 0.2621\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1961 - val_loss: 0.1941\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1502 - val_loss: 0.1405\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1412 - val_loss: 0.3638\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1529 - val_loss: 0.1109\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1144 - val_loss: 0.2721\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1018 - val_loss: 0.2379\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0931 - val_loss: 0.1748\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0840 - val_loss: 0.2373\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0970 - val_loss: 0.1110\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1275 - val_loss: 0.2618\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0879 - val_loss: 0.1642\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0817 - val_loss: 0.1952\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0752 - val_loss: 0.1250\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0719 - val_loss: 0.1645\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0771 - val_loss: 0.1633\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0721 - val_loss: 0.2138\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0710 - val_loss: 0.1837\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0725 - val_loss: 0.2430\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0710 - val_loss: 0.1131\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0864 - val_loss: 0.1522\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0727 - val_loss: 0.1461\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0709 - val_loss: 0.1478\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0664 - val_loss: 0.1541\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0641 - val_loss: 0.1348\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0823 - val_loss: 0.1217\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0862 - val_loss: 0.1407\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0675 - val_loss: 0.1265\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0689 - val_loss: 0.1365\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0643 - val_loss: 0.1466\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0631 - val_loss: 0.1259\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0620 - val_loss: 0.1250\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0638 - val_loss: 0.1136\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0606 - val_loss: 0.1433\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0610 - val_loss: 0.1384\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0617 - val_loss: 0.1108\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0607 - val_loss: 0.1445\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0589 - val_loss: 0.1382\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0591 - val_loss: 0.2665\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0575 - val_loss: 0.2330\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0592 - val_loss: 0.2738\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0609 - val_loss: 0.2660\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0583 - val_loss: 0.2722\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0947 - val_loss: 0.1116\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1052 - val_loss: 0.2810\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0745 - val_loss: 0.1499\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0629 - val_loss: 0.2232\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0605 - val_loss: 0.1953\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0567 - val_loss: 0.1575\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0576 - val_loss: 0.2777\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0577 - val_loss: 0.2602\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0559 - val_loss: 0.1819\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0548 - val_loss: 0.1439\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0555 - val_loss: 0.1876\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0586 - val_loss: 0.1208\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1308 - val_loss: 0.2242\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1293 - val_loss: 0.1295\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1024 - val_loss: 0.1227\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0728 - val_loss: 0.1602\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0628 - val_loss: 0.1218\n",
      "8/8 [==============================] - 0s 29ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 4s 67ms/step - loss: 0.5670 - val_loss: 0.3398\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4692 - val_loss: 0.3980\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4328 - val_loss: 0.3435\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4053 - val_loss: 0.2899\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3905 - val_loss: 0.3519\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3649 - val_loss: 0.2460\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3522 - val_loss: 0.2539\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2972 - val_loss: 0.2656\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3093 - val_loss: 0.2402\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2659 - val_loss: 0.2501\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2628 - val_loss: 0.2028\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2548 - val_loss: 0.1841\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2330 - val_loss: 0.2354\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2591 - val_loss: 0.2131\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2337 - val_loss: 0.2026\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2369 - val_loss: 0.2160\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2199 - val_loss: 0.2307\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2358 - val_loss: 0.2789\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2010 - val_loss: 0.1848\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2008 - val_loss: 0.1849\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1799 - val_loss: 0.2208\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1896 - val_loss: 0.2243\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1773 - val_loss: 0.1849\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1717 - val_loss: 0.1778\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1685 - val_loss: 0.2086\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1568 - val_loss: 0.1715\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1448 - val_loss: 0.1303\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1403 - val_loss: 0.1843\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1625 - val_loss: 0.1710\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1695 - val_loss: 0.1802\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1540 - val_loss: 0.1769\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1422 - val_loss: 0.1638\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1551 - val_loss: 0.1761\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1469 - val_loss: 0.2166\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1845 - val_loss: 0.2030\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2795 - val_loss: 0.2606\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2079 - val_loss: 0.1757\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1952 - val_loss: 0.1578\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1714 - val_loss: 0.1360\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1567 - val_loss: 0.1516\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1412 - val_loss: 0.1773\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1395 - val_loss: 0.2071\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1403 - val_loss: 0.1478\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1331 - val_loss: 0.1481\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1256 - val_loss: 0.1605\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1230 - val_loss: 0.1459\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1405 - val_loss: 0.1428\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1300 - val_loss: 0.1404\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1387 - val_loss: 0.1505\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1249 - val_loss: 0.1618\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2122 - val_loss: 0.1424\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1511 - val_loss: 0.1363\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1651 - val_loss: 0.1671\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1570 - val_loss: 0.1767\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1400 - val_loss: 0.1670\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1269 - val_loss: 0.1513\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1254 - val_loss: 0.1279\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1162 - val_loss: 0.1993\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1125 - val_loss: 0.1170\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1084 - val_loss: 0.1380\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1126 - val_loss: 0.1509\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1093 - val_loss: 0.1470\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3327 - val_loss: 0.1923\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2461 - val_loss: 0.1497\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1749 - val_loss: 0.1383\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1638 - val_loss: 0.1489\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1445 - val_loss: 0.1629\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1376 - val_loss: 0.1622\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1359 - val_loss: 0.1450\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1239 - val_loss: 0.1671\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1335 - val_loss: 0.1496\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1247 - val_loss: 0.1522\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1328 - val_loss: 0.1363\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1246 - val_loss: 0.1433\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1247 - val_loss: 0.1603\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1555 - val_loss: 0.1598\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1229 - val_loss: 0.1519\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1160 - val_loss: 0.1625\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1400 - val_loss: 0.1641\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1406 - val_loss: 0.1388\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1359 - val_loss: 0.1489\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1223 - val_loss: 0.1385\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1127 - val_loss: 0.1530\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1071 - val_loss: 0.1471\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1110 - val_loss: 0.1365\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1217 - val_loss: 0.1405\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1113 - val_loss: 0.1559\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1032 - val_loss: 0.1407\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1005 - val_loss: 0.1670\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1049 - val_loss: 0.1704\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0990 - val_loss: 0.1524\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1086 - val_loss: 0.1661\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1041 - val_loss: 0.1325\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1051 - val_loss: 0.1376\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1105 - val_loss: 0.1639\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1051 - val_loss: 0.1394\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0993 - val_loss: 0.1417\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0994 - val_loss: 0.1501\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1091 - val_loss: 0.1463\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1039 - val_loss: 0.1596\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1029 - val_loss: 0.1550\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1016 - val_loss: 0.1461\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0974 - val_loss: 0.1525\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1011 - val_loss: 0.1583\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0931 - val_loss: 0.2159\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0910 - val_loss: 0.1369\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0996 - val_loss: 0.1425\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1003 - val_loss: 0.1932\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1002 - val_loss: 0.1996\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0910 - val_loss: 0.1473\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0874 - val_loss: 0.1375\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0912 - val_loss: 0.1326\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0969 - val_loss: 0.1282\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1009 - val_loss: 0.1468\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0960 - val_loss: 0.1531\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0888 - val_loss: 0.1531\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0953 - val_loss: 0.1513\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0901 - val_loss: 0.1553\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0916 - val_loss: 0.1313\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1955 - val_loss: 0.1545\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2691 - val_loss: 0.1473\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1792 - val_loss: 0.1801\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1418 - val_loss: 0.1259\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1381 - val_loss: 0.1442\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1358 - val_loss: 0.1565\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1230 - val_loss: 0.1384\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1048 - val_loss: 0.1254\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1042 - val_loss: 0.1218\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1002 - val_loss: 0.1320\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1004 - val_loss: 0.1126\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1021 - val_loss: 0.1345\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0955 - val_loss: 0.1429\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0959 - val_loss: 0.1341\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0932 - val_loss: 0.1372\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0889 - val_loss: 0.1242\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0882 - val_loss: 0.1283\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0884 - val_loss: 0.1252\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0873 - val_loss: 0.1515\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0851 - val_loss: 0.1225\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0880 - val_loss: 0.1388\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0926 - val_loss: 0.1145\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0879 - val_loss: 0.1345\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0851 - val_loss: 0.1542\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0849 - val_loss: 0.1497\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0853 - val_loss: 0.1671\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0882 - val_loss: 0.1619\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0855 - val_loss: 0.1192\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0819 - val_loss: 0.1310\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0816 - val_loss: 0.1300\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0822 - val_loss: 0.1198\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0826 - val_loss: 0.1512\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0896 - val_loss: 0.1540\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0859 - val_loss: 0.1420\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0790 - val_loss: 0.1580\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0861 - val_loss: 0.1390\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0859 - val_loss: 0.1251\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0831 - val_loss: 0.1817\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1106 - val_loss: 0.3853\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2223 - val_loss: 0.1300\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1640 - val_loss: 0.1395\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1364 - val_loss: 0.1180\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1047 - val_loss: 0.1068\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0955 - val_loss: 0.1401\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0880 - val_loss: 0.1182\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0911 - val_loss: 0.1186\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1007 - val_loss: 0.1212\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0891 - val_loss: 0.1257\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0886 - val_loss: 0.1201\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0885 - val_loss: 0.1264\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0867 - val_loss: 0.1197\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0810 - val_loss: 0.2003\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0905 - val_loss: 0.1357\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0853 - val_loss: 0.1533\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0805 - val_loss: 0.1351\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0778 - val_loss: 0.1323\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0778 - val_loss: 0.1478\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0853 - val_loss: 0.1727\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0841 - val_loss: 0.1386\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0820 - val_loss: 0.1407\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0781 - val_loss: 0.1574\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0808 - val_loss: 0.1173\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0750 - val_loss: 0.1236\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0746 - val_loss: 0.1438\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0739 - val_loss: 0.1334\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0755 - val_loss: 0.1270\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0778 - val_loss: 0.1330\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0776 - val_loss: 0.1169\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0929 - val_loss: 0.1280\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1090 - val_loss: 0.1282\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1022 - val_loss: 0.1474\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2591 - val_loss: 0.2646\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2518 - val_loss: 0.2077\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2047 - val_loss: 0.1224\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1410 - val_loss: 0.1718\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1152 - val_loss: 0.1899\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0967 - val_loss: 0.1327\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1029 - val_loss: 0.1280\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1022 - val_loss: 0.1499\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0882 - val_loss: 0.1542\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0839 - val_loss: 0.1355\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0808 - val_loss: 0.1448\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0785 - val_loss: 0.1395\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0790 - val_loss: 0.1339\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0790 - val_loss: 0.1268\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0758 - val_loss: 0.1533\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0758 - val_loss: 0.1450\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0774 - val_loss: 0.1262\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0729 - val_loss: 0.1851\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0757 - val_loss: 0.1549\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0723 - val_loss: 0.1276\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0721 - val_loss: 0.1860\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0719 - val_loss: 0.1357\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0721 - val_loss: 0.1873\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0721 - val_loss: 0.1251\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0722 - val_loss: 0.1709\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0735 - val_loss: 0.1383\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0723 - val_loss: 0.1976\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0730 - val_loss: 0.1325\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0749 - val_loss: 0.1777\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0719 - val_loss: 0.1246\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0724 - val_loss: 0.1555\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0749 - val_loss: 0.1372\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0788 - val_loss: 0.1644\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0820 - val_loss: 0.1280\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0760 - val_loss: 0.1089\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0734 - val_loss: 0.1306\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0785 - val_loss: 0.1768\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0773 - val_loss: 0.1621\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0738 - val_loss: 0.1504\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0692 - val_loss: 0.1382\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0698 - val_loss: 0.1574\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0691 - val_loss: 0.1443\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0702 - val_loss: 0.1421\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0679 - val_loss: 0.1569\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0664 - val_loss: 0.1663\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0655 - val_loss: 0.1425\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0717 - val_loss: 0.1655\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0800 - val_loss: 0.1296\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0698 - val_loss: 0.1240\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0685 - val_loss: 0.1453\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1108 - val_loss: 0.1471\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1060 - val_loss: 0.1262\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0820 - val_loss: 0.1284\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1192 - val_loss: 0.1214\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0850 - val_loss: 0.1831\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0794 - val_loss: 0.1496\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0746 - val_loss: 0.1281\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0691 - val_loss: 0.1398\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0681 - val_loss: 0.1467\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0656 - val_loss: 0.1650\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0710 - val_loss: 0.1300\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0674 - val_loss: 0.1422\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0650 - val_loss: 0.1803\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0666 - val_loss: 0.1206\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0715 - val_loss: 0.1336\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0641 - val_loss: 0.1775\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0638 - val_loss: 0.1249\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0639 - val_loss: 0.2083\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0664 - val_loss: 0.1272\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0648 - val_loss: 0.1462\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0677 - val_loss: 0.1228\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0653 - val_loss: 0.1229\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0637 - val_loss: 0.1596\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0645 - val_loss: 0.1250\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0646 - val_loss: 0.1777\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0653 - val_loss: 0.1416\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0681 - val_loss: 0.1352\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0691 - val_loss: 0.1238\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0650 - val_loss: 0.1572\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1581 - val_loss: 0.1172\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1218 - val_loss: 0.1024\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1236 - val_loss: 0.1159\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0970 - val_loss: 0.1255\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1800 - val_loss: 0.2167\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2339 - val_loss: 0.3253\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1573 - val_loss: 0.1261\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1240 - val_loss: 0.1440\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1199 - val_loss: 0.1215\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1035 - val_loss: 0.1580\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0921 - val_loss: 0.1318\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0853 - val_loss: 0.1422\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0848 - val_loss: 0.1437\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0874 - val_loss: 0.1111\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0806 - val_loss: 0.1236\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0730 - val_loss: 0.1359\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0735 - val_loss: 0.1429\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0758 - val_loss: 0.1616\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0695 - val_loss: 0.1438\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0708 - val_loss: 0.1547\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0719 - val_loss: 0.1660\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0704 - val_loss: 0.1180\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0709 - val_loss: 0.1369\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0678 - val_loss: 0.1201\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0677 - val_loss: 0.1670\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0657 - val_loss: 0.1539\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0647 - val_loss: 0.1361\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0693 - val_loss: 0.1450\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0644 - val_loss: 0.1548\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0640 - val_loss: 0.1370\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0647 - val_loss: 0.1560\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0642 - val_loss: 0.1470\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0630 - val_loss: 0.1276\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0675 - val_loss: 0.1264\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0629 - val_loss: 0.1429\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0632 - val_loss: 0.1333\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0782 - val_loss: 0.1560\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0716 - val_loss: 0.1152\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0668 - val_loss: 0.1184\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0621 - val_loss: 0.1514\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0614 - val_loss: 0.1707\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0611 - val_loss: 0.1289\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0617 - val_loss: 0.1782\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0625 - val_loss: 0.1566\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0597 - val_loss: 0.1459\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0678 - val_loss: 0.1732\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0646 - val_loss: 0.2068\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0709 - val_loss: 0.1385\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0660 - val_loss: 0.1539\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0653 - val_loss: 0.1543\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0614 - val_loss: 0.1345\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0590 - val_loss: 0.1633\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0600 - val_loss: 0.1497\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0593 - val_loss: 0.1374\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0641 - val_loss: 0.1231\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0673 - val_loss: 0.1665\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0624 - val_loss: 0.1633\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0603 - val_loss: 0.1598\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0595 - val_loss: 0.1547\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0572 - val_loss: 0.1659\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0589 - val_loss: 0.1435\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0564 - val_loss: 0.1614\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0564 - val_loss: 0.1734\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0586 - val_loss: 0.1669\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0597 - val_loss: 0.1433\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0569 - val_loss: 0.1509\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0562 - val_loss: 0.1718\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0549 - val_loss: 0.1722\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0569 - val_loss: 0.1770\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0629 - val_loss: 0.1434\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0983 - val_loss: 0.1160\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0971 - val_loss: 0.1270\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1086 - val_loss: 0.2215\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0813 - val_loss: 0.1461\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0693 - val_loss: 0.1469\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0644 - val_loss: 0.1235\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0641 - val_loss: 0.1581\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0623 - val_loss: 0.1677\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0695 - val_loss: 0.1651\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0625 - val_loss: 0.1512\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0592 - val_loss: 0.1334\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0589 - val_loss: 0.1284\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0571 - val_loss: 0.1551\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0557 - val_loss: 0.1458\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0566 - val_loss: 0.1419\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0553 - val_loss: 0.1412\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0555 - val_loss: 0.1635\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0544 - val_loss: 0.1468\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0550 - val_loss: 0.1411\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0588 - val_loss: 0.1264\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0553 - val_loss: 0.1454\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0549 - val_loss: 0.1533\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0535 - val_loss: 0.1423\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0574 - val_loss: 0.1667\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0548 - val_loss: 0.1390\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0540 - val_loss: 0.1493\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0544 - val_loss: 0.1649\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0523 - val_loss: 0.1430\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0524 - val_loss: 0.1651\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0520 - val_loss: 0.1563\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0554 - val_loss: 0.1411\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0784 - val_loss: 0.1688\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0636 - val_loss: 0.1776\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0575 - val_loss: 0.1341\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0542 - val_loss: 0.1816\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0536 - val_loss: 0.1448\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0532 - val_loss: 0.1444\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0514 - val_loss: 0.1514\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0529 - val_loss: 0.2058\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0676 - val_loss: 0.1466\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0753 - val_loss: 0.2019\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0728 - val_loss: 0.1429\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0592 - val_loss: 0.1541\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0545 - val_loss: 0.1468\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0601 - val_loss: 0.1718\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0658 - val_loss: 0.1431\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0574 - val_loss: 0.1434\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0524 - val_loss: 0.1379\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0517 - val_loss: 0.1456\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0524 - val_loss: 0.1693\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0536 - val_loss: 0.1226\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0518 - val_loss: 0.1580\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0521 - val_loss: 0.1491\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0524 - val_loss: 0.1824\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1109 - val_loss: 0.1486\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1560 - val_loss: 0.1538\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1285 - val_loss: 0.1348\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0892 - val_loss: 0.1784\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0759 - val_loss: 0.1325\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0634 - val_loss: 0.1429\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0582 - val_loss: 0.1436\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0564 - val_loss: 0.1513\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0546 - val_loss: 0.1445\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0556 - val_loss: 0.1573\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0536 - val_loss: 0.1653\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0537 - val_loss: 0.1724\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0543 - val_loss: 0.1484\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0562 - val_loss: 0.1829\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0552 - val_loss: 0.1812\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0516 - val_loss: 0.1461\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0518 - val_loss: 0.1235\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0542 - val_loss: 0.1449\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0514 - val_loss: 0.1578\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0536 - val_loss: 0.1452\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0521 - val_loss: 0.1555\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0500 - val_loss: 0.1594\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0516 - val_loss: 0.1628\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0500 - val_loss: 0.1589\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0493 - val_loss: 0.1451\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0503 - val_loss: 0.1516\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0555 - val_loss: 0.1558\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0550 - val_loss: 0.1769\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0548 - val_loss: 0.1462\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0560 - val_loss: 0.1544\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0515 - val_loss: 0.1645\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0503 - val_loss: 0.1455\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0529 - val_loss: 0.1785\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0627 - val_loss: 0.1636\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0619 - val_loss: 0.1414\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0544 - val_loss: 0.1596\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0501 - val_loss: 0.1457\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0495 - val_loss: 0.1784\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0491 - val_loss: 0.1511\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0482 - val_loss: 0.1558\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0489 - val_loss: 0.1446\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0489 - val_loss: 0.1594\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0489 - val_loss: 0.1512\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0503 - val_loss: 0.1802\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0484 - val_loss: 0.1652\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0473 - val_loss: 0.1489\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0548 - val_loss: 0.1271\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0559 - val_loss: 0.1637\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0491 - val_loss: 0.1695\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0486 - val_loss: 0.1572\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0501 - val_loss: 0.2538\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0935 - val_loss: 0.2278\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1009 - val_loss: 0.1386\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1346 - val_loss: 0.1258\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0868 - val_loss: 0.1272\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1302 - val_loss: 0.2182\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1342 - val_loss: 0.1640\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0891 - val_loss: 0.1172\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0695 - val_loss: 0.1217\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0641 - val_loss: 0.1221\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0595 - val_loss: 0.1461\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0604 - val_loss: 0.1372\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0541 - val_loss: 0.1359\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0530 - val_loss: 0.1484\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0520 - val_loss: 0.1478\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0512 - val_loss: 0.1659\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0512 - val_loss: 0.1399\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0507 - val_loss: 0.1496\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0508 - val_loss: 0.1699\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0503 - val_loss: 0.1375\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0497 - val_loss: 0.1527\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0485 - val_loss: 0.1721\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0496 - val_loss: 0.1632\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0494 - val_loss: 0.1503\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0505 - val_loss: 0.2139\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0518 - val_loss: 0.1577\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0521 - val_loss: 0.1614\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0486 - val_loss: 0.1577\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0481 - val_loss: 0.1481\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0491 - val_loss: 0.1760\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0501 - val_loss: 0.1638\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0527 - val_loss: 0.1914\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0562 - val_loss: 0.1661\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0529 - val_loss: 0.1497\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0482 - val_loss: 0.1576\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0468 - val_loss: 0.1572\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0494 - val_loss: 0.2396\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1116 - val_loss: 0.1574\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1195 - val_loss: 0.1134\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1017 - val_loss: 0.1365\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0683 - val_loss: 0.1116\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0711 - val_loss: 0.1218\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1520 - val_loss: 0.1276\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0779 - val_loss: 0.1312\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0724 - val_loss: 0.1456\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0601 - val_loss: 0.1529\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0557 - val_loss: 0.1616\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0537 - val_loss: 0.1594\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0516 - val_loss: 0.1704\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0516 - val_loss: 0.1427\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0503 - val_loss: 0.1637\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0482 - val_loss: 0.1688\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0506 - val_loss: 0.1455\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0487 - val_loss: 0.1732\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0485 - val_loss: 0.1792\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0477 - val_loss: 0.1522\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0477 - val_loss: 0.1660\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 66ms/step - loss: 0.5503 - val_loss: 0.3974\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3880 - val_loss: 0.2902\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3516 - val_loss: 0.2647\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3368 - val_loss: 0.2673\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3119 - val_loss: 0.2687\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3577 - val_loss: 0.3602\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3184 - val_loss: 0.1986\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2797 - val_loss: 0.2065\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2699 - val_loss: 0.2302\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2876 - val_loss: 0.2355\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3129 - val_loss: 0.2136\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2786 - val_loss: 0.2401\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2639 - val_loss: 0.2302\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2884 - val_loss: 0.2137\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2524 - val_loss: 0.2107\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2923 - val_loss: 0.2743\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2568 - val_loss: 0.2119\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2420 - val_loss: 0.2489\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2443 - val_loss: 0.2165\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2582 - val_loss: 0.2225\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2334 - val_loss: 0.2007\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2317 - val_loss: 0.2257\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2317 - val_loss: 0.1906\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2349 - val_loss: 0.2234\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2409 - val_loss: 0.2001\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2305 - val_loss: 0.2461\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2219 - val_loss: 0.2233\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2388 - val_loss: 0.2059\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2277 - val_loss: 0.2470\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2116 - val_loss: 0.2155\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2048 - val_loss: 0.2036\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2051 - val_loss: 0.2535\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2178 - val_loss: 0.2284\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1929 - val_loss: 0.2800\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2032 - val_loss: 0.2520\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2406 - val_loss: 0.2202\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2016 - val_loss: 0.1854\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1796 - val_loss: 0.2302\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1983 - val_loss: 0.1927\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1866 - val_loss: 0.1946\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1847 - val_loss: 0.2726\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1768 - val_loss: 0.1914\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1611 - val_loss: 0.1735\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1657 - val_loss: 0.2239\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3412 - val_loss: 0.2847\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3310 - val_loss: 0.2386\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2936 - val_loss: 0.2717\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2621 - val_loss: 0.2414\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2230 - val_loss: 0.1722\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2072 - val_loss: 0.1633\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1847 - val_loss: 0.1828\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1791 - val_loss: 0.1931\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1861 - val_loss: 0.1720\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1794 - val_loss: 0.2357\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1612 - val_loss: 0.1888\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1566 - val_loss: 0.1528\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1486 - val_loss: 0.2092\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1850 - val_loss: 0.2340\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1573 - val_loss: 0.2341\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1624 - val_loss: 0.1869\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1504 - val_loss: 0.1927\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1561 - val_loss: 0.2643\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1750 - val_loss: 0.1936\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1486 - val_loss: 0.2124\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1502 - val_loss: 0.1855\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1349 - val_loss: 0.2666\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1357 - val_loss: 0.1662\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1541 - val_loss: 0.1558\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1434 - val_loss: 0.1740\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1383 - val_loss: 0.1641\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1526 - val_loss: 0.1939\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1607 - val_loss: 0.1843\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1536 - val_loss: 0.1486\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1307 - val_loss: 0.1691\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1293 - val_loss: 0.1491\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1275 - val_loss: 0.1799\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1392 - val_loss: 0.1935\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1918 - val_loss: 0.1595\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1482 - val_loss: 0.1921\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1313 - val_loss: 0.1943\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1341 - val_loss: 0.1928\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1270 - val_loss: 0.1557\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1218 - val_loss: 0.1744\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1207 - val_loss: 0.1598\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1252 - val_loss: 0.2276\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1376 - val_loss: 0.2597\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1428 - val_loss: 0.1560\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1403 - val_loss: 0.2178\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1228 - val_loss: 0.2105\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1526 - val_loss: 0.3416\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2647 - val_loss: 0.1805\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1722 - val_loss: 0.1789\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1390 - val_loss: 0.1733\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1310 - val_loss: 0.1762\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1323 - val_loss: 0.1809\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1278 - val_loss: 0.2034\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1378 - val_loss: 0.1604\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1347 - val_loss: 0.1225\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1269 - val_loss: 0.1426\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1202 - val_loss: 0.2468\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 5s 77ms/step - loss: 0.5756 - val_loss: 0.4032\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4456 - val_loss: 0.4551\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3939 - val_loss: 0.4535\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3598 - val_loss: 0.4443\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3366 - val_loss: 0.4118\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3138 - val_loss: 0.3490\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3096 - val_loss: 0.2816\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3057 - val_loss: 0.3325\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2845 - val_loss: 0.2617\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3014 - val_loss: 0.3304\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2871 - val_loss: 0.2526\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2992 - val_loss: 0.3545\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2760 - val_loss: 0.3431\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3077 - val_loss: 0.3246\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2592 - val_loss: 0.2180\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2581 - val_loss: 0.2041\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2596 - val_loss: 0.2155\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2727 - val_loss: 0.2704\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2578 - val_loss: 0.2417\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2411 - val_loss: 0.2468\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2579 - val_loss: 0.2571\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2494 - val_loss: 0.2981\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2632 - val_loss: 0.2000\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2614 - val_loss: 0.2410\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2319 - val_loss: 0.2376\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2195 - val_loss: 0.2294\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2583 - val_loss: 0.2231\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2115 - val_loss: 0.3463\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2189 - val_loss: 0.3640\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2120 - val_loss: 0.2343\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2162 - val_loss: 0.2772\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1920 - val_loss: 0.1849\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1874 - val_loss: 0.2271\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1888 - val_loss: 0.3170\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2161 - val_loss: 0.4105\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2067 - val_loss: 0.3081\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2109 - val_loss: 0.3192\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1957 - val_loss: 0.2071\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1795 - val_loss: 0.4390\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1916 - val_loss: 0.2265\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1829 - val_loss: 0.2530\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1825 - val_loss: 0.2615\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1695 - val_loss: 0.3120\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1536 - val_loss: 0.4963\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1600 - val_loss: 0.3637\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1369 - val_loss: 0.2932\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1288 - val_loss: 0.3511\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1404 - val_loss: 0.1673\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1544 - val_loss: 0.2639\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1359 - val_loss: 0.3352\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1389 - val_loss: 0.2031\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1215 - val_loss: 0.2702\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1198 - val_loss: 0.4137\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1185 - val_loss: 0.2564\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1133 - val_loss: 0.4590\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1106 - val_loss: 0.3162\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1282 - val_loss: 0.2315\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1399 - val_loss: 0.5055\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1219 - val_loss: 0.3162\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1160 - val_loss: 0.3935\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1326 - val_loss: 0.4374\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1226 - val_loss: 0.1952\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1185 - val_loss: 0.5167\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1023 - val_loss: 0.4146\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0992 - val_loss: 0.3252\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0987 - val_loss: 0.3341\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1013 - val_loss: 0.1714\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1458 - val_loss: 0.1251\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2522 - val_loss: 0.2299\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2444 - val_loss: 0.3724\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1939 - val_loss: 0.2494\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1623 - val_loss: 0.2443\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1835 - val_loss: 0.2873\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1290 - val_loss: 0.2542\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1102 - val_loss: 0.2967\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1014 - val_loss: 0.3260\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0971 - val_loss: 0.1777\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1080 - val_loss: 0.1443\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1201 - val_loss: 0.1658\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0966 - val_loss: 0.2611\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0930 - val_loss: 0.2378\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0923 - val_loss: 0.1637\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0914 - val_loss: 0.2132\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0852 - val_loss: 0.1835\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0839 - val_loss: 0.2194\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0967 - val_loss: 0.2037\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0852 - val_loss: 0.2218\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0814 - val_loss: 0.1826\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0803 - val_loss: 0.1919\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0808 - val_loss: 0.1791\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0811 - val_loss: 0.1681\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0786 - val_loss: 0.1911\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0785 - val_loss: 0.2383\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0756 - val_loss: 0.2985\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0805 - val_loss: 0.2333\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0754 - val_loss: 0.1819\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0749 - val_loss: 0.2087\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0719 - val_loss: 0.1544\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0727 - val_loss: 0.2083\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0720 - val_loss: 0.3503\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1406 - val_loss: 0.2240\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2984 - val_loss: 0.2561\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2140 - val_loss: 0.1652\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1540 - val_loss: 0.1582\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2217 - val_loss: 0.1579\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2050 - val_loss: 0.3398\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1270 - val_loss: 0.3627\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1003 - val_loss: 0.1606\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2115 - val_loss: 0.2137\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1384 - val_loss: 0.2702\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1062 - val_loss: 0.2761\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1148 - val_loss: 0.2126\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1935 - val_loss: 0.2801\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1416 - val_loss: 0.1573\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1044 - val_loss: 0.1694\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0864 - val_loss: 0.1419\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0832 - val_loss: 0.1470\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1118 - val_loss: 0.1544\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0849 - val_loss: 0.1312\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0907 - val_loss: 0.1450\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0804 - val_loss: 0.2045\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0743 - val_loss: 0.1772\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0705 - val_loss: 0.2105\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0701 - val_loss: 0.1824\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0688 - val_loss: 0.1916\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0673 - val_loss: 0.1527\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1228 - val_loss: 0.3716\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4144 - val_loss: 0.3161\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3787 - val_loss: 0.3249\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2771 - val_loss: 0.1869\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2433 - val_loss: 0.2141\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2403 - val_loss: 0.1730\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1649 - val_loss: 0.1979\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1370 - val_loss: 0.4073\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1297 - val_loss: 0.1763\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0970 - val_loss: 0.2335\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0996 - val_loss: 0.2832\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0846 - val_loss: 0.1734\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0888 - val_loss: 0.1416\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0773 - val_loss: 0.1935\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0750 - val_loss: 0.1534\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0716 - val_loss: 0.1810\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0692 - val_loss: 0.1863\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0708 - val_loss: 0.1512\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0670 - val_loss: 0.1662\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0663 - val_loss: 0.1771\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0647 - val_loss: 0.1672\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0640 - val_loss: 0.1618\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0650 - val_loss: 0.2058\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0657 - val_loss: 0.1743\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0664 - val_loss: 0.1968\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0638 - val_loss: 0.3122\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0642 - val_loss: 0.1341\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0621 - val_loss: 0.1532\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0587 - val_loss: 0.1574\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0598 - val_loss: 0.1561\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0604 - val_loss: 0.1723\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0597 - val_loss: 0.2622\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3118 - val_loss: 0.2517\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2738 - val_loss: 0.2362\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2763 - val_loss: 0.2407\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2559 - val_loss: 0.2253\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2496 - val_loss: 0.3099\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2364 - val_loss: 0.1964\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1931 - val_loss: 0.1652\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1521 - val_loss: 0.1845\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1213 - val_loss: 0.2079\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0934 - val_loss: 0.1737\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1004 - val_loss: 0.1282\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1442 - val_loss: 0.2424\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1375 - val_loss: 0.1976\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0842 - val_loss: 0.1495\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0720 - val_loss: 0.1413\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0723 - val_loss: 0.1661\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0709 - val_loss: 0.2326\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0638 - val_loss: 0.1451\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0637 - val_loss: 0.1416\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0612 - val_loss: 0.1471\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0594 - val_loss: 0.1546\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0575 - val_loss: 0.1704\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0561 - val_loss: 0.1514\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0559 - val_loss: 0.1528\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0553 - val_loss: 0.1670\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0554 - val_loss: 0.1646\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0551 - val_loss: 0.1275\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0561 - val_loss: 0.1542\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0572 - val_loss: 0.1769\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0551 - val_loss: 0.1347\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0572 - val_loss: 0.1539\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0532 - val_loss: 0.1670\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0512 - val_loss: 0.1581\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0512 - val_loss: 0.1789\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0510 - val_loss: 0.1547\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0521 - val_loss: 0.1618\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0515 - val_loss: 0.1555\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0499 - val_loss: 0.1659\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0500 - val_loss: 0.1489\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0493 - val_loss: 0.1419\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0486 - val_loss: 0.1583\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0487 - val_loss: 0.1303\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 4s 67ms/step - loss: 0.6546 - val_loss: 0.4876\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4761 - val_loss: 0.3895\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3883 - val_loss: 0.3201\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3267 - val_loss: 0.3588\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3063 - val_loss: 0.3045\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3178 - val_loss: 0.3604\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2800 - val_loss: 0.2688\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2891 - val_loss: 0.4128\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2818 - val_loss: 0.3643\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2958 - val_loss: 0.2981\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3550 - val_loss: 0.2991\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2725 - val_loss: 0.2172\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2615 - val_loss: 0.2089\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2538 - val_loss: 0.3367\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2275 - val_loss: 0.2240\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3090 - val_loss: 0.2253\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2683 - val_loss: 0.2156\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2582 - val_loss: 0.2320\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2825 - val_loss: 0.3929\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2517 - val_loss: 0.2158\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2416 - val_loss: 0.2382\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2515 - val_loss: 0.2819\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2268 - val_loss: 0.3161\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1986 - val_loss: 0.3808\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2003 - val_loss: 0.2643\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2015 - val_loss: 0.3075\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1884 - val_loss: 0.2325\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1841 - val_loss: 0.3162\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1767 - val_loss: 0.2269\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1995 - val_loss: 0.2024\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2145 - val_loss: 0.2449\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1874 - val_loss: 0.2704\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1746 - val_loss: 0.2964\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1681 - val_loss: 0.2790\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2223 - val_loss: 0.2868\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2372 - val_loss: 0.2557\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1807 - val_loss: 0.2766\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2238 - val_loss: 0.2089\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2052 - val_loss: 0.2211\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1929 - val_loss: 0.1525\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1825 - val_loss: 0.1933\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1507 - val_loss: 0.3004\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1391 - val_loss: 0.3493\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1322 - val_loss: 0.4099\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2015 - val_loss: 0.2122\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1907 - val_loss: 0.1930\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1758 - val_loss: 0.2029\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1397 - val_loss: 0.2232\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1334 - val_loss: 0.2148\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1219 - val_loss: 0.1832\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1258 - val_loss: 0.2547\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1089 - val_loss: 0.3935\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1562 - val_loss: 0.2328\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1170 - val_loss: 0.2397\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1050 - val_loss: 0.2231\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1023 - val_loss: 0.3266\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1016 - val_loss: 0.1802\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1001 - val_loss: 0.2669\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1006 - val_loss: 0.2307\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1012 - val_loss: 0.2116\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1055 - val_loss: 0.4560\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1037 - val_loss: 0.2754\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0942 - val_loss: 0.5229\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1088 - val_loss: 0.1931\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1267 - val_loss: 0.1938\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1193 - val_loss: 0.2208\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0994 - val_loss: 0.2249\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0970 - val_loss: 0.1975\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1015 - val_loss: 0.2521\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0957 - val_loss: 0.2484\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0917 - val_loss: 0.1697\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0883 - val_loss: 0.2694\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0873 - val_loss: 0.1877\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0829 - val_loss: 0.2509\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0880 - val_loss: 0.3637\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0848 - val_loss: 0.2544\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0852 - val_loss: 0.1550\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0884 - val_loss: 0.1620\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0902 - val_loss: 0.2684\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0852 - val_loss: 0.4367\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0882 - val_loss: 0.3587\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0778 - val_loss: 0.1999\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0784 - val_loss: 0.2890\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0868 - val_loss: 0.2243\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0750 - val_loss: 0.3782\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0744 - val_loss: 0.2336\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0706 - val_loss: 0.2278\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0696 - val_loss: 0.2273\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0713 - val_loss: 0.2791\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1755 - val_loss: 0.1492\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1355 - val_loss: 0.1387\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1295 - val_loss: 0.1133\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1147 - val_loss: 0.2131\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0871 - val_loss: 0.2028\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0785 - val_loss: 0.2332\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0721 - val_loss: 0.2946\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0686 - val_loss: 0.2252\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0678 - val_loss: 0.3067\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0922 - val_loss: 0.2175\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0784 - val_loss: 0.1909\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0686 - val_loss: 0.1726\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0685 - val_loss: 0.3110\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0640 - val_loss: 0.2794\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0636 - val_loss: 0.2363\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0652 - val_loss: 0.1726\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0661 - val_loss: 0.1912\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0686 - val_loss: 0.1647\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0826 - val_loss: 0.3981\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0843 - val_loss: 0.2056\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0656 - val_loss: 0.1638\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0605 - val_loss: 0.1513\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0590 - val_loss: 0.1569\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0575 - val_loss: 0.1725\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0571 - val_loss: 0.2621\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0576 - val_loss: 0.3676\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2761 - val_loss: 0.3227\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1742 - val_loss: 0.3069\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1350 - val_loss: 0.1866\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1365 - val_loss: 0.1256\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1153 - val_loss: 0.1534\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0976 - val_loss: 0.1455\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0933 - val_loss: 0.1180\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0944 - val_loss: 0.1298\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0719 - val_loss: 0.1800\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0656 - val_loss: 0.1686\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0631 - val_loss: 0.1643\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0607 - val_loss: 0.1774\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0614 - val_loss: 0.1350\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0629 - val_loss: 0.1593\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0591 - val_loss: 0.1641\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0566 - val_loss: 0.2100\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0558 - val_loss: 0.1482\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0558 - val_loss: 0.1941\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0548 - val_loss: 0.1931\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0591 - val_loss: 0.1367\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0566 - val_loss: 0.1896\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0566 - val_loss: 0.1534\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0528 - val_loss: 0.1512\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0518 - val_loss: 0.1690\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0512 - val_loss: 0.1722\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0940 - val_loss: 0.1879\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1839 - val_loss: 0.2473\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1547 - val_loss: 0.3151\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1369 - val_loss: 0.2332\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1361 - val_loss: 0.1570\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0834 - val_loss: 0.1661\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0740 - val_loss: 0.1337\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0630 - val_loss: 0.2981\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0621 - val_loss: 0.1700\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0693 - val_loss: 0.1997\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0570 - val_loss: 0.1528\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0541 - val_loss: 0.1846\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0517 - val_loss: 0.1772\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0501 - val_loss: 0.1791\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0497 - val_loss: 0.2515\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0491 - val_loss: 0.1961\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0482 - val_loss: 0.1988\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0475 - val_loss: 0.2228\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0473 - val_loss: 0.2156\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0469 - val_loss: 0.2072\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0465 - val_loss: 0.2104\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0476 - val_loss: 0.1951\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0459 - val_loss: 0.2312\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0480 - val_loss: 0.1904\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0458 - val_loss: 0.1442\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0457 - val_loss: 0.2448\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0449 - val_loss: 0.2600\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0438 - val_loss: 0.2103\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0438 - val_loss: 0.2195\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0436 - val_loss: 0.3224\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0429 - val_loss: 0.2420\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0426 - val_loss: 0.2613\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0428 - val_loss: 0.2593\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0425 - val_loss: 0.2137\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0424 - val_loss: 0.3093\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0422 - val_loss: 0.2789\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0417 - val_loss: 0.3133\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0414 - val_loss: 0.2033\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0406 - val_loss: 0.2016\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0407 - val_loss: 0.2467\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0412 - val_loss: 0.2879\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0423 - val_loss: 0.2636\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0412 - val_loss: 0.3462\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0466 - val_loss: 0.1409\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0503 - val_loss: 0.2357\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0499 - val_loss: 0.1266\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0771 - val_loss: 0.1976\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0865 - val_loss: 0.1829\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2546 - val_loss: 0.1943\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1880 - val_loss: 0.1598\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1501 - val_loss: 0.1545\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1257 - val_loss: 0.1628\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0929 - val_loss: 0.1715\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0724 - val_loss: 0.1692\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0595 - val_loss: 0.1540\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0536 - val_loss: 0.1388\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0511 - val_loss: 0.1525\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0479 - val_loss: 0.1508\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0457 - val_loss: 0.1707\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0451 - val_loss: 0.1708\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0440 - val_loss: 0.1717\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0431 - val_loss: 0.1981\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0427 - val_loss: 0.1437\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0422 - val_loss: 0.1601\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0422 - val_loss: 0.1664\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0410 - val_loss: 0.1688\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0408 - val_loss: 0.1631\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0406 - val_loss: 0.1536\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0406 - val_loss: 0.1853\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0400 - val_loss: 0.1838\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0395 - val_loss: 0.1801\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0392 - val_loss: 0.1683\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0389 - val_loss: 0.1713\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0388 - val_loss: 0.1719\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0385 - val_loss: 0.1602\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0382 - val_loss: 0.1715\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0379 - val_loss: 0.1611\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0377 - val_loss: 0.1744\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0377 - val_loss: 0.1878\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0372 - val_loss: 0.1824\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0380 - val_loss: 0.1678\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0380 - val_loss: 0.1604\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0372 - val_loss: 0.1686\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0372 - val_loss: 0.1764\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0370 - val_loss: 0.2294\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0369 - val_loss: 0.1805\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0363 - val_loss: 0.1742\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0362 - val_loss: 0.1643\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0361 - val_loss: 0.2088\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0358 - val_loss: 0.2194\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0358 - val_loss: 0.1723\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0359 - val_loss: 0.1698\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0355 - val_loss: 0.1873\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0352 - val_loss: 0.1767\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0370 - val_loss: 0.5509\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1115 - val_loss: 0.1253\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1754 - val_loss: 0.1616\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2104 - val_loss: 0.1815\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1295 - val_loss: 0.1575\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0853 - val_loss: 0.1213\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0612 - val_loss: 0.1159\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0531 - val_loss: 0.1264\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0456 - val_loss: 0.1252\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0425 - val_loss: 0.1358\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0409 - val_loss: 0.1384\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0397 - val_loss: 0.1410\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0391 - val_loss: 0.1317\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0384 - val_loss: 0.1359\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0375 - val_loss: 0.1374\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0371 - val_loss: 0.1428\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0368 - val_loss: 0.1462\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0365 - val_loss: 0.1441\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0363 - val_loss: 0.1497\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0360 - val_loss: 0.1387\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0357 - val_loss: 0.1408\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0355 - val_loss: 0.1393\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0352 - val_loss: 0.1571\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0349 - val_loss: 0.1413\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0347 - val_loss: 0.1427\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0348 - val_loss: 0.1378\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0345 - val_loss: 0.1411\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0341 - val_loss: 0.1560\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0342 - val_loss: 0.1482\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0337 - val_loss: 0.1512\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0336 - val_loss: 0.1491\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0336 - val_loss: 0.1472\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0334 - val_loss: 0.1657\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0332 - val_loss: 0.1602\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0332 - val_loss: 0.1489\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0329 - val_loss: 0.1623\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0328 - val_loss: 0.1614\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0326 - val_loss: 0.1897\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0326 - val_loss: 0.1648\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0325 - val_loss: 0.1706\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0326 - val_loss: 0.1617\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0325 - val_loss: 0.1535\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0325 - val_loss: 0.1694\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0322 - val_loss: 0.1636\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0322 - val_loss: 0.1601\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0320 - val_loss: 0.1622\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0323 - val_loss: 0.1758\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0319 - val_loss: 0.1858\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1345 - val_loss: 0.1650\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1915 - val_loss: 0.1538\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1432 - val_loss: 0.1550\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0950 - val_loss: 0.1238\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0664 - val_loss: 0.1444\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0528 - val_loss: 0.1517\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0469 - val_loss: 0.1557\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0432 - val_loss: 0.1493\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0412 - val_loss: 0.1774\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0400 - val_loss: 0.1744\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0380 - val_loss: 0.1672\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0372 - val_loss: 0.1743\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0368 - val_loss: 0.1702\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0361 - val_loss: 0.1777\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0351 - val_loss: 0.1666\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0346 - val_loss: 0.1573\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0346 - val_loss: 0.1769\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0343 - val_loss: 0.1890\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0342 - val_loss: 0.1779\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0338 - val_loss: 0.1738\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0333 - val_loss: 0.1875\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0333 - val_loss: 0.1624\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0331 - val_loss: 0.2017\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0332 - val_loss: 0.1709\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0330 - val_loss: 0.1741\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0324 - val_loss: 0.1610\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0321 - val_loss: 0.1811\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0320 - val_loss: 0.2052\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0319 - val_loss: 0.2086\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0317 - val_loss: 0.1803\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0315 - val_loss: 0.1876\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0314 - val_loss: 0.1906\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0314 - val_loss: 0.1828\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0312 - val_loss: 0.1775\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0314 - val_loss: 0.1563\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0318 - val_loss: 0.1625\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0312 - val_loss: 0.1735\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0308 - val_loss: 0.1922\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0310 - val_loss: 0.1784\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0357 - val_loss: 0.1500\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2891 - val_loss: 0.1835\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1798 - val_loss: 0.1542\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1619 - val_loss: 0.1437\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1084 - val_loss: 0.1149\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1276 - val_loss: 0.2375\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0962 - val_loss: 0.1190\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0655 - val_loss: 0.1722\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0586 - val_loss: 0.1678\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0470 - val_loss: 0.1565\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0417 - val_loss: 0.1371\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0406 - val_loss: 0.1479\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0379 - val_loss: 0.1511\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0370 - val_loss: 0.1610\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0357 - val_loss: 0.1548\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0352 - val_loss: 0.1548\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0345 - val_loss: 0.1580\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0338 - val_loss: 0.1697\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0334 - val_loss: 0.1714\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0330 - val_loss: 0.1700\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0328 - val_loss: 0.1734\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0324 - val_loss: 0.1745\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0324 - val_loss: 0.1870\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0323 - val_loss: 0.1697\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0322 - val_loss: 0.1731\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0316 - val_loss: 0.1698\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0394 - val_loss: 0.1563\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0339 - val_loss: 0.1748\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0321 - val_loss: 0.1663\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0318 - val_loss: 0.1885\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0313 - val_loss: 0.1903\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0361 - val_loss: 0.1404\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1701 - val_loss: 0.2473\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2892 - val_loss: 0.2849\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2513 - val_loss: 0.3419\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2104 - val_loss: 0.2116\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1772 - val_loss: 0.2225\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1486 - val_loss: 0.2474\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1316 - val_loss: 0.2209\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1134 - val_loss: 0.1986\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1747 - val_loss: 0.3066\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1456 - val_loss: 0.2100\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1167 - val_loss: 0.3433\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0970 - val_loss: 0.2168\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0850 - val_loss: 0.2065\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0816 - val_loss: 0.5522\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0714 - val_loss: 0.2923\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0608 - val_loss: 0.1933\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0551 - val_loss: 0.2972\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0516 - val_loss: 0.3468\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0489 - val_loss: 0.2907\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0471 - val_loss: 0.2857\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0478 - val_loss: 0.4212\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0497 - val_loss: 0.2692\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0472 - val_loss: 0.2361\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0453 - val_loss: 0.3370\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0423 - val_loss: 0.2485\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0418 - val_loss: 0.3448\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0409 - val_loss: 0.3048\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0397 - val_loss: 0.2658\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0430 - val_loss: 0.1224\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0926 - val_loss: 0.1542\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0943 - val_loss: 0.2431\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0616 - val_loss: 0.2910\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0508 - val_loss: 0.3189\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0430 - val_loss: 0.3312\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0404 - val_loss: 0.3525\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0390 - val_loss: 0.3789\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0382 - val_loss: 0.3306\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0371 - val_loss: 0.4126\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0365 - val_loss: 0.3870\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0360 - val_loss: 0.3987\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0358 - val_loss: 0.4264\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0354 - val_loss: 0.3938\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0350 - val_loss: 0.3845\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0348 - val_loss: 0.3342\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0367 - val_loss: 0.4196\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0350 - val_loss: 0.4027\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0342 - val_loss: 0.3601\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0339 - val_loss: 0.3536\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0337 - val_loss: 0.3666\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0334 - val_loss: 0.3531\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0333 - val_loss: 0.3651\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0333 - val_loss: 0.3445\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0331 - val_loss: 0.3417\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0328 - val_loss: 0.3499\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0325 - val_loss: 0.3513\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0324 - val_loss: 0.3426\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0321 - val_loss: 0.2903\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0320 - val_loss: 0.4369\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0317 - val_loss: 0.3760\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0316 - val_loss: 0.3729\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0315 - val_loss: 0.3983\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0313 - val_loss: 0.3716\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0314 - val_loss: 0.4380\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0311 - val_loss: 0.3734\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0310 - val_loss: 0.3744\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0308 - val_loss: 0.4020\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0307 - val_loss: 0.3763\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0306 - val_loss: 0.4163\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0306 - val_loss: 0.3375\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0303 - val_loss: 0.3807\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0303 - val_loss: 0.3633\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0302 - val_loss: 0.3844\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0301 - val_loss: 0.3827\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0299 - val_loss: 0.3468\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0299 - val_loss: 0.4002\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0301 - val_loss: 0.3409\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0300 - val_loss: 0.3328\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0298 - val_loss: 0.3322\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0298 - val_loss: 0.3578\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0295 - val_loss: 0.3999\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0296 - val_loss: 0.3696\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0298 - val_loss: 0.3707\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0302 - val_loss: 0.5006\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0301 - val_loss: 0.4882\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0297 - val_loss: 0.3778\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0292 - val_loss: 0.2811\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0290 - val_loss: 0.4229\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0289 - val_loss: 0.3481\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0289 - val_loss: 0.3652\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0289 - val_loss: 0.3191\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0287 - val_loss: 0.2089\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0286 - val_loss: 0.2910\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0285 - val_loss: 0.2499\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0284 - val_loss: 0.3938\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0284 - val_loss: 0.3028\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0285 - val_loss: 0.3403\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0284 - val_loss: 0.2504\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0284 - val_loss: 0.2713\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0281 - val_loss: 0.2846\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0282 - val_loss: 0.3453\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0283 - val_loss: 0.2359\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0282 - val_loss: 0.2900\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0282 - val_loss: 0.2621\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0282 - val_loss: 0.2723\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0281 - val_loss: 0.2990\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0280 - val_loss: 0.3460\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0401 - val_loss: 0.5319\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1503 - val_loss: 0.1262\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1383 - val_loss: 0.1322\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0894 - val_loss: 0.1620\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0621 - val_loss: 0.2694\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0429 - val_loss: 0.1702\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0389 - val_loss: 0.1683\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0368 - val_loss: 0.1603\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0340 - val_loss: 0.1719\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0329 - val_loss: 0.1757\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0320 - val_loss: 0.1689\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0313 - val_loss: 0.1830\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0310 - val_loss: 0.1733\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0361 - val_loss: 0.1646\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0320 - val_loss: 0.1922\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0305 - val_loss: 0.1771\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0299 - val_loss: 0.1926\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0296 - val_loss: 0.1832\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0294 - val_loss: 0.1866\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0291 - val_loss: 0.1923\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0289 - val_loss: 0.1890\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0287 - val_loss: 0.1957\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0286 - val_loss: 0.1951\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0284 - val_loss: 0.1967\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0283 - val_loss: 0.1904\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0282 - val_loss: 0.1927\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0282 - val_loss: 0.1877\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0280 - val_loss: 0.1897\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0279 - val_loss: 0.1843\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0279 - val_loss: 0.1916\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0277 - val_loss: 0.1964\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0278 - val_loss: 0.1869\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0275 - val_loss: 0.1862\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0275 - val_loss: 0.1943\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0272 - val_loss: 0.1897\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0273 - val_loss: 0.1964\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0271 - val_loss: 0.1987\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0271 - val_loss: 0.2258\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0270 - val_loss: 0.2043\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0269 - val_loss: 0.1974\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0268 - val_loss: 0.1952\n",
      "8/8 [==============================] - 0s 29ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 68ms/step - loss: 0.5381 - val_loss: 0.3461\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4153 - val_loss: 0.2729\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3778 - val_loss: 0.3417\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3435 - val_loss: 0.3934\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3073 - val_loss: 0.4209\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3022 - val_loss: 0.4236\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3074 - val_loss: 0.3384\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2808 - val_loss: 0.2780\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3771 - val_loss: 0.4954\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3204 - val_loss: 0.3397\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2864 - val_loss: 0.3604\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2725 - val_loss: 0.4922\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2888 - val_loss: 0.3223\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2770 - val_loss: 0.2662\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2754 - val_loss: 0.3934\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2646 - val_loss: 0.3033\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2553 - val_loss: 0.3523\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2637 - val_loss: 0.2987\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2490 - val_loss: 0.3256\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2612 - val_loss: 0.2315\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2610 - val_loss: 0.2609\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2604 - val_loss: 0.2174\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2605 - val_loss: 0.3078\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2379 - val_loss: 0.3924\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2343 - val_loss: 0.3237\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2555 - val_loss: 0.2538\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2272 - val_loss: 0.2526\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2294 - val_loss: 0.2116\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2380 - val_loss: 0.3205\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2105 - val_loss: 0.3589\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1986 - val_loss: 0.2395\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1941 - val_loss: 0.2033\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1929 - val_loss: 0.3854\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1764 - val_loss: 0.4817\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2040 - val_loss: 0.2761\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2247 - val_loss: 0.1961\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2080 - val_loss: 0.2670\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1894 - val_loss: 0.2890\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1918 - val_loss: 0.2565\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1609 - val_loss: 0.2548\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1491 - val_loss: 0.1826\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1489 - val_loss: 0.2930\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1465 - val_loss: 0.2831\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1502 - val_loss: 0.2029\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1448 - val_loss: 0.1773\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1396 - val_loss: 0.1900\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1341 - val_loss: 0.2664\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1870 - val_loss: 0.2036\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2235 - val_loss: 0.1874\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1714 - val_loss: 0.1652\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1489 - val_loss: 0.1471\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1584 - val_loss: 0.3412\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1380 - val_loss: 0.2078\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1409 - val_loss: 0.2115\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1288 - val_loss: 0.2576\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1245 - val_loss: 0.2283\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1219 - val_loss: 0.1710\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1225 - val_loss: 0.2118\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1160 - val_loss: 0.2008\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1287 - val_loss: 0.1550\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1417 - val_loss: 0.2176\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1257 - val_loss: 0.2573\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1220 - val_loss: 0.3551\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1271 - val_loss: 0.2106\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1148 - val_loss: 0.3204\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1116 - val_loss: 0.2109\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1106 - val_loss: 0.3155\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1054 - val_loss: 0.2263\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1159 - val_loss: 0.1712\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1153 - val_loss: 0.1975\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1210 - val_loss: 0.2268\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1295 - val_loss: 0.3237\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1355 - val_loss: 0.1773\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1245 - val_loss: 0.1567\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1144 - val_loss: 0.1555\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1192 - val_loss: 0.2381\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1249 - val_loss: 0.1840\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1172 - val_loss: 0.2272\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1208 - val_loss: 0.2371\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1053 - val_loss: 0.2666\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1025 - val_loss: 0.1959\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1009 - val_loss: 0.2438\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1014 - val_loss: 0.2854\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1017 - val_loss: 0.1946\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1063 - val_loss: 0.2198\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1183 - val_loss: 0.3198\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1102 - val_loss: 0.1869\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1082 - val_loss: 0.1725\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1041 - val_loss: 0.1997\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1305 - val_loss: 0.1781\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1645 - val_loss: 0.1936\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1248 - val_loss: 0.1611\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1048 - val_loss: 0.2481\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1093 - val_loss: 0.1856\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1157 - val_loss: 0.3517\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1121 - val_loss: 0.2585\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1003 - val_loss: 0.2055\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0979 - val_loss: 0.1682\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0968 - val_loss: 0.2339\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0993 - val_loss: 0.2573\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 4s 70ms/step - loss: 0.6112 - val_loss: 0.3750\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4574 - val_loss: 0.3308\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4149 - val_loss: 0.3788\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3860 - val_loss: 0.4378\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3871 - val_loss: 0.3074\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3864 - val_loss: 0.3368\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3688 - val_loss: 0.3395\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3326 - val_loss: 0.2737\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2869 - val_loss: 0.3260\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2854 - val_loss: 0.2555\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2725 - val_loss: 0.2135\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2570 - val_loss: 0.2301\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2735 - val_loss: 0.2778\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2688 - val_loss: 0.2397\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2486 - val_loss: 0.2199\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2254 - val_loss: 0.1461\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2096 - val_loss: 0.2622\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2167 - val_loss: 0.2056\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2147 - val_loss: 0.1950\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1788 - val_loss: 0.2157\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2015 - val_loss: 0.1848\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1981 - val_loss: 0.2142\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1867 - val_loss: 0.2018\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1679 - val_loss: 0.1972\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1701 - val_loss: 0.1748\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1672 - val_loss: 0.1420\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1526 - val_loss: 0.2083\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1733 - val_loss: 0.2365\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1921 - val_loss: 0.2412\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1608 - val_loss: 0.1248\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1521 - val_loss: 0.1663\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1944 - val_loss: 0.2906\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1756 - val_loss: 0.1554\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1531 - val_loss: 0.2707\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1619 - val_loss: 0.1508\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1625 - val_loss: 0.1392\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1351 - val_loss: 0.1328\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1618 - val_loss: 0.1410\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1649 - val_loss: 0.1374\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1346 - val_loss: 0.1469\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1324 - val_loss: 0.1365\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1287 - val_loss: 0.2151\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1460 - val_loss: 0.1580\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1479 - val_loss: 0.1522\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1389 - val_loss: 0.1263\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1237 - val_loss: 0.1131\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1267 - val_loss: 0.1103\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1331 - val_loss: 0.1202\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1237 - val_loss: 0.1394\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1982 - val_loss: 0.2160\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2540 - val_loss: 0.1643\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1726 - val_loss: 0.1619\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1493 - val_loss: 0.1275\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1399 - val_loss: 0.1324\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1774 - val_loss: 0.1567\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1457 - val_loss: 0.1193\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1242 - val_loss: 0.1512\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1340 - val_loss: 0.2536\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1292 - val_loss: 0.1502\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1415 - val_loss: 0.1723\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1256 - val_loss: 0.1605\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1224 - val_loss: 0.1260\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1117 - val_loss: 0.1418\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1105 - val_loss: 0.1585\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1135 - val_loss: 0.1368\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1118 - val_loss: 0.1285\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1325 - val_loss: 0.1132\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1275 - val_loss: 0.1067\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1096 - val_loss: 0.1315\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1084 - val_loss: 0.1240\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1068 - val_loss: 0.1601\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1133 - val_loss: 0.1442\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2069 - val_loss: 0.1546\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1802 - val_loss: 0.1600\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1856 - val_loss: 0.1497\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1397 - val_loss: 0.1463\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1182 - val_loss: 0.1336\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1107 - val_loss: 0.1540\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1154 - val_loss: 0.1189\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1171 - val_loss: 0.1398\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1102 - val_loss: 0.1351\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1076 - val_loss: 0.1374\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1089 - val_loss: 0.1239\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1031 - val_loss: 0.1125\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1010 - val_loss: 0.1085\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1034 - val_loss: 0.1342\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1139 - val_loss: 0.1359\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1033 - val_loss: 0.1040\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0980 - val_loss: 0.1178\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1115 - val_loss: 0.1561\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1281 - val_loss: 0.1086\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1114 - val_loss: 0.1089\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1012 - val_loss: 0.1237\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0993 - val_loss: 0.1460\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0959 - val_loss: 0.1049\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0950 - val_loss: 0.1072\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0924 - val_loss: 0.1111\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0947 - val_loss: 0.1378\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0918 - val_loss: 0.1115\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0915 - val_loss: 0.1320\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0915 - val_loss: 0.1224\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0917 - val_loss: 0.1289\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0910 - val_loss: 0.1239\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0944 - val_loss: 0.1213\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0929 - val_loss: 0.1078\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0932 - val_loss: 0.1204\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0900 - val_loss: 0.1076\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0941 - val_loss: 0.1135\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1062 - val_loss: 0.1005\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1009 - val_loss: 0.1079\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0938 - val_loss: 0.1046\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0990 - val_loss: 0.1200\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0996 - val_loss: 0.1270\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1067 - val_loss: 0.1359\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0945 - val_loss: 0.1430\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0882 - val_loss: 0.1195\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0882 - val_loss: 0.1125\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0910 - val_loss: 0.1344\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0917 - val_loss: 0.1096\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0852 - val_loss: 0.1098\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0867 - val_loss: 0.1362\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0838 - val_loss: 0.1336\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0905 - val_loss: 0.1077\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0835 - val_loss: 0.1039\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0874 - val_loss: 0.1065\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0832 - val_loss: 0.1075\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0867 - val_loss: 0.1040\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0809 - val_loss: 0.1391\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0844 - val_loss: 0.1245\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1197 - val_loss: 0.1134\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1122 - val_loss: 0.1665\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0992 - val_loss: 0.1052\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1043 - val_loss: 0.1739\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1356 - val_loss: 0.1239\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1733 - val_loss: 0.2264\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2951 - val_loss: 0.5697\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4140 - val_loss: 0.3509\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3718 - val_loss: 0.3076\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3447 - val_loss: 0.2930\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3387 - val_loss: 0.3466\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3170 - val_loss: 0.3033\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2958 - val_loss: 0.2757\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2965 - val_loss: 0.2805\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2746 - val_loss: 0.2982\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2620 - val_loss: 0.2298\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2513 - val_loss: 0.2816\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2422 - val_loss: 0.3172\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2248 - val_loss: 0.3859\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2487 - val_loss: 0.2037\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2232 - val_loss: 0.2287\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1974 - val_loss: 0.2589\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1894 - val_loss: 0.2131\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1553 - val_loss: 0.1304\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1437 - val_loss: 0.2019\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1443 - val_loss: 0.1158\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1310 - val_loss: 0.1127\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1419 - val_loss: 0.1231\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1572 - val_loss: 0.1198\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1377 - val_loss: 0.1251\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1279 - val_loss: 0.1170\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1274 - val_loss: 0.1492\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1190 - val_loss: 0.1164\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1183 - val_loss: 0.1483\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1232 - val_loss: 0.1682\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1896 - val_loss: 0.1157\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1382 - val_loss: 0.1190\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1206 - val_loss: 0.1099\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1120 - val_loss: 0.1237\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1205 - val_loss: 0.1185\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1072 - val_loss: 0.1093\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0974 - val_loss: 0.1231\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0974 - val_loss: 0.1245\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1440 - val_loss: 0.4642\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3123 - val_loss: 0.1719\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1685 - val_loss: 0.1452\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1545 - val_loss: 0.1276\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1351 - val_loss: 0.1408\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1174 - val_loss: 0.1174\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1099 - val_loss: 0.1117\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1022 - val_loss: 0.1146\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1019 - val_loss: 0.1643\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1010 - val_loss: 0.1411\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1030 - val_loss: 0.1250\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0966 - val_loss: 0.1227\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0980 - val_loss: 0.1342\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0971 - val_loss: 0.1182\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0921 - val_loss: 0.1281\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0907 - val_loss: 0.1554\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0916 - val_loss: 0.1227\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0889 - val_loss: 0.1277\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0891 - val_loss: 0.1106\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0857 - val_loss: 0.1377\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0872 - val_loss: 0.1116\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0883 - val_loss: 0.1533\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0906 - val_loss: 0.1257\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0959 - val_loss: 0.1230\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1014 - val_loss: 0.1262\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0900 - val_loss: 0.1125\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0885 - val_loss: 0.1448\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1046 - val_loss: 0.1160\n",
      "8/8 [==============================] - 0s 29ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 4s 66ms/step - loss: 0.5437 - val_loss: 0.3744\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4302 - val_loss: 0.4387\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4007 - val_loss: 0.3105\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3692 - val_loss: 0.3309\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3462 - val_loss: 0.5208\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3408 - val_loss: 0.2304\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3179 - val_loss: 0.2309\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3102 - val_loss: 0.2697\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2798 - val_loss: 0.2022\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2912 - val_loss: 0.2277\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3029 - val_loss: 0.2297\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2730 - val_loss: 0.2174\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2573 - val_loss: 0.2000\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2536 - val_loss: 0.2036\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2325 - val_loss: 0.1819\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2525 - val_loss: 0.2041\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2338 - val_loss: 0.2536\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2357 - val_loss: 0.1719\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2253 - val_loss: 0.2041\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2078 - val_loss: 0.2313\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2007 - val_loss: 0.2277\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2337 - val_loss: 0.1869\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1875 - val_loss: 0.2247\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1718 - val_loss: 0.2061\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1711 - val_loss: 0.1653\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1658 - val_loss: 0.1921\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1627 - val_loss: 0.1811\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1652 - val_loss: 0.1817\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1635 - val_loss: 0.1856\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1875 - val_loss: 0.1602\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1443 - val_loss: 0.2129\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1368 - val_loss: 0.2071\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1411 - val_loss: 0.1342\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1366 - val_loss: 0.1702\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1281 - val_loss: 0.1308\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1234 - val_loss: 0.1590\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1314 - val_loss: 0.1666\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1211 - val_loss: 0.1679\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1499 - val_loss: 0.1513\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1492 - val_loss: 0.1530\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1265 - val_loss: 0.1397\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1501 - val_loss: 0.2254\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2188 - val_loss: 0.2146\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2016 - val_loss: 0.1540\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1355 - val_loss: 0.1333\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1173 - val_loss: 0.1405\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1147 - val_loss: 0.1582\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1086 - val_loss: 0.1169\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1049 - val_loss: 0.1085\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1115 - val_loss: 0.1461\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1146 - val_loss: 0.1261\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1171 - val_loss: 0.2041\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1070 - val_loss: 0.1826\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1222 - val_loss: 0.4303\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1234 - val_loss: 0.2677\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1070 - val_loss: 0.1965\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0952 - val_loss: 0.2213\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0980 - val_loss: 0.1588\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0923 - val_loss: 0.1505\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1314 - val_loss: 0.1270\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1641 - val_loss: 0.1206\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1884 - val_loss: 0.1215\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1615 - val_loss: 0.1767\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1353 - val_loss: 0.1661\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1219 - val_loss: 0.1574\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1035 - val_loss: 0.1666\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1100 - val_loss: 0.1317\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0942 - val_loss: 0.1221\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0862 - val_loss: 0.1151\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0822 - val_loss: 0.1255\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0807 - val_loss: 0.1809\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0876 - val_loss: 0.1518\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0855 - val_loss: 0.1351\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0799 - val_loss: 0.1795\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0746 - val_loss: 0.1154\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0756 - val_loss: 0.1582\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0745 - val_loss: 0.2560\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1811 - val_loss: 0.1956\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1515 - val_loss: 0.1868\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1600 - val_loss: 0.1365\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1719 - val_loss: 0.1318\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1281 - val_loss: 0.2245\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0975 - val_loss: 0.1533\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0872 - val_loss: 0.1518\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0830 - val_loss: 0.2005\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0819 - val_loss: 0.1470\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0764 - val_loss: 0.1355\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0712 - val_loss: 0.1884\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0696 - val_loss: 0.1863\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0693 - val_loss: 0.1398\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0683 - val_loss: 0.2281\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0675 - val_loss: 0.1709\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0638 - val_loss: 0.1350\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0634 - val_loss: 0.1356\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0628 - val_loss: 0.1545\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0609 - val_loss: 0.2024\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0615 - val_loss: 0.1493\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0579 - val_loss: 0.1611\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0572 - val_loss: 0.1719\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0572 - val_loss: 0.1596\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0557 - val_loss: 0.1371\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0548 - val_loss: 0.1474\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0542 - val_loss: 0.1823\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0538 - val_loss: 0.1626\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0527 - val_loss: 0.1783\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0519 - val_loss: 0.1684\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0563 - val_loss: 0.2180\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0562 - val_loss: 0.2283\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0567 - val_loss: 0.1558\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0524 - val_loss: 0.1764\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0571 - val_loss: 0.1893\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2185 - val_loss: 0.2121\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1605 - val_loss: 0.2441\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1257 - val_loss: 0.1684\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1421 - val_loss: 0.1699\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2154 - val_loss: 0.1394\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2560 - val_loss: 0.3050\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2263 - val_loss: 0.1399\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1500 - val_loss: 0.1918\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1108 - val_loss: 0.1829\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1261 - val_loss: 0.1191\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0982 - val_loss: 0.2292\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1476 - val_loss: 0.2116\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1406 - val_loss: 0.3186\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2133 - val_loss: 0.1495\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1066 - val_loss: 0.1431\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1066 - val_loss: 0.1198\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0838 - val_loss: 0.1212\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0721 - val_loss: 0.1384\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0730 - val_loss: 0.1256\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0721 - val_loss: 0.1694\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0718 - val_loss: 0.1326\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0650 - val_loss: 0.1414\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0601 - val_loss: 0.1397\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0603 - val_loss: 0.1283\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0629 - val_loss: 0.1450\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0561 - val_loss: 0.1451\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0542 - val_loss: 0.1363\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0596 - val_loss: 0.1431\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0555 - val_loss: 0.1357\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0539 - val_loss: 0.1368\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0520 - val_loss: 0.1251\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0509 - val_loss: 0.1340\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0503 - val_loss: 0.1752\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0493 - val_loss: 0.1333\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0485 - val_loss: 0.1466\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0482 - val_loss: 0.1530\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0508 - val_loss: 0.1170\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0569 - val_loss: 0.1285\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0495 - val_loss: 0.1346\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0472 - val_loss: 0.1345\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0469 - val_loss: 0.1494\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0465 - val_loss: 0.1434\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0468 - val_loss: 0.1482\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0457 - val_loss: 0.1544\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0453 - val_loss: 0.1422\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0453 - val_loss: 0.1488\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0446 - val_loss: 0.1513\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0441 - val_loss: 0.1323\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0436 - val_loss: 0.1425\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0433 - val_loss: 0.1574\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0427 - val_loss: 0.1916\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0431 - val_loss: 0.1399\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0437 - val_loss: 0.1265\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0427 - val_loss: 0.1470\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0429 - val_loss: 0.1572\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0420 - val_loss: 0.1371\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0417 - val_loss: 0.1569\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0414 - val_loss: 0.1344\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0413 - val_loss: 0.1554\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0410 - val_loss: 0.1545\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0410 - val_loss: 0.1459\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0412 - val_loss: 0.1561\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0410 - val_loss: 0.1530\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0408 - val_loss: 0.1274\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0401 - val_loss: 0.1376\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0400 - val_loss: 0.1609\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0462 - val_loss: 0.1469\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0423 - val_loss: 0.1387\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0406 - val_loss: 0.1330\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0405 - val_loss: 0.1549\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0822 - val_loss: 0.2675\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2302 - val_loss: 0.2087\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1568 - val_loss: 0.1621\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1285 - val_loss: 0.1403\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1153 - val_loss: 0.1169\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0722 - val_loss: 0.1133\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0586 - val_loss: 0.1013\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0531 - val_loss: 0.1152\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0488 - val_loss: 0.1108\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0454 - val_loss: 0.1159\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0434 - val_loss: 0.1188\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0425 - val_loss: 0.1282\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0418 - val_loss: 0.1182\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0413 - val_loss: 0.1196\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0405 - val_loss: 0.1189\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0401 - val_loss: 0.1204\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0401 - val_loss: 0.1223\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0397 - val_loss: 0.1346\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0394 - val_loss: 0.1282\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0387 - val_loss: 0.1285\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0385 - val_loss: 0.1341\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0382 - val_loss: 0.1308\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0382 - val_loss: 0.1286\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0379 - val_loss: 0.1305\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0378 - val_loss: 0.1261\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0373 - val_loss: 0.1293\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0373 - val_loss: 0.1320\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0373 - val_loss: 0.1308\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0372 - val_loss: 0.1379\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0955 - val_loss: 0.2217\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2304 - val_loss: 0.1642\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1842 - val_loss: 0.1519\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1346 - val_loss: 0.1430\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1199 - val_loss: 0.1509\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0992 - val_loss: 0.1302\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0672 - val_loss: 0.1186\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0585 - val_loss: 0.1303\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0640 - val_loss: 0.1204\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0506 - val_loss: 0.1183\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0469 - val_loss: 0.1111\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0472 - val_loss: 0.1140\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0499 - val_loss: 0.1155\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0510 - val_loss: 0.1192\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0445 - val_loss: 0.1151\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0423 - val_loss: 0.1206\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0412 - val_loss: 0.1223\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0406 - val_loss: 0.1252\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0402 - val_loss: 0.1254\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0397 - val_loss: 0.1185\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0394 - val_loss: 0.1183\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0389 - val_loss: 0.1143\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0383 - val_loss: 0.1125\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0379 - val_loss: 0.1225\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0376 - val_loss: 0.1220\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0373 - val_loss: 0.1282\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0374 - val_loss: 0.1210\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0371 - val_loss: 0.1214\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0369 - val_loss: 0.1259\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0366 - val_loss: 0.1295\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0364 - val_loss: 0.1295\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0361 - val_loss: 0.1265\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0362 - val_loss: 0.1303\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0360 - val_loss: 0.1280\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0355 - val_loss: 0.1309\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0355 - val_loss: 0.1234\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0352 - val_loss: 0.1282\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0350 - val_loss: 0.1274\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0349 - val_loss: 0.1285\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0345 - val_loss: 0.1259\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0346 - val_loss: 0.1264\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0344 - val_loss: 0.1286\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0344 - val_loss: 0.1389\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0342 - val_loss: 0.1335\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0339 - val_loss: 0.1373\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0336 - val_loss: 0.1422\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0337 - val_loss: 0.1419\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0335 - val_loss: 0.1413\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0334 - val_loss: 0.1363\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0332 - val_loss: 0.1398\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0333 - val_loss: 0.1425\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0331 - val_loss: 0.1359\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0329 - val_loss: 0.1325\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0625 - val_loss: 0.1783\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1141 - val_loss: 0.1382\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0887 - val_loss: 0.1545\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1341 - val_loss: 0.1565\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2046 - val_loss: 0.1760\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1256 - val_loss: 0.1431\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0816 - val_loss: 0.1214\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0658 - val_loss: 0.1299\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0549 - val_loss: 0.1193\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0486 - val_loss: 0.1110\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0457 - val_loss: 0.1165\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0429 - val_loss: 0.1180\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0957 - val_loss: 0.3124\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1094 - val_loss: 0.1661\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0781 - val_loss: 0.1681\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0844 - val_loss: 0.1441\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0863 - val_loss: 0.1348\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0552 - val_loss: 0.1412\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0502 - val_loss: 0.1133\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0440 - val_loss: 0.1245\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0418 - val_loss: 0.1261\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0405 - val_loss: 0.1294\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0396 - val_loss: 0.1320\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0390 - val_loss: 0.1301\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0385 - val_loss: 0.1278\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.1315\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0374 - val_loss: 0.1331\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0372 - val_loss: 0.1306\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0370 - val_loss: 0.1300\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0364 - val_loss: 0.1356\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0364 - val_loss: 0.1376\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0360 - val_loss: 0.1400\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0359 - val_loss: 0.1333\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0355 - val_loss: 0.1369\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0353 - val_loss: 0.1377\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0353 - val_loss: 0.1355\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0349 - val_loss: 0.1383\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0346 - val_loss: 0.1447\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0346 - val_loss: 0.1400\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0344 - val_loss: 0.1407\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0340 - val_loss: 0.1384\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0340 - val_loss: 0.1351\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0338 - val_loss: 0.1418\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0337 - val_loss: 0.1396\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0337 - val_loss: 0.1383\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0335 - val_loss: 0.1378\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0336 - val_loss: 0.1294\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0334 - val_loss: 0.1345\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0331 - val_loss: 0.1450\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0328 - val_loss: 0.1407\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0330 - val_loss: 0.1397\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0327 - val_loss: 0.1348\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0325 - val_loss: 0.1726\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0363 - val_loss: 0.1296\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0337 - val_loss: 0.1337\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0332 - val_loss: 0.1330\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0327 - val_loss: 0.1368\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0323 - val_loss: 0.1359\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0322 - val_loss: 0.1412\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0321 - val_loss: 0.1368\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0320 - val_loss: 0.1404\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0317 - val_loss: 0.1347\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0317 - val_loss: 0.1373\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0318 - val_loss: 0.1363\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0316 - val_loss: 0.1523\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0316 - val_loss: 0.1457\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0321 - val_loss: 0.1415\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0314 - val_loss: 0.1400\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0313 - val_loss: 0.1504\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0313 - val_loss: 0.1542\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0311 - val_loss: 0.1485\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0312 - val_loss: 0.1433\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0313 - val_loss: 0.1401\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0308 - val_loss: 0.1439\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0308 - val_loss: 0.1483\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0306 - val_loss: 0.1432\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0305 - val_loss: 0.1495\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0306 - val_loss: 0.1581\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0304 - val_loss: 0.1515\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0304 - val_loss: 0.1463\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0306 - val_loss: 0.1372\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0306 - val_loss: 0.1500\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0302 - val_loss: 0.1394\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0300 - val_loss: 0.1559\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0303 - val_loss: 0.1556\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0299 - val_loss: 0.1451\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0299 - val_loss: 0.1662\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0299 - val_loss: 0.1522\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0297 - val_loss: 0.1489\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0298 - val_loss: 0.1557\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0297 - val_loss: 0.1550\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0296 - val_loss: 0.1613\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0298 - val_loss: 0.1600\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0298 - val_loss: 0.1629\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0294 - val_loss: 0.1550\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0294 - val_loss: 0.1624\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0306 - val_loss: 0.1486\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1007 - val_loss: 0.1893\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1944 - val_loss: 0.3734\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1395 - val_loss: 0.1200\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1323 - val_loss: 0.1374\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0897 - val_loss: 0.1354\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0627 - val_loss: 0.1968\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1629 - val_loss: 0.1155\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0747 - val_loss: 0.1313\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0588 - val_loss: 0.1121\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0461 - val_loss: 0.1246\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0407 - val_loss: 0.1246\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0384 - val_loss: 0.1265\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0368 - val_loss: 0.1224\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0358 - val_loss: 0.1232\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0350 - val_loss: 0.1228\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0341 - val_loss: 0.1278\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0338 - val_loss: 0.1289\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0333 - val_loss: 0.1306\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0329 - val_loss: 0.1273\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0325 - val_loss: 0.1339\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0322 - val_loss: 0.1317\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0322 - val_loss: 0.1347\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0322 - val_loss: 0.1292\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0354 - val_loss: 0.1311\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0351 - val_loss: 0.1531\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0333 - val_loss: 0.1357\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0317 - val_loss: 0.1318\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0313 - val_loss: 0.1375\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0314 - val_loss: 0.1345\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0306 - val_loss: 0.1346\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0305 - val_loss: 0.1362\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0303 - val_loss: 0.1354\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0302 - val_loss: 0.1352\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0301 - val_loss: 0.1400\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0299 - val_loss: 0.1318\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0297 - val_loss: 0.1335\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0296 - val_loss: 0.1315\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0296 - val_loss: 0.1364\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0294 - val_loss: 0.1395\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0294 - val_loss: 0.1399\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0294 - val_loss: 0.1376\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0292 - val_loss: 0.1393\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0292 - val_loss: 0.1416\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0290 - val_loss: 0.1396\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0289 - val_loss: 0.1416\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0288 - val_loss: 0.1401\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0288 - val_loss: 0.1416\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0288 - val_loss: 0.1414\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0286 - val_loss: 0.1425\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0286 - val_loss: 0.1431\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0285 - val_loss: 0.1438\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0286 - val_loss: 0.1368\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0284 - val_loss: 0.1449\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0285 - val_loss: 0.1456\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0281 - val_loss: 0.1444\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0281 - val_loss: 0.1464\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0281 - val_loss: 0.1450\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0282 - val_loss: 0.1460\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0281 - val_loss: 0.1454\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0281 - val_loss: 0.1416\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0279 - val_loss: 0.1401\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0279 - val_loss: 0.1458\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0278 - val_loss: 0.1466\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1662 - val_loss: 0.1769\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1705 - val_loss: 0.2178\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1596 - val_loss: 0.1796\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0828 - val_loss: 0.1952\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0522 - val_loss: 0.1603\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0426 - val_loss: 0.1390\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.1629\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0358 - val_loss: 0.1603\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0352 - val_loss: 0.1596\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0351 - val_loss: 0.1438\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0336 - val_loss: 0.1486\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0323 - val_loss: 0.1488\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0318 - val_loss: 0.1579\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0323 - val_loss: 0.1625\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0313 - val_loss: 0.1497\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0310 - val_loss: 0.1548\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0306 - val_loss: 0.1614\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0304 - val_loss: 0.1513\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0306 - val_loss: 0.1470\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0306 - val_loss: 0.1548\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0297 - val_loss: 0.1576\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0296 - val_loss: 0.1581\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0293 - val_loss: 0.1596\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0290 - val_loss: 0.1579\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0289 - val_loss: 0.1619\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0289 - val_loss: 0.1652\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0288 - val_loss: 0.1617\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0286 - val_loss: 0.1709\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0283 - val_loss: 0.1690\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0285 - val_loss: 0.1678\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0284 - val_loss: 0.1676\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0285 - val_loss: 0.1759\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0281 - val_loss: 0.1702\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0281 - val_loss: 0.1730\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0280 - val_loss: 0.1669\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0279 - val_loss: 0.1780\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0278 - val_loss: 0.1737\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0276 - val_loss: 0.1788\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0275 - val_loss: 0.1747\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0276 - val_loss: 0.1729\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0275 - val_loss: 0.1752\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0276 - val_loss: 0.1656\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0274 - val_loss: 0.1688\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0278 - val_loss: 0.1614\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0277 - val_loss: 0.1710\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0273 - val_loss: 0.1711\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0273 - val_loss: 0.1655\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0271 - val_loss: 0.1670\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0270 - val_loss: 0.1680\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0270 - val_loss: 0.1728\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0269 - val_loss: 0.1678\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0268 - val_loss: 0.1767\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0268 - val_loss: 0.1688\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0267 - val_loss: 0.1727\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0267 - val_loss: 0.1678\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0266 - val_loss: 0.1722\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0266 - val_loss: 0.1775\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0267 - val_loss: 0.1708\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0268 - val_loss: 0.1740\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0268 - val_loss: 0.1714\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0266 - val_loss: 0.1781\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0264 - val_loss: 0.1716\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0264 - val_loss: 0.1735\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0263 - val_loss: 0.1642\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0262 - val_loss: 0.1792\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0262 - val_loss: 0.1781\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0263 - val_loss: 0.1692\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0263 - val_loss: 0.1811\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0262 - val_loss: 0.1805\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0262 - val_loss: 0.1704\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0263 - val_loss: 0.1764\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0262 - val_loss: 0.1748\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0260 - val_loss: 0.1792\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0260 - val_loss: 0.1752\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0260 - val_loss: 0.1823\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0260 - val_loss: 0.1819\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0260 - val_loss: 0.1779\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 5s 78ms/step - loss: 71.9771 - val_loss: 0.3320\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4612 - val_loss: 0.3242\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4391 - val_loss: 0.4611\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4258 - val_loss: 0.3946\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4156 - val_loss: 0.3232\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4140 - val_loss: 0.4220\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4063 - val_loss: 0.4645\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4226 - val_loss: 0.3821\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3950 - val_loss: 0.2927\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4111 - val_loss: 0.3292\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3830 - val_loss: 0.3668\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4230 - val_loss: 0.5017\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3798 - val_loss: 0.3730\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3830 - val_loss: 0.2985\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3583 - val_loss: 0.3240\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3601 - val_loss: 0.3469\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3658 - val_loss: 0.3546\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.3770 - val_loss: 0.2955\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3651 - val_loss: 0.3257\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 410.5623 - val_loss: 0.5075\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5516 - val_loss: 0.4103\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5372 - val_loss: 0.3888\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3826\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5357 - val_loss: 0.3791\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3785\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5358 - val_loss: 0.3810\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5357 - val_loss: 0.3804\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5357 - val_loss: 0.3805\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5241 - val_loss: 0.3645\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5259 - val_loss: 0.3572\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5053 - val_loss: 0.3914\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5010 - val_loss: 0.3392\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4955 - val_loss: 0.3336\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4907 - val_loss: 0.3343\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4901 - val_loss: 0.3271\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4889 - val_loss: 0.3348\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4892 - val_loss: 0.3255\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4868 - val_loss: 0.3259\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4873 - val_loss: 0.3214\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4858 - val_loss: 0.3287\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4852 - val_loss: 0.3211\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4848 - val_loss: 0.3522\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.7395 - val_loss: 0.3183\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4894 - val_loss: 0.3303\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4900 - val_loss: 0.3201\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4859 - val_loss: 0.3407\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4886 - val_loss: 0.3333\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4858 - val_loss: 0.3287\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4858 - val_loss: 0.3277\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4838 - val_loss: 0.3395\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4839 - val_loss: 0.3461\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4828 - val_loss: 0.3312\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4829 - val_loss: 0.3526\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4839 - val_loss: 0.3313\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4855 - val_loss: 0.3192\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4824 - val_loss: 0.3317\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4819 - val_loss: 0.3165\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4799 - val_loss: 0.3983\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4849 - val_loss: 0.3697\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4859 - val_loss: 0.3100\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4807 - val_loss: 0.3128\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4844 - val_loss: 0.3455\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4832 - val_loss: 0.3273\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4815 - val_loss: 0.3418\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4811 - val_loss: 0.3289\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4801 - val_loss: 0.3239\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4818 - val_loss: 0.3286\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4758 - val_loss: 0.3507\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4764 - val_loss: 0.3305\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4747 - val_loss: 0.3089\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4783 - val_loss: 0.3647\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4782 - val_loss: 0.3218\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4766 - val_loss: 0.3119\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4772 - val_loss: 0.3440\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4793 - val_loss: 0.3375\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4736 - val_loss: 0.3210\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4753 - val_loss: 0.3090\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4732 - val_loss: 0.3526\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4765 - val_loss: 0.3217\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4709 - val_loss: 0.3215\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4707 - val_loss: 0.3206\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4684 - val_loss: 0.3223\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4680 - val_loss: 0.3121\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4698 - val_loss: 0.3504\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4682 - val_loss: 0.3190\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4697 - val_loss: 0.3323\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4698 - val_loss: 0.3291\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4666 - val_loss: 0.3344\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4677 - val_loss: 0.3503\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4652 - val_loss: 0.3201\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4652 - val_loss: 0.3073\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4708 - val_loss: 0.3247\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4680 - val_loss: 0.3079\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4648 - val_loss: 0.3370\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4662 - val_loss: 0.3322\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4727 - val_loss: 0.3237\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4627 - val_loss: 0.3671\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.4634 - val_loss: 0.3739\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4645 - val_loss: 0.3682\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4603 - val_loss: 0.3345\n",
      "8/8 [==============================] - 0s 27ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 4s 66ms/step - loss: 2.2050 - val_loss: 0.4245\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.7151 - val_loss: 0.3910\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5369 - val_loss: 0.3705\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5382 - val_loss: 0.3763\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5379 - val_loss: 0.3659\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5370 - val_loss: 0.3741\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5363 - val_loss: 0.3916\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5366 - val_loss: 0.3802\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5365 - val_loss: 0.3771\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5360 - val_loss: 0.3821\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.5369 - val_loss: 0.3852\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5368 - val_loss: 0.3888\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5361 - val_loss: 0.3794\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5364 - val_loss: 0.3767\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5362 - val_loss: 0.3779\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5359 - val_loss: 0.3801\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5363 - val_loss: 0.3820\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5362 - val_loss: 0.3771\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5363 - val_loss: 0.3819\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3772\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3746\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3842\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5358 - val_loss: 0.3791\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5360 - val_loss: 0.3810\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5365 - val_loss: 0.3850\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5361 - val_loss: 0.3793\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5363 - val_loss: 0.3823\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5359 - val_loss: 0.3841\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3755\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5359 - val_loss: 0.3778\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5358 - val_loss: 0.3784\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5358 - val_loss: 0.3797\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5364 - val_loss: 0.3753\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3783\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5359 - val_loss: 0.3798\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3798\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5363 - val_loss: 0.3884\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5363 - val_loss: 0.3816\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5361 - val_loss: 0.3851\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3797\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5358 - val_loss: 0.3802\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5358 - val_loss: 0.3821\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5364 - val_loss: 0.3742\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5360 - val_loss: 0.3782\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5362 - val_loss: 0.3769\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5361 - val_loss: 0.3829\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5363 - val_loss: 0.3694\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5366 - val_loss: 0.3726\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3832\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3727\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5366 - val_loss: 0.3758\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3766\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5361 - val_loss: 0.3830\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3752\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3766\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5361 - val_loss: 0.3774\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3796\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3769\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5362 - val_loss: 0.3856\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3772\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3804\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3781\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5365 - val_loss: 0.3745\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.5359 - val_loss: 0.3771\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5363 - val_loss: 0.3745\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.5359 - val_loss: 0.3781\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3778\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3775\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3792\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5361 - val_loss: 0.3799\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5359 - val_loss: 0.3773\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5359 - val_loss: 0.3815\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5358 - val_loss: 0.3826\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5359 - val_loss: 0.3825\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5361 - val_loss: 0.3782\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5362 - val_loss: 0.3761\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5365 - val_loss: 0.3856\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5365 - val_loss: 0.3808\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5362 - val_loss: 0.3855\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5366 - val_loss: 0.3762\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3779\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5359 - val_loss: 0.3780\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5361 - val_loss: 0.3830\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3800\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5363 - val_loss: 0.3795\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5364 - val_loss: 0.3885\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5367 - val_loss: 0.3818\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5360 - val_loss: 0.3813\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5364 - val_loss: 0.3854\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5365 - val_loss: 0.3793\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5360 - val_loss: 0.3780\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5363 - val_loss: 0.3792\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3745\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3779\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5368 - val_loss: 0.3803\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5362 - val_loss: 0.3812\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5359 - val_loss: 0.3792\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3797\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5357 - val_loss: 0.3772\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3800\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5397 - val_loss: 0.4222\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5396 - val_loss: 0.3723\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5372 - val_loss: 0.3706\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.5365 - val_loss: 0.3726\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5367 - val_loss: 0.3801\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.5364 - val_loss: 0.4030\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5359 - val_loss: 0.3662\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5366 - val_loss: 0.3767\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5359 - val_loss: 0.3801\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5365 - val_loss: 0.3776\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3782\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5362 - val_loss: 0.3779\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3743\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3781\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3866\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3828\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5359 - val_loss: 0.3750\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5362 - val_loss: 0.3788\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5358 - val_loss: 0.3779\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5362 - val_loss: 0.3741\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5367 - val_loss: 0.3787\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5358 - val_loss: 0.3824\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5364 - val_loss: 0.3816\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5363 - val_loss: 0.3733\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5362 - val_loss: 0.3778\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5362 - val_loss: 0.3854\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5363 - val_loss: 0.3830\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5360 - val_loss: 0.3777\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5364 - val_loss: 0.3813\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5373 - val_loss: 0.3793\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5370 - val_loss: 0.3771\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3772\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3872\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5365 - val_loss: 0.3806\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3796\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5358 - val_loss: 0.3780\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5359 - val_loss: 0.3783\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5363 - val_loss: 0.3838\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5356 - val_loss: 0.3735\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5363 - val_loss: 0.3831\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3796\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3809\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5359 - val_loss: 0.3784\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3793\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5362 - val_loss: 0.3802\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.5359 - val_loss: 0.3719\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3793\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5366 - val_loss: 0.3777\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3783\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3773\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3760\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5362 - val_loss: 0.3792\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5360 - val_loss: 0.3903\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3798\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3786\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5358 - val_loss: 0.3759\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3812\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5367 - val_loss: 0.3855\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3795\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3753\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3828\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5361 - val_loss: 0.3819\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5359 - val_loss: 0.3778\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5363 - val_loss: 0.3843\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5362 - val_loss: 0.3819\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5359 - val_loss: 0.3795\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5362 - val_loss: 0.3853\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5361 - val_loss: 0.3792\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5361 - val_loss: 0.3766\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5361 - val_loss: 0.3847\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3785\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5362 - val_loss: 0.3726\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5363 - val_loss: 0.3784\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5359 - val_loss: 0.3784\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3814\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5361 - val_loss: 0.3756\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3815\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3867\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3771\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5361 - val_loss: 0.3858\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5364 - val_loss: 0.3793\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5361 - val_loss: 0.3822\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5365 - val_loss: 0.3809\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5362 - val_loss: 0.3860\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5360 - val_loss: 0.3772\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5364 - val_loss: 0.3752\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.5365 - val_loss: 0.3789\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5358 - val_loss: 0.3809\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5361 - val_loss: 0.3774\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5362 - val_loss: 0.3861\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5360 - val_loss: 0.3833\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5361 - val_loss: 0.3752\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5364 - val_loss: 0.3817\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5376 - val_loss: 0.3699\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5369 - val_loss: 0.3879\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5360 - val_loss: 0.3766\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5363 - val_loss: 0.3807\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5366 - val_loss: 0.3723\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5365 - val_loss: 0.3733\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5357 - val_loss: 0.3799\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 4s 65ms/step - loss: 1.0685 - val_loss: 0.3019\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4938 - val_loss: 0.2940\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4394 - val_loss: 0.3103\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4350 - val_loss: 0.2939\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4063 - val_loss: 0.3656\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3757 - val_loss: 0.2125\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3822 - val_loss: 0.2989\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3416 - val_loss: 0.3917\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3541 - val_loss: 0.2484\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3212 - val_loss: 0.5021\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3875 - val_loss: 0.2654\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2888 - val_loss: 0.3117\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3874 - val_loss: 0.2626\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2946 - val_loss: 0.2431\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3207 - val_loss: 0.3361\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.3120 - val_loss: 0.4060\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3702 - val_loss: 0.2446\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2798 - val_loss: 0.2887\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2704 - val_loss: 0.2551\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3317 - val_loss: 0.2332\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2844 - val_loss: 0.2508\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2843 - val_loss: 0.3321\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2771 - val_loss: 0.2502\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2863 - val_loss: 0.2548\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3186 - val_loss: 0.3421\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3066 - val_loss: 0.2807\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2810 - val_loss: 0.3409\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2703 - val_loss: 0.2493\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2706 - val_loss: 0.2092\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2669 - val_loss: 0.2466\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2711 - val_loss: 0.2000\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2710 - val_loss: 0.1961\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2646 - val_loss: 0.2487\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2801 - val_loss: 0.2787\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2648 - val_loss: 0.3116\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2721 - val_loss: 0.2389\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2874 - val_loss: 0.2127\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4710 - val_loss: 0.3401\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4004 - val_loss: 0.3039\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3276 - val_loss: 0.2501\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3461 - val_loss: 0.2535\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2921 - val_loss: 0.3508\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2990 - val_loss: 0.4208\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2991 - val_loss: 0.2211\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2699 - val_loss: 0.2962\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2697 - val_loss: 0.3182\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2803 - val_loss: 0.2128\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2823 - val_loss: 0.2091\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2755 - val_loss: 0.2365\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2928 - val_loss: 0.2958\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2731 - val_loss: 0.3351\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2779 - val_loss: 0.2602\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2721 - val_loss: 0.4212\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2782 - val_loss: 0.2119\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3913 - val_loss: 0.2954\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4018 - val_loss: 0.3861\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3306 - val_loss: 0.2571\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2919 - val_loss: 0.2990\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2862 - val_loss: 0.3464\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2820 - val_loss: 0.3639\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2725 - val_loss: 0.2294\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2760 - val_loss: 0.2194\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2661 - val_loss: 0.3105\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2649 - val_loss: 0.2979\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2702 - val_loss: 0.2084\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2716 - val_loss: 0.4001\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2984 - val_loss: 0.4406\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2691 - val_loss: 0.2284\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2630 - val_loss: 0.2286\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2706 - val_loss: 0.2351\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2684 - val_loss: 0.2490\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2603 - val_loss: 0.3313\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3314 - val_loss: 0.3422\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3208 - val_loss: 0.3023\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3072 - val_loss: 0.4098\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2685 - val_loss: 0.2630\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2705 - val_loss: 0.2157\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2646 - val_loss: 0.3008\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2686 - val_loss: 0.2333\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2597 - val_loss: 0.2195\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2694 - val_loss: 0.2519\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2581 - val_loss: 0.2660\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2617 - val_loss: 0.2040\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2627 - val_loss: 0.2449\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2555 - val_loss: 0.2898\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2591 - val_loss: 0.2094\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2644 - val_loss: 0.2158\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2602 - val_loss: 0.2454\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2550 - val_loss: 0.2253\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2572 - val_loss: 0.2847\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2548 - val_loss: 0.2465\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2670 - val_loss: 0.2417\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2545 - val_loss: 0.2810\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2646 - val_loss: 0.2823\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2522 - val_loss: 0.2262\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2562 - val_loss: 0.3273\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4043 - val_loss: 0.3215\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3301 - val_loss: 0.2578\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2611 - val_loss: 0.2830\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2721 - val_loss: 0.2731\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2549 - val_loss: 0.2413\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2547 - val_loss: 0.2264\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2570 - val_loss: 0.2108\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2551 - val_loss: 0.2658\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2616 - val_loss: 0.2812\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2696 - val_loss: 0.2076\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2579 - val_loss: 0.2748\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2724 - val_loss: 0.2433\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2739 - val_loss: 0.2657\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2880 - val_loss: 0.3033\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3162 - val_loss: 0.3422\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2718 - val_loss: 0.2839\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2565 - val_loss: 0.2760\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2523 - val_loss: 0.2562\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2524 - val_loss: 0.2326\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2478 - val_loss: 0.2653\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2495 - val_loss: 0.3771\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2531 - val_loss: 0.2917\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2516 - val_loss: 0.2609\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2507 - val_loss: 0.2872\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2488 - val_loss: 0.2440\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3234 - val_loss: 0.2848\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2701 - val_loss: 0.1990\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2658 - val_loss: 0.3181\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2529 - val_loss: 0.3582\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2584 - val_loss: 0.2192\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2470 - val_loss: 0.2257\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2524 - val_loss: 0.2353\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2473 - val_loss: 0.2829\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2478 - val_loss: 0.2672\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2423 - val_loss: 0.2736\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2453 - val_loss: 0.2617\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2468 - val_loss: 0.2010\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2551 - val_loss: 0.2674\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2578 - val_loss: 0.2354\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2499 - val_loss: 0.2645\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2404 - val_loss: 0.3179\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2443 - val_loss: 0.2480\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2467 - val_loss: 0.2323\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2476 - val_loss: 0.2587\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2457 - val_loss: 0.2365\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2470 - val_loss: 0.2346\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2739 - val_loss: 0.2681\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3249 - val_loss: 0.2846\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2732 - val_loss: 0.2182\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2587 - val_loss: 0.2337\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2465 - val_loss: 0.2769\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2467 - val_loss: 0.2472\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2436 - val_loss: 0.2690\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2516 - val_loss: 0.2269\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2450 - val_loss: 0.2273\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2448 - val_loss: 0.2239\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2437 - val_loss: 0.2194\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2448 - val_loss: 0.2462\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2410 - val_loss: 0.2468\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3224 - val_loss: 0.4904\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4425 - val_loss: 0.2411\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3647 - val_loss: 0.2687\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2978 - val_loss: 0.2567\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2666 - val_loss: 0.2135\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2656 - val_loss: 0.2134\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3281 - val_loss: 0.5818\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2799 - val_loss: 0.2630\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2634 - val_loss: 0.2497\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2654 - val_loss: 0.2444\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2539 - val_loss: 0.3008\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2645 - val_loss: 0.2122\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2539 - val_loss: 0.2287\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2609 - val_loss: 0.2203\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2560 - val_loss: 0.2306\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2525 - val_loss: 0.2257\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2590 - val_loss: 0.4175\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2558 - val_loss: 0.2507\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2643 - val_loss: 0.2252\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2602 - val_loss: 0.3361\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2715 - val_loss: 0.2741\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2535 - val_loss: 0.2112\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2733 - val_loss: 0.2121\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2509 - val_loss: 0.2024\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2578 - val_loss: 0.2485\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2490 - val_loss: 0.2322\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2455 - val_loss: 0.2949\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2480 - val_loss: 0.2468\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2468 - val_loss: 0.2772\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2529 - val_loss: 0.2530\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2440 - val_loss: 0.2440\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2423 - val_loss: 0.2257\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2405 - val_loss: 0.2246\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2537 - val_loss: 0.1973\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2473 - val_loss: 0.2163\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2471 - val_loss: 0.2485\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2454 - val_loss: 0.2611\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2409 - val_loss: 0.2220\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2425 - val_loss: 0.2705\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2500 - val_loss: 0.2680\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2421 - val_loss: 0.2257\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2463 - val_loss: 0.2198\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2383 - val_loss: 0.2166\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2456 - val_loss: 0.2636\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3467 - val_loss: 0.2593\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2660 - val_loss: 0.2555\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2491 - val_loss: 0.2312\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2508 - val_loss: 0.2272\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2536 - val_loss: 0.3991\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3212 - val_loss: 0.2887\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4090 - val_loss: 0.2674\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4061 - val_loss: 0.3297\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3167 - val_loss: 0.2947\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2621 - val_loss: 0.2516\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2558 - val_loss: 0.2738\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2499 - val_loss: 0.2633\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2523 - val_loss: 0.3209\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2455 - val_loss: 0.2788\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2491 - val_loss: 0.2466\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2462 - val_loss: 0.3932\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2482 - val_loss: 0.2298\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2438 - val_loss: 0.2765\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2476 - val_loss: 0.2692\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2519 - val_loss: 0.2254\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2413 - val_loss: 0.2157\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2440 - val_loss: 0.2438\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2379 - val_loss: 0.2877\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2353 - val_loss: 0.2246\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2440 - val_loss: 0.1976\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2479 - val_loss: 0.2064\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2387 - val_loss: 0.2043\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2373 - val_loss: 0.2057\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2326 - val_loss: 0.2333\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2388 - val_loss: 0.2479\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2387 - val_loss: 0.3095\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4492 - val_loss: 0.3498\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3780 - val_loss: 0.2530\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2519 - val_loss: 0.3116\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2467 - val_loss: 0.2609\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2444 - val_loss: 0.3840\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2420 - val_loss: 0.2373\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2453 - val_loss: 0.2795\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2406 - val_loss: 0.4273\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2598 - val_loss: 0.2141\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2395 - val_loss: 0.2538\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2367 - val_loss: 0.2659\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2362 - val_loss: 0.2248\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2476 - val_loss: 0.2176\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2520 - val_loss: 0.2836\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2373 - val_loss: 0.2305\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2358 - val_loss: 0.2356\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2331 - val_loss: 0.2634\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2343 - val_loss: 0.2259\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2355 - val_loss: 0.2210\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2383 - val_loss: 0.2756\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2331 - val_loss: 0.2560\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2313 - val_loss: 0.2613\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2310 - val_loss: 0.2091\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2335 - val_loss: 0.2191\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2360 - val_loss: 0.2580\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2356 - val_loss: 0.2184\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2346 - val_loss: 0.2004\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2362 - val_loss: 0.2617\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2298 - val_loss: 0.2422\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2940 - val_loss: 0.2390\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2623 - val_loss: 0.4320\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2385 - val_loss: 0.3848\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2501 - val_loss: 0.2473\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2311 - val_loss: 0.2042\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2432 - val_loss: 0.2936\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2314 - val_loss: 0.2124\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2341 - val_loss: 0.2965\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2291 - val_loss: 0.2370\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2275 - val_loss: 0.2037\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2353 - val_loss: 0.3046\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2241 - val_loss: 0.2699\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2282 - val_loss: 0.2588\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2390 - val_loss: 0.2046\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5024 - val_loss: 0.3408\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4338 - val_loss: 0.3335\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3703 - val_loss: 0.2977\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2553 - val_loss: 0.3552\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2414 - val_loss: 0.2623\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2491 - val_loss: 0.2165\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2395 - val_loss: 0.2824\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2347 - val_loss: 0.2177\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2294 - val_loss: 0.2077\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2335 - val_loss: 0.2775\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2342 - val_loss: 0.2029\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2329 - val_loss: 0.2651\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2329 - val_loss: 0.2585\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2299 - val_loss: 0.2336\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2287 - val_loss: 0.2060\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2328 - val_loss: 0.3068\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2393 - val_loss: 0.2041\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2321 - val_loss: 0.3195\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2401 - val_loss: 0.2570\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2333 - val_loss: 0.2376\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2301 - val_loss: 0.2271\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2270 - val_loss: 0.2031\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2296 - val_loss: 0.3464\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2326 - val_loss: 0.1944\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2298 - val_loss: 0.2236\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2221 - val_loss: 0.1971\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2401 - val_loss: 0.1978\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2307 - val_loss: 0.2436\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2279 - val_loss: 0.2234\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2328 - val_loss: 0.2105\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2512 - val_loss: 0.2435\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2275 - val_loss: 0.2788\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2280 - val_loss: 0.1997\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2220 - val_loss: 0.2036\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2226 - val_loss: 0.2060\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3849 - val_loss: 0.2841\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3203 - val_loss: 0.4111\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2452 - val_loss: 0.3434\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2346 - val_loss: 0.2631\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2281 - val_loss: 0.2389\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2239 - val_loss: 0.3464\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2311 - val_loss: 0.2750\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2227 - val_loss: 0.2867\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2293 - val_loss: 0.2153\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2232 - val_loss: 0.2065\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2267 - val_loss: 0.2313\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2401 - val_loss: 0.2661\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2216 - val_loss: 0.2456\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2186 - val_loss: 0.2633\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2235 - val_loss: 0.2230\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2285 - val_loss: 0.2013\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2396 - val_loss: 0.2178\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2192 - val_loss: 0.2170\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2210 - val_loss: 0.2234\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2203 - val_loss: 0.2248\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2180 - val_loss: 0.2432\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2199 - val_loss: 0.2623\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2183 - val_loss: 0.2352\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2146 - val_loss: 0.2286\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2185 - val_loss: 0.2354\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2232 - val_loss: 0.2259\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2222 - val_loss: 0.2477\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2149 - val_loss: 0.2792\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2143 - val_loss: 0.2248\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2164 - val_loss: 0.2246\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2204 - val_loss: 0.2169\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2233 - val_loss: 0.3663\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2175 - val_loss: 0.2554\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2179 - val_loss: 0.2934\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2174 - val_loss: 0.2201\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2177 - val_loss: 0.2509\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2159 - val_loss: 0.2325\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2185 - val_loss: 0.2628\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2138 - val_loss: 0.2504\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2221 - val_loss: 0.3187\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2263 - val_loss: 0.2167\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2317 - val_loss: 0.2615\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2258 - val_loss: 0.2513\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2154 - val_loss: 0.3168\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2220 - val_loss: 0.2992\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2607 - val_loss: 0.2982\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2422 - val_loss: 0.2211\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2297 - val_loss: 0.3269\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2224 - val_loss: 0.3165\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2242 - val_loss: 0.2446\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2206 - val_loss: 0.2941\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2271 - val_loss: 0.2181\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2288 - val_loss: 0.2083\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2188 - val_loss: 0.2111\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2206 - val_loss: 0.2754\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2640 - val_loss: 0.2599\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2471 - val_loss: 0.2721\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2224 - val_loss: 0.2712\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2149 - val_loss: 0.2147\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2175 - val_loss: 0.2779\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2131 - val_loss: 0.2751\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2143 - val_loss: 0.2601\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2206 - val_loss: 0.2814\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2152 - val_loss: 0.2155\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2224 - val_loss: 0.2234\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2160 - val_loss: 0.3451\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2149 - val_loss: 0.2914\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2137 - val_loss: 0.2585\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2206 - val_loss: 0.2265\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2073 - val_loss: 0.2196\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2092 - val_loss: 0.2312\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2324 - val_loss: 0.2402\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2139 - val_loss: 0.2542\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2107 - val_loss: 0.2471\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2149 - val_loss: 0.3170\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2098 - val_loss: 0.2503\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2077 - val_loss: 0.2781\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2109 - val_loss: 0.2152\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2185 - val_loss: 0.2502\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2119 - val_loss: 0.2527\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2097 - val_loss: 0.2993\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2114 - val_loss: 0.2354\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2109 - val_loss: 0.3013\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2097 - val_loss: 0.2427\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2076 - val_loss: 0.2918\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2102 - val_loss: 0.2616\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2138 - val_loss: 0.2035\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2417 - val_loss: 0.2190\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2105 - val_loss: 0.2379\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2070 - val_loss: 0.2118\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2099 - val_loss: 0.2189\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2086 - val_loss: 0.2799\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2050 - val_loss: 0.2186\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2277 - val_loss: 0.2016\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2165 - val_loss: 0.2523\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2091 - val_loss: 0.2162\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2092 - val_loss: 0.2368\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2055 - val_loss: 0.2577\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2065 - val_loss: 0.3457\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3066 - val_loss: 0.2300\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2740 - val_loss: 0.3044\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2411 - val_loss: 0.2761\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2192 - val_loss: 0.2661\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2151 - val_loss: 0.2224\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2336 - val_loss: 0.3674\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2164 - val_loss: 0.2311\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2099 - val_loss: 0.2553\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2148 - val_loss: 0.2868\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2154 - val_loss: 0.2263\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2106 - val_loss: 0.2262\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2123 - val_loss: 0.2466\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2115 - val_loss: 0.2842\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2060 - val_loss: 0.2881\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2090 - val_loss: 0.2273\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2104 - val_loss: 0.2271\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2116 - val_loss: 0.2278\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2147 - val_loss: 0.2165\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2085 - val_loss: 0.2223\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2049 - val_loss: 0.2899\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2063 - val_loss: 0.2796\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2062 - val_loss: 0.2780\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2065 - val_loss: 0.2716\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2092 - val_loss: 0.2871\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2168 - val_loss: 0.2960\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2038 - val_loss: 0.2620\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2135 - val_loss: 0.2325\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2119 - val_loss: 0.3265\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2057 - val_loss: 0.2433\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2070 - val_loss: 0.2534\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2048 - val_loss: 0.3098\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2114 - val_loss: 0.2905\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2062 - val_loss: 0.3223\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2078 - val_loss: 0.3425\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2072 - val_loss: 0.3538\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2105 - val_loss: 0.3562\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2087 - val_loss: 0.3365\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2527 - val_loss: 0.2049\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2283 - val_loss: 0.2340\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2147 - val_loss: 0.3020\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2063 - val_loss: 0.2615\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2108 - val_loss: 0.2945\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2116 - val_loss: 0.2664\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2047 - val_loss: 0.2389\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2068 - val_loss: 0.2407\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2080 - val_loss: 0.2107\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2080 - val_loss: 0.3106\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2078 - val_loss: 0.2836\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2102 - val_loss: 0.2932\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2012 - val_loss: 0.3048\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2064 - val_loss: 0.2962\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2069 - val_loss: 0.3964\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2089 - val_loss: 0.2341\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2028 - val_loss: 0.3135\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2027 - val_loss: 0.2658\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2030 - val_loss: 0.2726\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2019 - val_loss: 0.3217\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2030 - val_loss: 0.2071\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2034 - val_loss: 0.4258\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2027 - val_loss: 0.2198\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2034 - val_loss: 0.2309\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2136 - val_loss: 0.3536\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1992 - val_loss: 0.3059\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2025 - val_loss: 0.3040\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2049 - val_loss: 0.3212\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2043 - val_loss: 0.3316\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2037 - val_loss: 0.2714\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2047 - val_loss: 0.2711\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2139 - val_loss: 0.2146\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2067 - val_loss: 0.3320\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2055 - val_loss: 0.3194\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1977 - val_loss: 0.2293\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2033 - val_loss: 0.2577\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2035 - val_loss: 0.2904\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2123 - val_loss: 0.3498\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2018 - val_loss: 0.2697\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2004 - val_loss: 0.3077\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2092 - val_loss: 0.2200\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2209 - val_loss: 0.2297\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2049 - val_loss: 0.3869\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2031 - val_loss: 0.2815\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2009 - val_loss: 0.3707\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2130 - val_loss: 0.2389\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2045 - val_loss: 0.2935\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2064 - val_loss: 0.2182\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2018 - val_loss: 0.2606\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2038 - val_loss: 0.2720\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2024 - val_loss: 0.2563\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2025 - val_loss: 0.3053\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1972 - val_loss: 0.3229\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1999 - val_loss: 0.2719\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1970 - val_loss: 0.3212\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2037 - val_loss: 0.2564\n",
      "8/8 [==============================] - 0s 29ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 67ms/step - loss: 9706.3135 - val_loss: 0.4226\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4953 - val_loss: 0.3810\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4818 - val_loss: 0.3448\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4845 - val_loss: 0.3699\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4667 - val_loss: 0.3649\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4796 - val_loss: 0.3631\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4602 - val_loss: 0.3906\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4561 - val_loss: 0.4145\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4586 - val_loss: 0.3314\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4627 - val_loss: 0.4387\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4535 - val_loss: 0.3339\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4572 - val_loss: 0.3442\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4485 - val_loss: 0.3871\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4480 - val_loss: 0.3644\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4445 - val_loss: 0.3660\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4491 - val_loss: 0.3758\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4452 - val_loss: 0.3597\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4428 - val_loss: 0.4026\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4393 - val_loss: 0.3739\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4412 - val_loss: 0.4485\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4466 - val_loss: 0.3442\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4472 - val_loss: 0.3759\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4439 - val_loss: 0.3767\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4460 - val_loss: 0.3737\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4392 - val_loss: 0.3334\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4465 - val_loss: 0.4303\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4448 - val_loss: 0.3731\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4407 - val_loss: 0.3809\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4408 - val_loss: 0.3785\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4437 - val_loss: 0.4066\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4378 - val_loss: 0.3941\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4468 - val_loss: 0.3721\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4547 - val_loss: 0.3596\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4368 - val_loss: 0.3420\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4398 - val_loss: 0.3249\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4375 - val_loss: 0.3512\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4337 - val_loss: 0.3545\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4381 - val_loss: 0.3891\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4327 - val_loss: 0.3413\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4501 - val_loss: 0.3413\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4393 - val_loss: 0.3334\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4446 - val_loss: 0.3212\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4430 - val_loss: 0.3404\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4403 - val_loss: 0.3353\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4431 - val_loss: 0.3358\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4372 - val_loss: 0.3331\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4357 - val_loss: 0.3678\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4360 - val_loss: 0.3889\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4430 - val_loss: 0.3302\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4337 - val_loss: 0.3312\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4330 - val_loss: 0.3481\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4327 - val_loss: 0.3835\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4362 - val_loss: 0.3384\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4327 - val_loss: 0.3341\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4283 - val_loss: 0.3660\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4310 - val_loss: 0.3412\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4313 - val_loss: 0.3296\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4445 - val_loss: 0.3702\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4299 - val_loss: 0.3409\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4312 - val_loss: 0.3558\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4348 - val_loss: 0.3706\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4424 - val_loss: 0.3245\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4336 - val_loss: 0.3234\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4319 - val_loss: 0.3244\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4468 - val_loss: 0.3635\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4305 - val_loss: 0.3300\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4252 - val_loss: 0.3380\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4238 - val_loss: 0.3162\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4356 - val_loss: 0.3186\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4304 - val_loss: 0.3958\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4232 - val_loss: 0.3672\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4244 - val_loss: 0.3302\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4377 - val_loss: 0.3246\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4262 - val_loss: 0.3199\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4237 - val_loss: 0.3244\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4274 - val_loss: 0.3243\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4265 - val_loss: 0.3203\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4277 - val_loss: 0.3237\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4260 - val_loss: 0.3540\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4274 - val_loss: 0.3452\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4224 - val_loss: 0.3180\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4230 - val_loss: 0.3270\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4196 - val_loss: 0.3496\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4168 - val_loss: 0.3171\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4141 - val_loss: 0.3168\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4147 - val_loss: 0.4488\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4311 - val_loss: 0.3177\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4177 - val_loss: 0.3127\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4191 - val_loss: 0.3297\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.4126 - val_loss: 0.3068\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4291 - val_loss: 0.3191\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4284 - val_loss: 0.3298\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4100 - val_loss: 0.3035\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4146 - val_loss: 0.3528\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4182 - val_loss: 0.3149\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4155 - val_loss: 0.3905\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4259 - val_loss: 0.3160\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4117 - val_loss: 0.3256\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4113 - val_loss: 0.3309\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4058 - val_loss: 0.3082\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 4s 67ms/step - loss: 5.4565 - val_loss: 0.3623\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4752 - val_loss: 0.3869\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4297 - val_loss: 0.4206\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4196 - val_loss: 0.3227\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4127 - val_loss: 0.3243\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4035 - val_loss: 0.3177\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3790 - val_loss: 0.3520\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3871 - val_loss: 0.4924\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3618 - val_loss: 0.2949\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3821 - val_loss: 0.2757\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5280 - val_loss: 0.2720\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4114 - val_loss: 0.3268\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3944 - val_loss: 0.4038\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3782 - val_loss: 0.4658\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3959 - val_loss: 0.4134\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3713 - val_loss: 0.4111\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3762 - val_loss: 0.3218\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3681 - val_loss: 0.4180\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3749 - val_loss: 0.3536\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3627 - val_loss: 0.3674\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3652 - val_loss: 0.3593\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3512 - val_loss: 0.2983\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3730 - val_loss: 0.3671\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3589 - val_loss: 0.3032\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3657 - val_loss: 0.2880\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3566 - val_loss: 0.4575\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 59ms/step - loss: 0.3594 - val_loss: 0.4974\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3520 - val_loss: 0.3212\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3611 - val_loss: 0.4324\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3686 - val_loss: 0.3237\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3554 - val_loss: 0.4442\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3534 - val_loss: 0.4161\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3449 - val_loss: 0.4368\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3459 - val_loss: 0.4194\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3604 - val_loss: 0.2858\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3555 - val_loss: 0.4245\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3630 - val_loss: 0.3251\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3447 - val_loss: 0.3682\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3615 - val_loss: 0.3247\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3519 - val_loss: 0.3742\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3600 - val_loss: 0.3078\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3451 - val_loss: 0.3589\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3545 - val_loss: 0.3137\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3685 - val_loss: 0.4909\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3485 - val_loss: 0.2959\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3421 - val_loss: 0.3010\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3609 - val_loss: 0.3549\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3444 - val_loss: 0.3416\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3450 - val_loss: 0.4044\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3423 - val_loss: 0.3131\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3476 - val_loss: 0.2879\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3498 - val_loss: 0.3223\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3450 - val_loss: 0.4877\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3463 - val_loss: 0.3930\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3589 - val_loss: 0.3525\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3472 - val_loss: 0.3079\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3445 - val_loss: 0.2736\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3547 - val_loss: 0.2895\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3431 - val_loss: 0.3547\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3404 - val_loss: 0.3668\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3399 - val_loss: 0.2752\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3385 - val_loss: 0.2726\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3375 - val_loss: 0.3274\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3451 - val_loss: 0.3330\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3494 - val_loss: 0.2875\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3397 - val_loss: 0.3321\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3468 - val_loss: 0.5183\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3428 - val_loss: 0.2921\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3473 - val_loss: 0.4075\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3398 - val_loss: 0.3173\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3301 - val_loss: 0.3064\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3401 - val_loss: 0.2839\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3367 - val_loss: 0.2998\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3309 - val_loss: 0.3171\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3345 - val_loss: 0.3704\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3352 - val_loss: 0.4673\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3428 - val_loss: 0.3009\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3357 - val_loss: 0.3685\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3327 - val_loss: 0.3510\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3353 - val_loss: 0.2750\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3393 - val_loss: 0.2732\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3404 - val_loss: 0.5778\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3300 - val_loss: 0.2794\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3307 - val_loss: 0.3119\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3320 - val_loss: 0.5353\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3379 - val_loss: 0.3227\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3418 - val_loss: 0.3071\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3323 - val_loss: 0.3217\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3282 - val_loss: 0.3139\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3272 - val_loss: 0.3179\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3179 - val_loss: 0.3132\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3341 - val_loss: 0.3306\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3131 - val_loss: 0.3589\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2985 - val_loss: 0.3535\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3128 - val_loss: 0.2072\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3123 - val_loss: 0.2200\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3133 - val_loss: 0.2455\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2763 - val_loss: 0.2161\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3225 - val_loss: 0.2502\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2934 - val_loss: 0.2192\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2797 - val_loss: 0.2644\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2829 - val_loss: 0.2727\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3111 - val_loss: 0.2847\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2985 - val_loss: 0.2039\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2759 - val_loss: 0.2402\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3201 - val_loss: 0.2243\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2821 - val_loss: 0.2442\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2991 - val_loss: 0.2562\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2776 - val_loss: 0.2051\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2806 - val_loss: 0.2772\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2786 - val_loss: 0.2686\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3069 - val_loss: 0.4644\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3073 - val_loss: 0.2713\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2796 - val_loss: 0.3073\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2852 - val_loss: 0.2151\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2787 - val_loss: 0.2776\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2928 - val_loss: 0.2368\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2718 - val_loss: 0.2049\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2839 - val_loss: 0.2050\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2808 - val_loss: 0.3355\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2716 - val_loss: 0.2077\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2685 - val_loss: 0.3030\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2757 - val_loss: 0.2984\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2846 - val_loss: 0.2599\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2896 - val_loss: 0.2200\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2693 - val_loss: 0.2562\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2803 - val_loss: 0.2445\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2943 - val_loss: 0.2166\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2783 - val_loss: 0.2098\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2700 - val_loss: 0.2441\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2712 - val_loss: 0.2058\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2729 - val_loss: 0.2548\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2700 - val_loss: 0.2638\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2978 - val_loss: 0.2162\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2858 - val_loss: 0.2708\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2666 - val_loss: 0.2096\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2640 - val_loss: 0.2146\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2675 - val_loss: 0.2616\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3036 - val_loss: 0.2614\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3008 - val_loss: 0.2588\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2714 - val_loss: 0.2893\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2685 - val_loss: 0.2626\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2648 - val_loss: 0.2303\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2715 - val_loss: 0.2774\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2610 - val_loss: 0.2414\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2594 - val_loss: 0.2739\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2731 - val_loss: 0.2655\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2736 - val_loss: 0.2037\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2832 - val_loss: 0.2625\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2612 - val_loss: 0.2452\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2726 - val_loss: 0.2987\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2635 - val_loss: 0.2931\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2697 - val_loss: 0.3144\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2941 - val_loss: 0.2494\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2639 - val_loss: 0.2164\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2753 - val_loss: 0.2145\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2664 - val_loss: 0.2548\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2556 - val_loss: 0.3156\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2941 - val_loss: 0.2780\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2619 - val_loss: 0.2274\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2542 - val_loss: 0.2279\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2585 - val_loss: 0.2352\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2572 - val_loss: 0.2814\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2708 - val_loss: 0.2303\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2921 - val_loss: 0.2541\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2598 - val_loss: 0.2411\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2539 - val_loss: 0.2491\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2522 - val_loss: 0.2129\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2518 - val_loss: 0.2334\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2712 - val_loss: 0.3167\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3305 - val_loss: 0.3329\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2994 - val_loss: 0.2082\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2571 - val_loss: 0.2056\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2582 - val_loss: 0.2262\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2546 - val_loss: 0.2165\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2700 - val_loss: 0.3498\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3245 - val_loss: 0.2178\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2633 - val_loss: 0.4310\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3781 - val_loss: 0.3210\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3515 - val_loss: 0.2987\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3394 - val_loss: 0.3266\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3323 - val_loss: 0.3315\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3328 - val_loss: 0.2930\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3349 - val_loss: 0.3079\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3326 - val_loss: 0.3131\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3249 - val_loss: 0.3018\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3240 - val_loss: 0.3420\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3109 - val_loss: 0.2768\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2851 - val_loss: 0.2034\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2582 - val_loss: 0.2200\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2510 - val_loss: 0.2362\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2665 - val_loss: 0.2359\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2580 - val_loss: 0.2084\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2727 - val_loss: 0.2435\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2650 - val_loss: 0.2464\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2945 - val_loss: 0.2668\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2783 - val_loss: 0.2391\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2756 - val_loss: 0.2606\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2630 - val_loss: 0.2190\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2599 - val_loss: 0.2460\n",
      "8/8 [==============================] - 0s 27ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 5s 78ms/step - loss: 4.4984 - val_loss: 0.3593\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.5887 - val_loss: 0.4014\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5407 - val_loss: 0.3461\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4662 - val_loss: 0.3309\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4456 - val_loss: 0.4361\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4305 - val_loss: 0.3977\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4139 - val_loss: 0.3335\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.4078 - val_loss: 0.3107\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3928 - val_loss: 0.3961\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3886 - val_loss: 0.3894\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4041 - val_loss: 0.3501\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3846 - val_loss: 0.3454\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3713 - val_loss: 0.3310\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3725 - val_loss: 0.4040\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3634 - val_loss: 0.3689\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3827 - val_loss: 0.3162\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3650 - val_loss: 0.3075\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3607 - val_loss: 0.3255\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3582 - val_loss: 0.3093\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3604 - val_loss: 0.3148\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3552 - val_loss: 0.3224\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3524 - val_loss: 0.2956\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3619 - val_loss: 0.3427\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3538 - val_loss: 0.3095\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3588 - val_loss: 0.2704\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3538 - val_loss: 0.3921\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3536 - val_loss: 0.2970\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3678 - val_loss: 0.3365\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3660 - val_loss: 0.3004\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3569 - val_loss: 0.3228\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3597 - val_loss: 0.4828\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3613 - val_loss: 0.3459\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3535 - val_loss: 0.4044\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3590 - val_loss: 0.3812\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3490 - val_loss: 0.3123\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3545 - val_loss: 0.2909\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3522 - val_loss: 0.2853\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3529 - val_loss: 0.3224\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3555 - val_loss: 0.2992\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3784 - val_loss: 0.2951\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3612 - val_loss: 0.2693\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3576 - val_loss: 0.2841\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3478 - val_loss: 0.3017\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3489 - val_loss: 0.3125\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3670 - val_loss: 0.3300\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3582 - val_loss: 0.2575\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3519 - val_loss: 0.2693\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3637 - val_loss: 0.2717\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3499 - val_loss: 0.2966\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3586 - val_loss: 0.4331\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3436 - val_loss: 0.3449\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3531 - val_loss: 0.5297\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3509 - val_loss: 0.2653\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3573 - val_loss: 0.2767\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3494 - val_loss: 0.2516\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3532 - val_loss: 0.2484\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3579 - val_loss: 0.3239\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3370 - val_loss: 0.2934\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3473 - val_loss: 0.3070\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3313 - val_loss: 0.2317\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3359 - val_loss: 0.2424\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3272 - val_loss: 0.2177\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3366 - val_loss: 0.2221\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3084 - val_loss: 0.2501\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3464 - val_loss: 0.2255\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3275 - val_loss: 0.2814\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3164 - val_loss: 0.1997\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3040 - val_loss: 0.2281\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3167 - val_loss: 0.1939\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2758 - val_loss: 0.1897\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2847 - val_loss: 0.2908\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3020 - val_loss: 0.2243\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2675 - val_loss: 0.2291\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3053 - val_loss: 0.2935\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2893 - val_loss: 0.3811\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2852 - val_loss: 0.2699\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3156 - val_loss: 0.2127\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2858 - val_loss: 0.2097\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2986 - val_loss: 0.1839\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2710 - val_loss: 0.3962\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2760 - val_loss: 0.2474\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2727 - val_loss: 0.2040\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2709 - val_loss: 0.2865\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2807 - val_loss: 0.1930\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2845 - val_loss: 0.2724\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2568 - val_loss: 0.3266\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2685 - val_loss: 0.3211\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2720 - val_loss: 0.2580\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2617 - val_loss: 0.1977\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2686 - val_loss: 0.2049\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2870 - val_loss: 0.2762\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2642 - val_loss: 0.2066\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2492 - val_loss: 0.2846\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2569 - val_loss: 0.2758\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2837 - val_loss: 0.2384\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2649 - val_loss: 0.2272\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2669 - val_loss: 0.2094\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2699 - val_loss: 0.2177\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2619 - val_loss: 0.2506\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2536 - val_loss: 0.2334\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2517 - val_loss: 0.2129\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2558 - val_loss: 0.2040\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2537 - val_loss: 0.2455\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2483 - val_loss: 0.2635\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2456 - val_loss: 0.2150\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2493 - val_loss: 0.2583\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2610 - val_loss: 0.3589\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2758 - val_loss: 0.2974\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2875 - val_loss: 0.1946\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2892 - val_loss: 0.2074\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2551 - val_loss: 0.1944\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2504 - val_loss: 0.2935\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2483 - val_loss: 0.2029\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2565 - val_loss: 0.2267\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2551 - val_loss: 0.1987\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2532 - val_loss: 0.2532\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2483 - val_loss: 0.2006\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2528 - val_loss: 0.2129\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2783 - val_loss: 0.2224\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2513 - val_loss: 0.2721\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2514 - val_loss: 0.2675\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2536 - val_loss: 0.3215\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2603 - val_loss: 0.2157\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2535 - val_loss: 0.2995\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2593 - val_loss: 0.2325\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2542 - val_loss: 0.2757\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2642 - val_loss: 0.2452\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2465 - val_loss: 0.2384\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2503 - val_loss: 0.2400\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2534 - val_loss: 0.2099\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2588 - val_loss: 0.3464\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2636 - val_loss: 0.2243\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2470 - val_loss: 0.2022\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2584 - val_loss: 0.2396\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2583 - val_loss: 0.2217\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2633 - val_loss: 0.2089\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2617 - val_loss: 0.2781\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2432 - val_loss: 0.2615\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2475 - val_loss: 0.2703\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2497 - val_loss: 0.2881\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2575 - val_loss: 0.2418\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.5465 - val_loss: 0.3046\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3571 - val_loss: 0.5288\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2803 - val_loss: 0.2438\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2565 - val_loss: 0.2596\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2798 - val_loss: 0.2653\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2878 - val_loss: 0.2837\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2543 - val_loss: 0.2389\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2522 - val_loss: 0.2086\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2581 - val_loss: 0.2606\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2579 - val_loss: 0.2102\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2477 - val_loss: 0.2221\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2475 - val_loss: 0.2371\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2426 - val_loss: 0.2210\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2666 - val_loss: 0.4492\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2634 - val_loss: 0.3144\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2479 - val_loss: 0.2879\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2514 - val_loss: 0.2096\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2521 - val_loss: 0.2258\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2432 - val_loss: 0.3424\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2434 - val_loss: 0.2576\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2466 - val_loss: 0.2922\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2457 - val_loss: 0.2566\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2495 - val_loss: 0.3464\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2722 - val_loss: 0.2099\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2725 - val_loss: 0.2140\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2557 - val_loss: 0.2406\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2467 - val_loss: 0.3445\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2496 - val_loss: 0.2301\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2458 - val_loss: 0.2223\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2649 - val_loss: 0.2298\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2489 - val_loss: 0.2162\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2421 - val_loss: 0.2302\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2446 - val_loss: 0.2259\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2416 - val_loss: 0.2275\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2423 - val_loss: 0.2215\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2590 - val_loss: 0.2419\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2492 - val_loss: 0.2283\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2448 - val_loss: 0.2177\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2410 - val_loss: 0.2373\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2420 - val_loss: 0.2140\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2565 - val_loss: 0.3518\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2679 - val_loss: 0.2303\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2508 - val_loss: 0.3083\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2888 - val_loss: 0.2388\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2529 - val_loss: 0.2297\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2514 - val_loss: 0.3993\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2493 - val_loss: 0.2760\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2425 - val_loss: 0.2671\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2526 - val_loss: 0.3383\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2493 - val_loss: 0.2350\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2440 - val_loss: 0.3104\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2530 - val_loss: 0.2056\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2518 - val_loss: 0.2429\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2465 - val_loss: 0.3098\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2421 - val_loss: 0.2715\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2946 - val_loss: 0.2958\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2573 - val_loss: 0.2768\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2470 - val_loss: 0.3007\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2460 - val_loss: 0.2510\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2464 - val_loss: 0.2667\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2406 - val_loss: 0.2887\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2607 - val_loss: 0.2753\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2552 - val_loss: 0.2378\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2445 - val_loss: 0.2263\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2477 - val_loss: 0.2198\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2421 - val_loss: 0.2034\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2443 - val_loss: 0.2418\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2447 - val_loss: 0.2165\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2443 - val_loss: 0.2769\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2392 - val_loss: 0.2080\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2425 - val_loss: 0.2149\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2573 - val_loss: 0.2831\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2381 - val_loss: 0.3041\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2434 - val_loss: 0.3567\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2505 - val_loss: 0.1975\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2512 - val_loss: 0.2794\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2423 - val_loss: 0.2662\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2500 - val_loss: 0.2734\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2411 - val_loss: 0.2614\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2374 - val_loss: 0.2607\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2557 - val_loss: 0.2356\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2486 - val_loss: 0.3596\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2500 - val_loss: 0.2377\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2473 - val_loss: 0.3049\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2484 - val_loss: 0.3207\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2371 - val_loss: 0.1889\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2438 - val_loss: 0.2988\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2443 - val_loss: 0.2348\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2353 - val_loss: 0.2627\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2390 - val_loss: 0.2284\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2370 - val_loss: 0.2502\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2472 - val_loss: 0.2794\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2527 - val_loss: 0.2160\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2369 - val_loss: 0.2771\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2447 - val_loss: 0.2107\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2474 - val_loss: 0.1915\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2468 - val_loss: 0.2463\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2371 - val_loss: 0.2492\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2408 - val_loss: 0.2542\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2406 - val_loss: 0.2385\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2403 - val_loss: 0.2327\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2469 - val_loss: 0.2863\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2598 - val_loss: 0.3021\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2458 - val_loss: 0.2163\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2452 - val_loss: 0.3055\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2358 - val_loss: 0.2502\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2365 - val_loss: 0.2457\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2364 - val_loss: 0.2503\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2326 - val_loss: 0.2431\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2377 - val_loss: 0.2225\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2411 - val_loss: 0.2580\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2451 - val_loss: 0.2022\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2347 - val_loss: 0.2448\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2320 - val_loss: 0.2922\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2412 - val_loss: 0.3261\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2454 - val_loss: 0.1986\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2327 - val_loss: 0.2865\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2348 - val_loss: 0.2216\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2465 - val_loss: 0.2078\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2495 - val_loss: 0.2999\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2503 - val_loss: 0.1937\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2361 - val_loss: 0.2610\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2455 - val_loss: 0.3068\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2404 - val_loss: 0.3666\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2373 - val_loss: 0.3280\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2428 - val_loss: 0.2582\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2336 - val_loss: 0.2727\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2427 - val_loss: 0.2229\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2309 - val_loss: 0.2846\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2284 - val_loss: 0.2101\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2317 - val_loss: 0.2129\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2374 - val_loss: 0.2650\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2485 - val_loss: 0.2413\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2371 - val_loss: 0.2701\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2398 - val_loss: 0.2351\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2282 - val_loss: 0.3256\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2475 - val_loss: 0.2720\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2833 - val_loss: 0.2336\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2337 - val_loss: 0.2183\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2316 - val_loss: 0.2272\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2307 - val_loss: 0.3215\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2309 - val_loss: 0.2896\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2291 - val_loss: 0.2241\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2354 - val_loss: 0.2001\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2371 - val_loss: 0.3607\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2387 - val_loss: 0.2243\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2301 - val_loss: 0.1954\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2422 - val_loss: 0.1970\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2427 - val_loss: 0.2670\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2396 - val_loss: 0.3355\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2474 - val_loss: 0.2398\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2321 - val_loss: 0.2896\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2217 - val_loss: 0.1976\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2309 - val_loss: 0.3321\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2431 - val_loss: 0.2308\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2462 - val_loss: 0.2641\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2274 - val_loss: 0.2863\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2333 - val_loss: 0.2579\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2266 - val_loss: 0.2762\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2230 - val_loss: 0.2080\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2307 - val_loss: 0.2368\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2536 - val_loss: 0.3397\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2336 - val_loss: 0.2125\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2422 - val_loss: 0.2096\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2287 - val_loss: 0.2046\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2291 - val_loss: 0.2562\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2481 - val_loss: 0.2313\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2302 - val_loss: 0.2185\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2265 - val_loss: 0.2686\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2215 - val_loss: 0.2387\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2194 - val_loss: 0.3006\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2267 - val_loss: 0.2591\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2374 - val_loss: 0.3047\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2502 - val_loss: 0.2721\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2387 - val_loss: 0.2178\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2264 - val_loss: 0.1957\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2214 - val_loss: 0.1919\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2363 - val_loss: 0.2777\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2240 - val_loss: 0.2149\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2266 - val_loss: 0.2412\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2364 - val_loss: 0.1947\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2315 - val_loss: 0.2540\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2283 - val_loss: 0.3151\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2233 - val_loss: 0.2325\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2507 - val_loss: 0.3693\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2263 - val_loss: 0.2580\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2234 - val_loss: 0.2427\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2275 - val_loss: 0.2340\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2212 - val_loss: 0.2838\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2220 - val_loss: 0.1905\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2294 - val_loss: 0.2442\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2291 - val_loss: 0.2634\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2205 - val_loss: 0.2206\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2200 - val_loss: 0.2417\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2300 - val_loss: 0.1947\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2229 - val_loss: 0.2664\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2227 - val_loss: 0.2238\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2306 - val_loss: 0.2457\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2279 - val_loss: 0.3366\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2232 - val_loss: 0.2483\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2220 - val_loss: 0.2519\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2250 - val_loss: 0.2040\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2227 - val_loss: 0.2861\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2227 - val_loss: 0.2894\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2217 - val_loss: 0.2628\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2185 - val_loss: 0.3060\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2308 - val_loss: 0.2624\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2284 - val_loss: 0.2666\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2318 - val_loss: 0.3356\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2331 - val_loss: 0.2607\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2167 - val_loss: 0.2088\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2191 - val_loss: 0.2601\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2140 - val_loss: 0.1757\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2199 - val_loss: 0.2461\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2238 - val_loss: 0.2095\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2252 - val_loss: 0.2108\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2128 - val_loss: 0.3253\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2179 - val_loss: 0.2621\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2201 - val_loss: 0.1855\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2205 - val_loss: 0.3080\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2195 - val_loss: 0.2076\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2211 - val_loss: 0.2110\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2170 - val_loss: 0.2622\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2151 - val_loss: 0.2925\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2156 - val_loss: 0.2340\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2247 - val_loss: 0.3452\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2214 - val_loss: 0.3037\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2198 - val_loss: 0.2017\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2188 - val_loss: 0.1837\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2380 - val_loss: 0.2348\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2205 - val_loss: 0.2690\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2164 - val_loss: 0.2322\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2323 - val_loss: 0.2356\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2380 - val_loss: 0.2120\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2160 - val_loss: 0.2491\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2197 - val_loss: 0.2761\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2210 - val_loss: 0.1958\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2167 - val_loss: 0.1862\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2164 - val_loss: 0.2204\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2134 - val_loss: 0.1934\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2263 - val_loss: 0.1897\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2232 - val_loss: 0.1933\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2198 - val_loss: 0.2013\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2131 - val_loss: 0.2064\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2218 - val_loss: 0.2773\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2169 - val_loss: 0.1755\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2288 - val_loss: 0.2872\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2150 - val_loss: 0.3520\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2230 - val_loss: 0.2089\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2174 - val_loss: 0.1747\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2187 - val_loss: 0.1893\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2095 - val_loss: 0.3310\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2193 - val_loss: 0.2291\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2166 - val_loss: 0.4808\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2245 - val_loss: 0.3051\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2091 - val_loss: 0.2226\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2117 - val_loss: 0.2141\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2137 - val_loss: 0.3064\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2122 - val_loss: 0.2221\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2150 - val_loss: 0.2098\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2308 - val_loss: 0.2159\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2195 - val_loss: 0.3190\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2301 - val_loss: 0.2103\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2143 - val_loss: 0.2648\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2107 - val_loss: 0.2291\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2128 - val_loss: 0.3236\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2133 - val_loss: 0.2159\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2137 - val_loss: 0.2862\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2163 - val_loss: 0.2234\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2071 - val_loss: 0.2128\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2097 - val_loss: 0.2834\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2095 - val_loss: 0.2135\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2074 - val_loss: 0.3152\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2119 - val_loss: 0.2547\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2155 - val_loss: 0.2482\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2080 - val_loss: 0.3221\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2113 - val_loss: 0.1960\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2164 - val_loss: 0.2079\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2351 - val_loss: 0.2327\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2141 - val_loss: 0.3020\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2091 - val_loss: 0.2834\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2062 - val_loss: 0.1743\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2103 - val_loss: 0.3124\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2091 - val_loss: 0.2870\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2086 - val_loss: 0.2035\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2065 - val_loss: 0.1888\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2331 - val_loss: 0.2030\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2168 - val_loss: 0.2257\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2049 - val_loss: 0.2887\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2085 - val_loss: 0.1818\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2100 - val_loss: 0.2316\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2031 - val_loss: 0.1962\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2031 - val_loss: 0.1926\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2132 - val_loss: 0.3198\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2125 - val_loss: 0.3094\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2126 - val_loss: 0.3096\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2096 - val_loss: 0.2859\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2088 - val_loss: 0.1895\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2102 - val_loss: 0.2809\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2066 - val_loss: 0.2064\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2120 - val_loss: 0.1767\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2257 - val_loss: 0.2000\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2104 - val_loss: 0.1775\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2220 - val_loss: 0.2227\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2319 - val_loss: 0.2047\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2052 - val_loss: 0.2052\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2050 - val_loss: 0.2860\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2232 - val_loss: 0.1846\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2062 - val_loss: 0.2512\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2070 - val_loss: 0.2549\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2047 - val_loss: 0.2391\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2102 - val_loss: 0.2941\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2045 - val_loss: 0.2843\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2377 - val_loss: 0.2455\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2164 - val_loss: 0.2476\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2142 - val_loss: 0.2716\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2049 - val_loss: 0.2482\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2111 - val_loss: 0.2051\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2097 - val_loss: 0.2099\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2065 - val_loss: 0.2895\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2043 - val_loss: 0.2071\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2075 - val_loss: 0.1851\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2102 - val_loss: 0.2612\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2068 - val_loss: 0.2236\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2088 - val_loss: 0.3198\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2083 - val_loss: 0.3483\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2022 - val_loss: 0.3024\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2016 - val_loss: 0.1863\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2043 - val_loss: 0.2016\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2216 - val_loss: 0.2241\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2143 - val_loss: 0.2507\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2055 - val_loss: 0.2421\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2047 - val_loss: 0.2320\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2018 - val_loss: 0.3272\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2069 - val_loss: 0.2475\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2127 - val_loss: 0.2253\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1997 - val_loss: 0.1956\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.1999 - val_loss: 0.2316\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2025 - val_loss: 0.2428\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2001 - val_loss: 0.2918\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2051 - val_loss: 0.1789\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2129 - val_loss: 0.1625\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2070 - val_loss: 0.1940\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2165 - val_loss: 0.2375\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2240 - val_loss: 0.2019\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2418 - val_loss: 0.2361\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2523 - val_loss: 0.2622\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2393 - val_loss: 0.1912\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2120 - val_loss: 0.1791\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2063 - val_loss: 0.2039\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2062 - val_loss: 0.2385\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1975 - val_loss: 0.1791\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2026 - val_loss: 0.1806\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2061 - val_loss: 0.2399\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1984 - val_loss: 0.2641\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2036 - val_loss: 0.1735\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2025 - val_loss: 0.2891\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2004 - val_loss: 0.2191\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1979 - val_loss: 0.2718\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 67ms/step - loss: 5081.7798 - val_loss: 0.4486\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4971 - val_loss: 0.3286\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4081 - val_loss: 0.4342\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3817 - val_loss: 0.2800\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3795 - val_loss: 0.3012\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3821 - val_loss: 0.2856\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4100 - val_loss: 0.2883\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3837 - val_loss: 0.3404\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3615 - val_loss: 0.3419\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3529 - val_loss: 0.2665\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3480 - val_loss: 0.2597\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3301 - val_loss: 0.2459\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3559 - val_loss: 0.2525\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3449 - val_loss: 0.1952\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3127 - val_loss: 0.2270\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3228 - val_loss: 0.3163\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3043 - val_loss: 0.2661\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2930 - val_loss: 0.1937\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2834 - val_loss: 0.4153\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3196 - val_loss: 0.3840\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2985 - val_loss: 0.2362\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2870 - val_loss: 0.2091\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2936 - val_loss: 0.2524\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2859 - val_loss: 0.2664\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2699 - val_loss: 0.2334\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2697 - val_loss: 0.4293\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3052 - val_loss: 0.2411\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2896 - val_loss: 0.2331\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2822 - val_loss: 0.2055\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2677 - val_loss: 0.2037\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2629 - val_loss: 0.2035\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2689 - val_loss: 0.2678\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2621 - val_loss: 0.2715\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2508 - val_loss: 0.2279\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2583 - val_loss: 0.2119\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2887 - val_loss: 0.2635\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3018 - val_loss: 0.2437\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2758 - val_loss: 0.2460\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2614 - val_loss: 0.2055\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2515 - val_loss: 0.2350\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2506 - val_loss: 0.3132\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2488 - val_loss: 0.2600\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3361 - val_loss: 0.2461\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2919 - val_loss: 0.2847\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2702 - val_loss: 0.2234\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2582 - val_loss: 0.2680\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2552 - val_loss: 0.2305\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2649 - val_loss: 0.2328\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2732 - val_loss: 0.2593\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2620 - val_loss: 0.3260\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2943 - val_loss: 0.2995\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3186 - val_loss: 0.2704\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2636 - val_loss: 0.2768\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2573 - val_loss: 0.3741\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2544 - val_loss: 0.2647\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2583 - val_loss: 0.2027\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2551 - val_loss: 0.2393\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2604 - val_loss: 0.2572\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2536 - val_loss: 0.2729\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2663 - val_loss: 0.2082\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2452 - val_loss: 0.2299\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2600 - val_loss: 0.3946\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2562 - val_loss: 0.2700\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2497 - val_loss: 0.2213\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2566 - val_loss: 0.3438\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2487 - val_loss: 0.2479\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2399 - val_loss: 0.2784\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2443 - val_loss: 0.2821\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3007 - val_loss: 0.2348\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2599 - val_loss: 0.2024\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2513 - val_loss: 0.2155\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2780 - val_loss: 0.2162\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2653 - val_loss: 0.2082\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2566 - val_loss: 0.2160\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2484 - val_loss: 0.2500\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2499 - val_loss: 0.2231\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2452 - val_loss: 0.2122\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2459 - val_loss: 0.2147\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2455 - val_loss: 0.2492\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2553 - val_loss: 0.2117\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2580 - val_loss: 0.2141\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2437 - val_loss: 0.3011\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2564 - val_loss: 0.2085\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2600 - val_loss: 0.2205\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2520 - val_loss: 0.2421\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2443 - val_loss: 0.1942\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2518 - val_loss: 0.2230\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2437 - val_loss: 0.2570\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2539 - val_loss: 0.2341\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2530 - val_loss: 0.2034\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2442 - val_loss: 0.2218\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2456 - val_loss: 0.2054\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2465 - val_loss: 0.2091\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2868 - val_loss: 0.2355\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2833 - val_loss: 0.2040\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2470 - val_loss: 0.2284\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2569 - val_loss: 0.2038\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2572 - val_loss: 0.3034\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2456 - val_loss: 0.2159\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2938 - val_loss: 0.2428\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 4s 65ms/step - loss: 1.4053 - val_loss: 0.3283\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4479 - val_loss: 0.3446\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4073 - val_loss: 0.5596\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4420 - val_loss: 0.3798\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3935 - val_loss: 0.3271\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3905 - val_loss: 0.3428\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3653 - val_loss: 0.3901\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3980 - val_loss: 0.3787\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3674 - val_loss: 0.2545\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3371 - val_loss: 0.3398\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3369 - val_loss: 0.2902\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3363 - val_loss: 0.2484\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3940 - val_loss: 0.3268\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3299 - val_loss: 0.3413\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4166 - val_loss: 0.3122\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.4148 - val_loss: 0.3655\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3982 - val_loss: 0.3505\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3972 - val_loss: 0.3448\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3633 - val_loss: 0.4433\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3860 - val_loss: 0.5816\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3767 - val_loss: 0.3946\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3554 - val_loss: 0.2929\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3669 - val_loss: 0.3123\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3783 - val_loss: 0.3022\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3536 - val_loss: 0.4064\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3389 - val_loss: 0.3186\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3437 - val_loss: 0.3408\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3480 - val_loss: 0.3129\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3434 - val_loss: 0.3405\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3518 - val_loss: 0.2662\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3364 - val_loss: 0.2760\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3306 - val_loss: 0.4318\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3509 - val_loss: 0.2791\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3296 - val_loss: 0.2866\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3412 - val_loss: 0.2957\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3371 - val_loss: 0.2778\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3581 - val_loss: 0.3328\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3459 - val_loss: 0.2806\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3441 - val_loss: 0.4487\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3328 - val_loss: 0.2692\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3338 - val_loss: 0.2767\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3274 - val_loss: 0.3895\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3364 - val_loss: 0.2690\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3291 - val_loss: 0.2830\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3470 - val_loss: 0.2902\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3303 - val_loss: 0.3480\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3240 - val_loss: 0.2688\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3240 - val_loss: 0.2727\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3247 - val_loss: 0.3392\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3251 - val_loss: 0.3134\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3219 - val_loss: 0.3078\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3415 - val_loss: 0.2779\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3409 - val_loss: 0.3763\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3248 - val_loss: 0.2875\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3324 - val_loss: 0.2906\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3296 - val_loss: 0.2870\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3349 - val_loss: 0.2831\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3332 - val_loss: 0.3582\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3323 - val_loss: 0.3721\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3378 - val_loss: 0.2615\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3343 - val_loss: 0.3220\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3441 - val_loss: 0.2490\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3269 - val_loss: 0.2739\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3249 - val_loss: 0.3253\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3342 - val_loss: 0.2882\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3338 - val_loss: 0.2770\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3220 - val_loss: 0.2865\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3364 - val_loss: 0.3096\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3235 - val_loss: 0.3001\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3205 - val_loss: 0.2721\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3241 - val_loss: 0.2565\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3227 - val_loss: 0.2696\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3220 - val_loss: 0.2748\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3359 - val_loss: 0.2802\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3408 - val_loss: 0.3178\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3416 - val_loss: 0.2908\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3349 - val_loss: 0.2994\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3338 - val_loss: 0.2654\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3272 - val_loss: 0.2872\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3236 - val_loss: 0.2873\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3229 - val_loss: 0.3145\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3293 - val_loss: 0.2918\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3376 - val_loss: 0.3179\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3179 - val_loss: 0.2606\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3321 - val_loss: 0.2976\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3238 - val_loss: 0.2748\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3215 - val_loss: 0.2686\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3403 - val_loss: 0.2359\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3362 - val_loss: 0.2694\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3142 - val_loss: 0.3265\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3211 - val_loss: 0.3445\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3133 - val_loss: 0.2902\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3114 - val_loss: 0.2937\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3190 - val_loss: 0.2923\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3160 - val_loss: 0.2907\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3181 - val_loss: 0.2618\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3204 - val_loss: 0.2977\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3252 - val_loss: 0.2807\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3322 - val_loss: 0.2592\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3205 - val_loss: 0.2624\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3224 - val_loss: 0.2861\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3317 - val_loss: 0.2941\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3204 - val_loss: 0.2647\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3186 - val_loss: 0.3034\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3169 - val_loss: 0.2989\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3160 - val_loss: 0.2610\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3208 - val_loss: 0.2714\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3205 - val_loss: 0.2981\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3110 - val_loss: 0.2609\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3183 - val_loss: 0.2973\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3212 - val_loss: 0.2739\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3314 - val_loss: 0.2716\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3199 - val_loss: 0.2810\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3187 - val_loss: 0.2521\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3200 - val_loss: 0.2884\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3152 - val_loss: 0.2653\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3203 - val_loss: 0.2859\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3152 - val_loss: 0.3212\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3116 - val_loss: 0.2958\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3209 - val_loss: 0.3359\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3466 - val_loss: 0.2619\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3134 - val_loss: 0.3535\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3216 - val_loss: 0.4448\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3400 - val_loss: 0.3507\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3132 - val_loss: 0.3317\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3252 - val_loss: 0.2830\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3155 - val_loss: 0.3446\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3066 - val_loss: 0.2541\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3123 - val_loss: 0.2836\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3054 - val_loss: 0.3023\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3079 - val_loss: 0.2513\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3087 - val_loss: 0.2996\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3140 - val_loss: 0.2762\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3134 - val_loss: 0.2674\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3086 - val_loss: 0.2981\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3145 - val_loss: 0.4656\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3204 - val_loss: 0.2556\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3131 - val_loss: 0.2574\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3084 - val_loss: 0.2636\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3108 - val_loss: 0.2590\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3129 - val_loss: 0.2740\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3219 - val_loss: 0.2611\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3097 - val_loss: 0.2608\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3204 - val_loss: 0.2517\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3110 - val_loss: 0.3325\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3064 - val_loss: 0.2782\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3015 - val_loss: 0.3096\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3064 - val_loss: 0.3225\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3108 - val_loss: 0.2863\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3073 - val_loss: 0.2882\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3306 - val_loss: 0.2626\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3264 - val_loss: 0.2929\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3211 - val_loss: 0.2736\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3085 - val_loss: 0.2760\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3060 - val_loss: 0.2565\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3041 - val_loss: 0.2983\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3167 - val_loss: 0.2577\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3158 - val_loss: 0.2791\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3159 - val_loss: 0.2631\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3105 - val_loss: 0.2574\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2999 - val_loss: 0.3131\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3157 - val_loss: 0.2595\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3092 - val_loss: 0.2536\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3129 - val_loss: 0.2899\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3048 - val_loss: 0.2714\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2999 - val_loss: 0.2790\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3056 - val_loss: 0.2500\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3123 - val_loss: 0.2610\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3043 - val_loss: 0.4124\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3102 - val_loss: 0.2376\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3126 - val_loss: 0.3786\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3088 - val_loss: 0.2585\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3146 - val_loss: 0.2560\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3191 - val_loss: 0.2941\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3066 - val_loss: 0.2780\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3198 - val_loss: 0.3041\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3058 - val_loss: 0.2680\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3042 - val_loss: 0.3372\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3145 - val_loss: 0.3339\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2994 - val_loss: 0.2652\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3070 - val_loss: 0.2866\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3001 - val_loss: 0.2655\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3098 - val_loss: 0.2579\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3082 - val_loss: 0.3570\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3046 - val_loss: 0.3190\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3137 - val_loss: 0.2646\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3003 - val_loss: 0.2544\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3028 - val_loss: 0.2518\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3034 - val_loss: 0.2761\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3186 - val_loss: 0.2983\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3089 - val_loss: 0.3323\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3018 - val_loss: 0.2573\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3177 - val_loss: 0.2737\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3007 - val_loss: 0.2878\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2977 - val_loss: 0.3077\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2989 - val_loss: 0.2653\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3018 - val_loss: 0.2617\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3085 - val_loss: 0.2438\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3035 - val_loss: 0.2385\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 4s 65ms/step - loss: 36.2763 - val_loss: 0.3554\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.5342 - val_loss: 0.3945\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.4577 - val_loss: 0.3388\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.4171 - val_loss: 0.3130\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3964 - val_loss: 0.2952\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3765 - val_loss: 0.2863\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3609 - val_loss: 0.2823\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3892 - val_loss: 0.3004\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3664 - val_loss: 0.2868\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3749 - val_loss: 0.4724\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3688 - val_loss: 0.2899\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3585 - val_loss: 0.3893\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3498 - val_loss: 0.4405\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3614 - val_loss: 0.5289\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3576 - val_loss: 0.3732\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3543 - val_loss: 0.4249\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3515 - val_loss: 0.3114\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3478 - val_loss: 0.4925\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3673 - val_loss: 0.2989\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3666 - val_loss: 0.3345\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3650 - val_loss: 0.3260\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3552 - val_loss: 0.2956\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3398 - val_loss: 0.3612\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3490 - val_loss: 0.4884\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3557 - val_loss: 0.2953\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3453 - val_loss: 0.3079\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3554 - val_loss: 0.4811\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3424 - val_loss: 0.2988\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3431 - val_loss: 0.3517\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3392 - val_loss: 0.3000\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3406 - val_loss: 0.2950\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.3366 - val_loss: 0.3112\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3414 - val_loss: 0.3026\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3452 - val_loss: 0.3333\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3366 - val_loss: 0.2861\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3507 - val_loss: 0.2577\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3282 - val_loss: 0.2620\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3501 - val_loss: 0.2608\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3343 - val_loss: 0.3180\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3709 - val_loss: 0.3658\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3311 - val_loss: 0.3078\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3104 - val_loss: 0.2243\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2900 - val_loss: 0.3197\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3544 - val_loss: 0.3761\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.3495 - val_loss: 0.3358\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3359 - val_loss: 0.2003\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2869 - val_loss: 0.2491\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2807 - val_loss: 0.2569\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2879 - val_loss: 0.2195\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3401 - val_loss: 0.3000\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3074 - val_loss: 0.2557\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2999 - val_loss: 0.6188\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2654 - val_loss: 0.2123\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2915 - val_loss: 0.2473\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2734 - val_loss: 0.2757\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3561 - val_loss: 0.3439\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3420 - val_loss: 0.2831\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3438 - val_loss: 0.3727\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3356 - val_loss: 0.2840\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3324 - val_loss: 0.2744\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3471 - val_loss: 0.2992\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3320 - val_loss: 0.4730\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3180 - val_loss: 0.2093\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2799 - val_loss: 0.2639\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2738 - val_loss: 0.2132\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2645 - val_loss: 0.2034\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2657 - val_loss: 0.2074\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2845 - val_loss: 0.2367\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2727 - val_loss: 0.2441\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2549 - val_loss: 0.2226\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2730 - val_loss: 0.2568\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3198 - val_loss: 0.3254\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2652 - val_loss: 0.2057\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2599 - val_loss: 0.1893\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2595 - val_loss: 0.3618\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2859 - val_loss: 0.2396\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3185 - val_loss: 0.2181\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.2843 - val_loss: 0.2249\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2518 - val_loss: 0.2213\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2574 - val_loss: 0.1987\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2793 - val_loss: 0.2528\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2723 - val_loss: 0.3971\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2564 - val_loss: 0.2399\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2749 - val_loss: 0.1931\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2580 - val_loss: 0.2341\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2754 - val_loss: 0.3818\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2998 - val_loss: 0.2121\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2584 - val_loss: 0.2166\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2482 - val_loss: 0.2719\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2679 - val_loss: 0.1912\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2674 - val_loss: 0.3082\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2644 - val_loss: 0.2385\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2479 - val_loss: 0.2097\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2485 - val_loss: 0.2025\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2727 - val_loss: 0.2674\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2543 - val_loss: 0.2388\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2551 - val_loss: 0.2364\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2523 - val_loss: 0.2012\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2534 - val_loss: 0.2835\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2616 - val_loss: 0.2542\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2499 - val_loss: 0.2421\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2549 - val_loss: 0.2048\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2505 - val_loss: 0.2243\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2721 - val_loss: 0.2474\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2512 - val_loss: 0.2014\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2494 - val_loss: 0.2210\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2596 - val_loss: 0.3003\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2623 - val_loss: 0.1965\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2570 - val_loss: 0.1989\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2805 - val_loss: 0.2254\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2879 - val_loss: 0.2359\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2501 - val_loss: 0.2067\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2507 - val_loss: 0.2102\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2524 - val_loss: 0.1818\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2514 - val_loss: 0.3070\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2584 - val_loss: 0.2225\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2548 - val_loss: 0.2241\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.3245 - val_loss: 0.3284\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2531 - val_loss: 0.3387\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2965 - val_loss: 0.2112\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2649 - val_loss: 0.2305\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2518 - val_loss: 0.2051\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2671 - val_loss: 0.3554\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2617 - val_loss: 0.3157\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2673 - val_loss: 0.2867\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2437 - val_loss: 0.1966\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2494 - val_loss: 0.3212\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2511 - val_loss: 0.2037\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2552 - val_loss: 0.2023\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2517 - val_loss: 0.1918\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2664 - val_loss: 0.2442\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2835 - val_loss: 0.2096\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2615 - val_loss: 0.3343\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2837 - val_loss: 0.2233\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2665 - val_loss: 0.2307\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2450 - val_loss: 0.1913\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2608 - val_loss: 0.1979\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2883 - val_loss: 0.2808\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2727 - val_loss: 0.2361\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2752 - val_loss: 0.2497\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2697 - val_loss: 0.2295\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2586 - val_loss: 0.1782\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2618 - val_loss: 0.1950\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2630 - val_loss: 0.3877\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2796 - val_loss: 0.2767\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2624 - val_loss: 0.2323\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2476 - val_loss: 0.1925\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2539 - val_loss: 0.2106\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2485 - val_loss: 0.1901\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2571 - val_loss: 0.2405\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2539 - val_loss: 0.3575\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2524 - val_loss: 0.2746\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2455 - val_loss: 0.2982\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2528 - val_loss: 0.4931\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2586 - val_loss: 0.4030\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2452 - val_loss: 0.2122\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2739 - val_loss: 0.2246\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2771 - val_loss: 0.1940\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2505 - val_loss: 0.4120\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2504 - val_loss: 0.1978\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2488 - val_loss: 0.2215\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2428 - val_loss: 0.2094\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2432 - val_loss: 0.2508\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2508 - val_loss: 0.3453\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2489 - val_loss: 0.1963\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2700 - val_loss: 0.2591\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2499 - val_loss: 0.2288\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2566 - val_loss: 0.1988\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2529 - val_loss: 0.3107\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2412 - val_loss: 0.1884\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2687 - val_loss: 0.2406\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2647 - val_loss: 0.3480\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2705 - val_loss: 0.3220\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2515 - val_loss: 0.1831\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2500 - val_loss: 0.3643\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2555 - val_loss: 0.2422\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2433 - val_loss: 0.2200\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2561 - val_loss: 0.2915\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2652 - val_loss: 0.2886\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2673 - val_loss: 0.2170\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2455 - val_loss: 0.1804\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2480 - val_loss: 0.2576\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2449 - val_loss: 0.2058\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2440 - val_loss: 0.4657\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2629 - val_loss: 0.2123\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2548 - val_loss: 0.2053\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2524 - val_loss: 0.2139\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2482 - val_loss: 0.2352\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2440 - val_loss: 0.2465\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2423 - val_loss: 0.2561\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2532 - val_loss: 0.2256\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2500 - val_loss: 0.4510\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2588 - val_loss: 0.2946\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2618 - val_loss: 0.2198\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2541 - val_loss: 0.1821\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2496 - val_loss: 0.2705\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2445 - val_loss: 0.2832\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2496 - val_loss: 0.2908\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2456 - val_loss: 0.2216\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2480 - val_loss: 0.2088\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2557 - val_loss: 0.2163\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2491 - val_loss: 0.2553\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2438 - val_loss: 0.2277\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2473 - val_loss: 0.1802\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2564 - val_loss: 0.2258\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2422 - val_loss: 0.2763\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2407 - val_loss: 0.2852\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2470 - val_loss: 0.2831\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2426 - val_loss: 0.2543\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2506 - val_loss: 0.2257\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2464 - val_loss: 0.1892\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2448 - val_loss: 0.2887\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2471 - val_loss: 0.2601\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2877 - val_loss: 0.2848\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2497 - val_loss: 0.3030\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2645 - val_loss: 0.2725\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2479 - val_loss: 0.1995\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2636 - val_loss: 0.1982\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2516 - val_loss: 0.2677\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2427 - val_loss: 0.1943\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2549 - val_loss: 0.2830\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2446 - val_loss: 0.1894\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2549 - val_loss: 0.3487\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2483 - val_loss: 0.2503\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2435 - val_loss: 0.1986\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2653 - val_loss: 0.2105\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2468 - val_loss: 0.2065\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2468 - val_loss: 0.2108\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2427 - val_loss: 0.2749\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2409 - val_loss: 0.1856\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2539 - val_loss: 0.2043\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2437 - val_loss: 0.2150\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2424 - val_loss: 0.2254\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2404 - val_loss: 0.2066\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2583 - val_loss: 0.2366\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2939 - val_loss: 0.2190\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2535 - val_loss: 0.2127\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3000 - val_loss: 0.2286\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2566 - val_loss: 0.2458\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2471 - val_loss: 0.4511\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3094 - val_loss: 0.2971\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2531 - val_loss: 0.2320\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2945 - val_loss: 0.2434\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.3137 - val_loss: 0.3581\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2997 - val_loss: 0.2318\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2505 - val_loss: 0.2183\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2368 - val_loss: 0.4099\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2453 - val_loss: 0.2207\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2396 - val_loss: 0.2069\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2409 - val_loss: 0.2116\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2409 - val_loss: 0.2452\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2404 - val_loss: 0.2612\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2816 - val_loss: 0.2470\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.3491 - val_loss: 0.3038\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3265 - val_loss: 0.2650\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2912 - val_loss: 0.2631\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2568 - val_loss: 0.3236\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2499 - val_loss: 0.2301\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2480 - val_loss: 0.3449\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2382 - val_loss: 0.2021\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2435 - val_loss: 0.1997\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2451 - val_loss: 0.2424\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2509 - val_loss: 0.2612\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2473 - val_loss: 0.2133\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2354 - val_loss: 0.3685\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2407 - val_loss: 0.2433\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2604 - val_loss: 0.1990\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2527 - val_loss: 0.3133\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2370 - val_loss: 0.2689\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2405 - val_loss: 0.2432\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2382 - val_loss: 0.2628\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2369 - val_loss: 0.2155\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2314 - val_loss: 0.2588\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2377 - val_loss: 0.2190\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2325 - val_loss: 0.1976\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2346 - val_loss: 0.3655\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2459 - val_loss: 0.2289\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2396 - val_loss: 0.2209\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2869 - val_loss: 0.3180\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2419 - val_loss: 0.2048\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2417 - val_loss: 0.2305\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2449 - val_loss: 0.3377\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2562 - val_loss: 0.3181\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2392 - val_loss: 0.4252\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2437 - val_loss: 0.2332\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2363 - val_loss: 0.2116\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2373 - val_loss: 0.2032\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2336 - val_loss: 0.2004\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2344 - val_loss: 0.2100\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2338 - val_loss: 0.2308\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2360 - val_loss: 0.2077\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2369 - val_loss: 0.2205\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2350 - val_loss: 0.2779\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2319 - val_loss: 0.4087\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2412 - val_loss: 0.1953\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2595 - val_loss: 0.2517\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2342 - val_loss: 0.2336\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2369 - val_loss: 0.2262\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2486 - val_loss: 0.2719\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2387 - val_loss: 0.3568\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2368 - val_loss: 0.3111\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2320 - val_loss: 0.3018\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2384 - val_loss: 0.2921\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2322 - val_loss: 0.2209\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2360 - val_loss: 0.2307\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2389 - val_loss: 0.2190\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2314 - val_loss: 0.2584\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2339 - val_loss: 0.2491\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2288 - val_loss: 0.2391\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2838 - val_loss: 0.3668\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2497 - val_loss: 0.4105\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2410 - val_loss: 0.2414\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2392 - val_loss: 0.2610\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2353 - val_loss: 0.2987\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2301 - val_loss: 0.2099\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2379 - val_loss: 0.2243\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2509 - val_loss: 0.3100\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2316 - val_loss: 0.2378\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2395 - val_loss: 0.1964\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2393 - val_loss: 0.1925\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2353 - val_loss: 0.1957\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2482 - val_loss: 0.2265\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2394 - val_loss: 0.2927\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2531 - val_loss: 0.2764\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2425 - val_loss: 0.2400\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2343 - val_loss: 0.2964\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2300 - val_loss: 0.2248\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2287 - val_loss: 0.2228\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2324 - val_loss: 0.2603\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2590 - val_loss: 0.2093\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2361 - val_loss: 0.2246\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2325 - val_loss: 0.1963\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2262 - val_loss: 0.2307\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2227 - val_loss: 0.2146\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2348 - val_loss: 0.2335\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2377 - val_loss: 0.2221\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2337 - val_loss: 0.3209\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2363 - val_loss: 0.2046\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2322 - val_loss: 0.2385\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2275 - val_loss: 0.2519\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2406 - val_loss: 0.2859\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3114 - val_loss: 0.2346\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.3113 - val_loss: 0.2852\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2398 - val_loss: 0.2575\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2450 - val_loss: 0.1880\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2318 - val_loss: 0.1943\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2294 - val_loss: 0.1995\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2328 - val_loss: 0.2148\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2296 - val_loss: 0.2046\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2327 - val_loss: 0.2721\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2776 - val_loss: 0.2231\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2315 - val_loss: 0.2535\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2333 - val_loss: 0.2339\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2265 - val_loss: 0.2453\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2396 - val_loss: 0.2941\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2353 - val_loss: 0.1950\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2471 - val_loss: 0.3568\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2349 - val_loss: 0.2267\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2286 - val_loss: 0.2153\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2247 - val_loss: 0.2434\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2322 - val_loss: 0.3583\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2296 - val_loss: 0.2139\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2294 - val_loss: 0.2584\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2359 - val_loss: 0.3273\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2406 - val_loss: 0.1688\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2338 - val_loss: 0.2048\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2321 - val_loss: 0.2025\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2313 - val_loss: 0.2569\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2329 - val_loss: 0.2899\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2416 - val_loss: 0.2999\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2313 - val_loss: 0.2752\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2245 - val_loss: 0.2053\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2253 - val_loss: 0.2671\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2271 - val_loss: 0.2618\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2293 - val_loss: 0.2036\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2259 - val_loss: 0.1935\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2401 - val_loss: 0.2213\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2269 - val_loss: 0.3077\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2283 - val_loss: 0.2314\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2325 - val_loss: 0.2133\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2357 - val_loss: 0.2070\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2482 - val_loss: 0.2233\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2227 - val_loss: 0.2956\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2281 - val_loss: 0.3153\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2328 - val_loss: 0.1909\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2261 - val_loss: 0.1972\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2363 - val_loss: 0.1974\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2268 - val_loss: 0.2422\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2252 - val_loss: 0.2700\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2696 - val_loss: 0.2536\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2665 - val_loss: 0.2318\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2528 - val_loss: 0.2343\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2278 - val_loss: 0.1908\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2268 - val_loss: 0.2833\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2348 - val_loss: 0.2430\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2243 - val_loss: 0.2255\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2287 - val_loss: 0.3173\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2279 - val_loss: 0.1941\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2231 - val_loss: 0.2040\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2277 - val_loss: 0.2211\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2279 - val_loss: 0.1967\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2565 - val_loss: 0.4100\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2409 - val_loss: 0.2104\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2242 - val_loss: 0.2856\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2213 - val_loss: 0.2451\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2249 - val_loss: 0.1970\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2266 - val_loss: 0.2087\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2284 - val_loss: 0.3415\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2231 - val_loss: 0.1976\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2335 - val_loss: 0.1986\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2225 - val_loss: 0.1896\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2351 - val_loss: 0.1913\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2240 - val_loss: 0.2528\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2208 - val_loss: 0.2419\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2241 - val_loss: 0.2209\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2429 - val_loss: 0.2566\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2244 - val_loss: 0.3042\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2455 - val_loss: 0.3810\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2287 - val_loss: 0.2356\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2220 - val_loss: 0.1861\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2259 - val_loss: 0.2064\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2167 - val_loss: 0.2272\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2198 - val_loss: 0.2307\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2179 - val_loss: 0.1948\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2152 - val_loss: 0.2170\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2181 - val_loss: 0.2225\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2200 - val_loss: 0.2818\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2262 - val_loss: 0.2324\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2201 - val_loss: 0.2693\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2320 - val_loss: 0.2127\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2338 - val_loss: 0.3042\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2348 - val_loss: 0.3064\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2327 - val_loss: 0.1926\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2296 - val_loss: 0.2834\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2141 - val_loss: 0.2351\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2224 - val_loss: 0.2312\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2146 - val_loss: 0.2265\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2186 - val_loss: 0.2519\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2280 - val_loss: 0.2261\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2296 - val_loss: 0.2060\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2155 - val_loss: 0.2411\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2242 - val_loss: 0.2483\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2206 - val_loss: 0.3053\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2186 - val_loss: 0.2367\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2186 - val_loss: 0.2594\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2402 - val_loss: 0.3038\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2207 - val_loss: 0.2481\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2223 - val_loss: 0.1966\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2198 - val_loss: 0.2755\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2206 - val_loss: 0.2084\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2177 - val_loss: 0.3155\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2274 - val_loss: 0.2869\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2155 - val_loss: 0.2626\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2283 - val_loss: 0.2858\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2224 - val_loss: 0.2966\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2197 - val_loss: 0.2173\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2180 - val_loss: 0.1856\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2218 - val_loss: 0.1957\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2415 - val_loss: 0.2767\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2246 - val_loss: 0.2768\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2171 - val_loss: 0.2260\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2255 - val_loss: 0.2948\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2271 - val_loss: 0.1957\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2198 - val_loss: 0.4575\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2201 - val_loss: 0.2420\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2141 - val_loss: 0.2670\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2162 - val_loss: 0.2448\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2165 - val_loss: 0.1816\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2181 - val_loss: 0.2041\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2143 - val_loss: 0.2666\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.2124 - val_loss: 0.2353\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2221 - val_loss: 0.2378\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2197 - val_loss: 0.1792\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2186 - val_loss: 0.2388\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2182 - val_loss: 0.2553\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2538 - val_loss: 0.2285\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2213 - val_loss: 0.2532\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2216 - val_loss: 0.3015\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2397 - val_loss: 0.2755\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2227 - val_loss: 0.2630\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2198 - val_loss: 0.2225\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2178 - val_loss: 0.1950\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2252 - val_loss: 0.2089\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2273 - val_loss: 0.2994\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2178 - val_loss: 0.2119\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2134 - val_loss: 0.2387\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2264 - val_loss: 0.1824\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2214 - val_loss: 0.3009\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2130 - val_loss: 0.2560\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2093 - val_loss: 0.2881\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2137 - val_loss: 0.2888\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2134 - val_loss: 0.3058\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2189 - val_loss: 0.1929\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.2256 - val_loss: 0.2167\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2181 - val_loss: 0.2144\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2144 - val_loss: 0.3003\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.2235 - val_loss: 0.2350\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2162 - val_loss: 0.3041\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.2230 - val_loss: 0.2562\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.2183 - val_loss: 0.2476\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 68ms/step - loss: 0.1641 - val_loss: 0.1388\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1509 - val_loss: 0.1190\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1273 - val_loss: 0.1234\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1183 - val_loss: 0.1224\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1078 - val_loss: 0.1121\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0960 - val_loss: 0.1114\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0919 - val_loss: 0.0989\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0860 - val_loss: 0.0962\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0829 - val_loss: 0.0861\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0845 - val_loss: 0.1031\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0820 - val_loss: 0.1005\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0802 - val_loss: 0.0695\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0789 - val_loss: 0.0775\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0779 - val_loss: 0.0693\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0790 - val_loss: 0.0817\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0738 - val_loss: 0.0693\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0739 - val_loss: 0.0845\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0723 - val_loss: 0.0698\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0736 - val_loss: 0.0731\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0713 - val_loss: 0.0848\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0713 - val_loss: 0.0801\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0689 - val_loss: 0.0704\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0662 - val_loss: 0.0632\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0688 - val_loss: 0.0605\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0600 - val_loss: 0.0534\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0614 - val_loss: 0.0547\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0568 - val_loss: 0.0503\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0569 - val_loss: 0.0751\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0564 - val_loss: 0.0551\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0536 - val_loss: 0.0494\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0509 - val_loss: 0.0489\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0508 - val_loss: 0.0542\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0494 - val_loss: 0.0650\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.0631 - val_loss: 0.0558\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0534 - val_loss: 0.0735\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0477 - val_loss: 0.0579\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0480 - val_loss: 0.0506\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0460 - val_loss: 0.0447\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0436 - val_loss: 0.0493\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0479 - val_loss: 0.0517\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0470 - val_loss: 0.0494\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0435 - val_loss: 0.0469\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0403 - val_loss: 0.0535\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0411 - val_loss: 0.0480\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0415 - val_loss: 0.0576\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0407 - val_loss: 0.0545\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0396 - val_loss: 0.0655\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0382 - val_loss: 0.0579\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0398 - val_loss: 0.0537\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0387 - val_loss: 0.0587\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0370 - val_loss: 0.0556\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0379 - val_loss: 0.0567\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0358 - val_loss: 0.0763\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0354 - val_loss: 0.0592\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0363 - val_loss: 0.0636\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0354 - val_loss: 0.0591\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0364 - val_loss: 0.0621\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0384 - val_loss: 0.0567\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0343 - val_loss: 0.0582\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0336 - val_loss: 0.0755\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0329 - val_loss: 0.0605\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0318 - val_loss: 0.0632\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0318 - val_loss: 0.0677\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0331 - val_loss: 0.0557\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0325 - val_loss: 0.0639\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0323 - val_loss: 0.0599\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0314 - val_loss: 0.0605\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0322 - val_loss: 0.0465\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0304 - val_loss: 0.0664\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0301 - val_loss: 0.0534\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0295 - val_loss: 0.0657\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0291 - val_loss: 0.0662\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0286 - val_loss: 0.0656\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0284 - val_loss: 0.0724\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0278 - val_loss: 0.0635\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0288 - val_loss: 0.0594\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0284 - val_loss: 0.0600\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0277 - val_loss: 0.0611\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0278 - val_loss: 0.0649\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0313 - val_loss: 0.0619\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0274 - val_loss: 0.0697\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0279 - val_loss: 0.0693\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0267 - val_loss: 0.0595\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0277 - val_loss: 0.0505\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0266 - val_loss: 0.0632\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0273 - val_loss: 0.0567\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0255 - val_loss: 0.0675\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0247 - val_loss: 0.0593\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0248 - val_loss: 0.0579\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0263 - val_loss: 0.0570\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0246 - val_loss: 0.0565\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0248 - val_loss: 0.0655\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0241 - val_loss: 0.0680\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0236 - val_loss: 0.0565\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0246 - val_loss: 0.0717\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0253 - val_loss: 0.0609\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0247 - val_loss: 0.0606\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0246 - val_loss: 0.0591\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0237 - val_loss: 0.0558\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0231 - val_loss: 0.0583\n",
      "8/8 [==============================] - 0s 27ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 5s 67ms/step - loss: 0.1603 - val_loss: 0.1307\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1506 - val_loss: 0.1117\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1263 - val_loss: 0.1196\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1230 - val_loss: 0.1050\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1066 - val_loss: 0.1085\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1011 - val_loss: 0.1165\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0951 - val_loss: 0.1018\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0890 - val_loss: 0.0930\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0839 - val_loss: 0.0895\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0829 - val_loss: 0.0786\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0795 - val_loss: 0.0845\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0793 - val_loss: 0.0889\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0802 - val_loss: 0.0806\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0762 - val_loss: 0.0646\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0786 - val_loss: 0.0789\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0750 - val_loss: 0.0660\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0730 - val_loss: 0.0692\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0712 - val_loss: 0.0680\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0699 - val_loss: 0.0670\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0666 - val_loss: 0.0671\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0673 - val_loss: 0.0625\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0687 - val_loss: 0.0615\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0665 - val_loss: 0.0662\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0623 - val_loss: 0.0576\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0613 - val_loss: 0.0600\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0634 - val_loss: 0.0594\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0608 - val_loss: 0.0528\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0581 - val_loss: 0.0560\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0555 - val_loss: 0.0614\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0558 - val_loss: 0.0630\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0535 - val_loss: 0.0637\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0515 - val_loss: 0.0641\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0506 - val_loss: 0.0642\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0481 - val_loss: 0.0654\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0489 - val_loss: 0.0826\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0815 - val_loss: 0.0805\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0655 - val_loss: 0.0684\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0576 - val_loss: 0.0756\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0528 - val_loss: 0.0634\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0485 - val_loss: 0.0679\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0477 - val_loss: 0.0698\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0453 - val_loss: 0.0920\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0445 - val_loss: 0.0654\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0475 - val_loss: 0.0840\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0447 - val_loss: 0.0705\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0453 - val_loss: 0.0868\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0418 - val_loss: 0.0786\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0405 - val_loss: 0.0910\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0419 - val_loss: 0.0956\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0424 - val_loss: 0.0825\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0405 - val_loss: 0.0699\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0405 - val_loss: 0.0777\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0381 - val_loss: 0.0872\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0391 - val_loss: 0.0941\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0385 - val_loss: 0.0777\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0368 - val_loss: 0.0835\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0368 - val_loss: 0.0718\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0381 - val_loss: 0.0743\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0362 - val_loss: 0.0934\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0373 - val_loss: 0.0700\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0361 - val_loss: 0.0730\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0346 - val_loss: 0.0847\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0341 - val_loss: 0.0749\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0342 - val_loss: 0.0774\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0333 - val_loss: 0.0798\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0332 - val_loss: 0.0782\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0337 - val_loss: 0.0834\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0330 - val_loss: 0.0825\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0320 - val_loss: 0.0878\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0321 - val_loss: 0.0796\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0311 - val_loss: 0.0805\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0326 - val_loss: 0.0764\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0322 - val_loss: 0.0831\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0309 - val_loss: 0.0802\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0409 - val_loss: 0.0759\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0371 - val_loss: 0.0681\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0326 - val_loss: 0.0675\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0321 - val_loss: 0.0691\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0317 - val_loss: 0.0670\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0304 - val_loss: 0.0875\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0295 - val_loss: 0.0840\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0291 - val_loss: 0.0768\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0293 - val_loss: 0.0899\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0323 - val_loss: 0.0588\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0317 - val_loss: 0.0884\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0294 - val_loss: 0.0744\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0287 - val_loss: 0.0769\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0298 - val_loss: 0.0735\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0282 - val_loss: 0.0778\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0282 - val_loss: 0.0866\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0332 - val_loss: 0.0658\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0305 - val_loss: 0.0699\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0274 - val_loss: 0.0810\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0279 - val_loss: 0.0722\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0273 - val_loss: 0.0809\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0268 - val_loss: 0.0811\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0258 - val_loss: 0.0879\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0265 - val_loss: 0.0827\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0260 - val_loss: 0.0766\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0260 - val_loss: 0.0774\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0255 - val_loss: 0.0797\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0258 - val_loss: 0.0753\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0269 - val_loss: 0.0818\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0254 - val_loss: 0.0798\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0251 - val_loss: 0.0859\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0247 - val_loss: 0.0747\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0263 - val_loss: 0.0777\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0240 - val_loss: 0.0938\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0238 - val_loss: 0.0786\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0612 - val_loss: 0.0836\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0472 - val_loss: 0.0623\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0346 - val_loss: 0.0724\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0287 - val_loss: 0.0794\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0272 - val_loss: 0.0601\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0257 - val_loss: 0.0682\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0258 - val_loss: 0.0688\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0250 - val_loss: 0.0660\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0241 - val_loss: 0.0740\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0239 - val_loss: 0.0630\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0282 - val_loss: 0.0755\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0321 - val_loss: 0.0610\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0259 - val_loss: 0.0652\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0238 - val_loss: 0.0710\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0238 - val_loss: 0.0733\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0236 - val_loss: 0.0670\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0234 - val_loss: 0.0682\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0225 - val_loss: 0.0724\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0225 - val_loss: 0.0824\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0226 - val_loss: 0.0773\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0220 - val_loss: 0.0743\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0219 - val_loss: 0.0761\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0219 - val_loss: 0.0713\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0214 - val_loss: 0.0800\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0216 - val_loss: 0.0762\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0212 - val_loss: 0.0857\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0210 - val_loss: 0.0704\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0212 - val_loss: 0.0744\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0208 - val_loss: 0.0734\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0210 - val_loss: 0.0769\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0209 - val_loss: 0.1075\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0210 - val_loss: 0.0856\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0201 - val_loss: 0.0893\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0201 - val_loss: 0.0689\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0205 - val_loss: 0.0803\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0200 - val_loss: 0.0834\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0222 - val_loss: 0.0651\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0211 - val_loss: 0.0750\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0205 - val_loss: 0.0738\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0200 - val_loss: 0.0756\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0198 - val_loss: 0.0689\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0194 - val_loss: 0.0777\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0200 - val_loss: 0.0660\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0199 - val_loss: 0.0650\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0193 - val_loss: 0.0790\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0190 - val_loss: 0.0799\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0187 - val_loss: 0.0958\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0189 - val_loss: 0.0840\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0185 - val_loss: 0.0800\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0188 - val_loss: 0.0840\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0189 - val_loss: 0.0787\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0196 - val_loss: 0.0812\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0197 - val_loss: 0.0842\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0189 - val_loss: 0.0756\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0181 - val_loss: 0.0775\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0179 - val_loss: 0.0737\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0179 - val_loss: 0.0757\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0179 - val_loss: 0.0802\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0175 - val_loss: 0.0815\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0176 - val_loss: 0.0831\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0187 - val_loss: 0.0720\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0182 - val_loss: 0.0757\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0174 - val_loss: 0.0711\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0175 - val_loss: 0.0757\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0174 - val_loss: 0.0847\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0175 - val_loss: 0.0774\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0178 - val_loss: 0.0773\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0192 - val_loss: 0.0640\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0184 - val_loss: 0.0847\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0175 - val_loss: 0.0762\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0172 - val_loss: 0.0788\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0167 - val_loss: 0.0761\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0171 - val_loss: 0.0772\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0167 - val_loss: 0.0757\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0177 - val_loss: 0.0761\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0168 - val_loss: 0.0801\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0168 - val_loss: 0.0776\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0166 - val_loss: 0.0853\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0165 - val_loss: 0.0834\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0162 - val_loss: 0.0874\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0165 - val_loss: 0.0834\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0171 - val_loss: 0.0642\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0168 - val_loss: 0.0708\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0169 - val_loss: 0.0700\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0163 - val_loss: 0.0690\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0163 - val_loss: 0.0775\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0159 - val_loss: 0.0716\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0161 - val_loss: 0.0739\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0157 - val_loss: 0.0843\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0157 - val_loss: 0.0820\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0159 - val_loss: 0.0816\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 4s 67ms/step - loss: 0.1568 - val_loss: 0.1238\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1393 - val_loss: 0.1323\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1190 - val_loss: 0.1367\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1049 - val_loss: 0.1480\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0983 - val_loss: 0.1200\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0927 - val_loss: 0.1538\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0898 - val_loss: 0.1390\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0905 - val_loss: 0.1679\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0911 - val_loss: 0.1487\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0819 - val_loss: 0.1517\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0793 - val_loss: 0.1644\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0784 - val_loss: 0.1399\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0765 - val_loss: 0.1599\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0810 - val_loss: 0.1240\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0743 - val_loss: 0.1432\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0698 - val_loss: 0.1382\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0683 - val_loss: 0.1282\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0659 - val_loss: 0.1395\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0636 - val_loss: 0.1361\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0620 - val_loss: 0.1253\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0783 - val_loss: 0.1204\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0642 - val_loss: 0.1776\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0680 - val_loss: 0.1835\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0604 - val_loss: 0.1097\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0561 - val_loss: 0.1285\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.0584 - val_loss: 0.1437\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0573 - val_loss: 0.1238\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0541 - val_loss: 0.0743\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0605 - val_loss: 0.0711\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0527 - val_loss: 0.0810\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0483 - val_loss: 0.0765\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0524 - val_loss: 0.0875\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0487 - val_loss: 0.0728\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0485 - val_loss: 0.1160\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0453 - val_loss: 0.0691\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0443 - val_loss: 0.0850\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0453 - val_loss: 0.0715\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0458 - val_loss: 0.0587\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0408 - val_loss: 0.0819\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0424 - val_loss: 0.0605\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0432 - val_loss: 0.0639\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0411 - val_loss: 0.0596\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0402 - val_loss: 0.0664\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0383 - val_loss: 0.0577\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0370 - val_loss: 0.0613\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0371 - val_loss: 0.0596\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0374 - val_loss: 0.0634\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0370 - val_loss: 0.0660\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0369 - val_loss: 0.0500\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0349 - val_loss: 0.0531\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0342 - val_loss: 0.0529\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0335 - val_loss: 0.0596\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0350 - val_loss: 0.0590\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0338 - val_loss: 0.0651\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0352 - val_loss: 0.0601\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0329 - val_loss: 0.0571\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0314 - val_loss: 0.0615\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0321 - val_loss: 0.0571\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0320 - val_loss: 0.0573\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0338 - val_loss: 0.0616\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0344 - val_loss: 0.0559\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0304 - val_loss: 0.0556\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0314 - val_loss: 0.0439\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0309 - val_loss: 0.0531\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0298 - val_loss: 0.0510\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0289 - val_loss: 0.0522\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0290 - val_loss: 0.0570\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0294 - val_loss: 0.0523\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0281 - val_loss: 0.0602\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0297 - val_loss: 0.0512\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0289 - val_loss: 0.0548\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0279 - val_loss: 0.0522\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0269 - val_loss: 0.0538\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0274 - val_loss: 0.0533\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0269 - val_loss: 0.0540\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0268 - val_loss: 0.0594\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0277 - val_loss: 0.0519\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0275 - val_loss: 0.0548\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0255 - val_loss: 0.0492\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0266 - val_loss: 0.0514\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0258 - val_loss: 0.0470\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0254 - val_loss: 0.0467\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0248 - val_loss: 0.0500\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0255 - val_loss: 0.0461\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0250 - val_loss: 0.0491\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0243 - val_loss: 0.0511\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0242 - val_loss: 0.0524\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0245 - val_loss: 0.0506\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0246 - val_loss: 0.0536\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0238 - val_loss: 0.0476\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0256 - val_loss: 0.0367\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0258 - val_loss: 0.0442\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0243 - val_loss: 0.0455\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0243 - val_loss: 0.0460\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0233 - val_loss: 0.0494\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0225 - val_loss: 0.0501\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0234 - val_loss: 0.0485\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0236 - val_loss: 0.0550\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0230 - val_loss: 0.0495\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0236 - val_loss: 0.0394\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0225 - val_loss: 0.0510\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0222 - val_loss: 0.0500\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0214 - val_loss: 0.0484\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0216 - val_loss: 0.0477\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0224 - val_loss: 0.0545\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0219 - val_loss: 0.0486\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0234 - val_loss: 0.0478\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0331 - val_loss: 0.0594\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0352 - val_loss: 0.0506\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0271 - val_loss: 0.0448\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0228 - val_loss: 0.0498\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0216 - val_loss: 0.0482\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0209 - val_loss: 0.0536\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0206 - val_loss: 0.0508\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0201 - val_loss: 0.0501\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0225 - val_loss: 0.0459\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0226 - val_loss: 0.0459\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0208 - val_loss: 0.0472\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0197 - val_loss: 0.0468\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0197 - val_loss: 0.0488\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0192 - val_loss: 0.0473\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0198 - val_loss: 0.0455\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0201 - val_loss: 0.0481\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0191 - val_loss: 0.0510\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0190 - val_loss: 0.0522\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0194 - val_loss: 0.0475\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0188 - val_loss: 0.0503\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0184 - val_loss: 0.0484\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0190 - val_loss: 0.0498\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0182 - val_loss: 0.0469\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0182 - val_loss: 0.0496\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0192 - val_loss: 0.0506\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0191 - val_loss: 0.0533\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0182 - val_loss: 0.0511\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0182 - val_loss: 0.0489\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0182 - val_loss: 0.0489\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0179 - val_loss: 0.0501\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0176 - val_loss: 0.0503\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0194 - val_loss: 0.0483\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0181 - val_loss: 0.0476\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0173 - val_loss: 0.0454\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0172 - val_loss: 0.0479\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0172 - val_loss: 0.0474\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0178 - val_loss: 0.0555\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0171 - val_loss: 0.0461\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0176 - val_loss: 0.0486\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0174 - val_loss: 0.0490\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0175 - val_loss: 0.0477\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0174 - val_loss: 0.0471\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0171 - val_loss: 0.0450\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0168 - val_loss: 0.0489\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0166 - val_loss: 0.0481\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0166 - val_loss: 0.0461\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0162 - val_loss: 0.0492\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0171 - val_loss: 0.0498\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0169 - val_loss: 0.0482\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0163 - val_loss: 0.0524\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0164 - val_loss: 0.0466\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0159 - val_loss: 0.0508\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0155 - val_loss: 0.0478\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0158 - val_loss: 0.0475\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0163 - val_loss: 0.0477\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0157 - val_loss: 0.0449\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0180 - val_loss: 0.0508\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0181 - val_loss: 0.0430\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0174 - val_loss: 0.0459\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0170 - val_loss: 0.0434\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0164 - val_loss: 0.0509\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0156 - val_loss: 0.0472\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0161 - val_loss: 0.0529\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0156 - val_loss: 0.0466\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0158 - val_loss: 0.0461\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 60ms/step - loss: 0.0154 - val_loss: 0.0494\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0152 - val_loss: 0.0483\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0148 - val_loss: 0.0460\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0149 - val_loss: 0.0515\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0148 - val_loss: 0.0488\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0147 - val_loss: 0.0496\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0151 - val_loss: 0.0492\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0149 - val_loss: 0.0500\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0147 - val_loss: 0.0509\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0164 - val_loss: 0.0608\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0286 - val_loss: 0.0580\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0293 - val_loss: 0.0484\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0215 - val_loss: 0.0444\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0168 - val_loss: 0.0436\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0152 - val_loss: 0.0451\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0149 - val_loss: 0.0471\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0146 - val_loss: 0.0463\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0144 - val_loss: 0.0504\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0144 - val_loss: 0.0488\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0143 - val_loss: 0.0507\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0143 - val_loss: 0.0472\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0142 - val_loss: 0.0494\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0141 - val_loss: 0.0493\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0140 - val_loss: 0.0497\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0142 - val_loss: 0.0524\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0139 - val_loss: 0.0488\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0141 - val_loss: 0.0489\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0139 - val_loss: 0.0475\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0138 - val_loss: 0.0478\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0136 - val_loss: 0.0501\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0137 - val_loss: 0.0487\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0141 - val_loss: 0.0492\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0138 - val_loss: 0.0505\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0135 - val_loss: 0.0532\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0136 - val_loss: 0.0503\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0135 - val_loss: 0.0490\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0136 - val_loss: 0.0519\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0135 - val_loss: 0.0479\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0135 - val_loss: 0.0508\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0133 - val_loss: 0.0523\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0133 - val_loss: 0.0511\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0133 - val_loss: 0.0511\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0132 - val_loss: 0.0490\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0133 - val_loss: 0.0542\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0134 - val_loss: 0.0539\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0137 - val_loss: 0.0526\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0135 - val_loss: 0.0525\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0134 - val_loss: 0.0537\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0131 - val_loss: 0.0502\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0135 - val_loss: 0.0503\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0371 - val_loss: 0.0599\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0649 - val_loss: 0.0566\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0507 - val_loss: 0.0576\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0400 - val_loss: 0.0566\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0330 - val_loss: 0.0537\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0280 - val_loss: 0.0513\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0246 - val_loss: 0.0584\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0265 - val_loss: 0.0526\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0201 - val_loss: 0.0490\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0182 - val_loss: 0.0502\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0169 - val_loss: 0.0524\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0158 - val_loss: 0.0518\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0156 - val_loss: 0.0574\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0148 - val_loss: 0.0500\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0144 - val_loss: 0.0537\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0141 - val_loss: 0.0538\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0143 - val_loss: 0.0518\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0142 - val_loss: 0.0504\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0141 - val_loss: 0.0523\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0137 - val_loss: 0.0522\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0134 - val_loss: 0.0528\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0134 - val_loss: 0.0525\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0133 - val_loss: 0.0527\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0131 - val_loss: 0.0548\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0131 - val_loss: 0.0511\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0131 - val_loss: 0.0533\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0130 - val_loss: 0.0531\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0129 - val_loss: 0.0537\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0129 - val_loss: 0.0535\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0129 - val_loss: 0.0534\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0128 - val_loss: 0.0540\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0129 - val_loss: 0.0506\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0128 - val_loss: 0.0536\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0129 - val_loss: 0.0512\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0127 - val_loss: 0.0501\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0127 - val_loss: 0.0527\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0126 - val_loss: 0.0542\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0125 - val_loss: 0.0553\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0125 - val_loss: 0.0521\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0125 - val_loss: 0.0537\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0126 - val_loss: 0.0565\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0126 - val_loss: 0.0523\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0125 - val_loss: 0.0524\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0124 - val_loss: 0.0554\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0127 - val_loss: 0.0523\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0126 - val_loss: 0.0531\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0124 - val_loss: 0.0510\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0123 - val_loss: 0.0527\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0128 - val_loss: 0.0528\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0123 - val_loss: 0.0574\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0122 - val_loss: 0.0532\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0123 - val_loss: 0.0554\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0122 - val_loss: 0.0599\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0123 - val_loss: 0.0548\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0122 - val_loss: 0.0622\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0122 - val_loss: 0.0528\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0121 - val_loss: 0.0527\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0121 - val_loss: 0.0591\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0120 - val_loss: 0.0551\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0121 - val_loss: 0.0553\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0122 - val_loss: 0.0544\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0120 - val_loss: 0.0544\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0120 - val_loss: 0.0557\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0120 - val_loss: 0.0581\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0121 - val_loss: 0.0509\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0120 - val_loss: 0.0603\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0121 - val_loss: 0.0556\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0119 - val_loss: 0.0555\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0118 - val_loss: 0.0590\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0118 - val_loss: 0.0635\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0118 - val_loss: 0.0493\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0118 - val_loss: 0.0552\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0119 - val_loss: 0.0529\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0118 - val_loss: 0.0564\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0120 - val_loss: 0.0501\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0120 - val_loss: 0.0571\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0120 - val_loss: 0.0568\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0121 - val_loss: 0.0524\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0124 - val_loss: 0.0516\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0119 - val_loss: 0.0534\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0118 - val_loss: 0.0528\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0120 - val_loss: 0.0560\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0118 - val_loss: 0.0498\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0115 - val_loss: 0.0596\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0116 - val_loss: 0.0524\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0118 - val_loss: 0.0584\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0118 - val_loss: 0.0609\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0115 - val_loss: 0.0579\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0116 - val_loss: 0.0542\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0120 - val_loss: 0.0581\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0417 - val_loss: 0.0408\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0210 - val_loss: 0.0484\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0142 - val_loss: 0.0512\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0127 - val_loss: 0.0477\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0120 - val_loss: 0.0508\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0118 - val_loss: 0.0486\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0116 - val_loss: 0.0529\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0115 - val_loss: 0.0558\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0114 - val_loss: 0.0553\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0115 - val_loss: 0.0590\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0114 - val_loss: 0.0530\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0114 - val_loss: 0.0566\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0113 - val_loss: 0.0591\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0112 - val_loss: 0.0614\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0114 - val_loss: 0.0557\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0112 - val_loss: 0.0569\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0111 - val_loss: 0.0593\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0111 - val_loss: 0.0532\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0112 - val_loss: 0.0556\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0112 - val_loss: 0.0587\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0111 - val_loss: 0.0556\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0110 - val_loss: 0.0549\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0110 - val_loss: 0.0585\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0110 - val_loss: 0.0547\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0575\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0596\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0571\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0111 - val_loss: 0.0567\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0576\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0615\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0110 - val_loss: 0.0602\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0109 - val_loss: 0.0635\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0110 - val_loss: 0.0613\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0109 - val_loss: 0.0569\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0108 - val_loss: 0.0586\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0108 - val_loss: 0.0613\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0109 - val_loss: 0.0579\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0110 - val_loss: 0.0630\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0112 - val_loss: 0.0575\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0108 - val_loss: 0.0573\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0108 - val_loss: 0.0600\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0107 - val_loss: 0.0560\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0107 - val_loss: 0.0663\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0109 - val_loss: 0.0581\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0109 - val_loss: 0.0566\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0107 - val_loss: 0.0556\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0106 - val_loss: 0.0627\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0108 - val_loss: 0.0612\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0107 - val_loss: 0.0614\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0108 - val_loss: 0.0622\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0109 - val_loss: 0.0572\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0117 - val_loss: 0.0707\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0116 - val_loss: 0.0531\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0112 - val_loss: 0.0607\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0334 - val_loss: 0.0714\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0326 - val_loss: 0.0441\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0220 - val_loss: 0.0478\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0140 - val_loss: 0.0545\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0121 - val_loss: 0.0536\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0114 - val_loss: 0.0556\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0112 - val_loss: 0.0549\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0561\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0109 - val_loss: 0.0581\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0108 - val_loss: 0.0562\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0107 - val_loss: 0.0582\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0106 - val_loss: 0.0563\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0106 - val_loss: 0.0590\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0105 - val_loss: 0.0588\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0105 - val_loss: 0.0586\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0104 - val_loss: 0.0617\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0105 - val_loss: 0.0597\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0104 - val_loss: 0.0580\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0104 - val_loss: 0.0604\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0104 - val_loss: 0.0617\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0104 - val_loss: 0.0609\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0104 - val_loss: 0.0612\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0103 - val_loss: 0.0613\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0104 - val_loss: 0.0636\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0103 - val_loss: 0.0653\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0103 - val_loss: 0.0620\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0102 - val_loss: 0.0614\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0103 - val_loss: 0.0622\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0102 - val_loss: 0.0679\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0103 - val_loss: 0.0612\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0102 - val_loss: 0.0612\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0102 - val_loss: 0.0643\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0102 - val_loss: 0.0632\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0102 - val_loss: 0.0617\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0102 - val_loss: 0.0634\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0101 - val_loss: 0.0617\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0102 - val_loss: 0.0621\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0104 - val_loss: 0.0661\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0103 - val_loss: 0.0625\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0105 - val_loss: 0.0624\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0102 - val_loss: 0.0592\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0101 - val_loss: 0.0641\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0100 - val_loss: 0.0612\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0101 - val_loss: 0.0594\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0103 - val_loss: 0.0652\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0101 - val_loss: 0.0620\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0632\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0100 - val_loss: 0.0624\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0629\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0101 - val_loss: 0.0627\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0100 - val_loss: 0.0650\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0649\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0676\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0103 - val_loss: 0.0690\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0100 - val_loss: 0.0622\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0597\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0099 - val_loss: 0.0617\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0646\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0694\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0100 - val_loss: 0.0656\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0101 - val_loss: 0.0653\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0623\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0099 - val_loss: 0.0677\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0659\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0099 - val_loss: 0.0623\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0098 - val_loss: 0.0675\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0624\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0636\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0629\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0098 - val_loss: 0.0592\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0099 - val_loss: 0.0625\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0098 - val_loss: 0.0635\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0098 - val_loss: 0.0574\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0098 - val_loss: 0.0631\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0099 - val_loss: 0.0576\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0098 - val_loss: 0.0618\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0100 - val_loss: 0.0627\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0099 - val_loss: 0.0621\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0634\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0097 - val_loss: 0.0655\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0098 - val_loss: 0.0658\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0098 - val_loss: 0.0658\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0097 - val_loss: 0.0713\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0098 - val_loss: 0.0676\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0098 - val_loss: 0.0697\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0096 - val_loss: 0.0634\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0096 - val_loss: 0.0691\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0096 - val_loss: 0.0760\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0415 - val_loss: 0.0623\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0421 - val_loss: 0.0506\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0279 - val_loss: 0.0431\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0180 - val_loss: 0.0458\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0139 - val_loss: 0.0450\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0131 - val_loss: 0.0522\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0123 - val_loss: 0.0474\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0114 - val_loss: 0.0527\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0108 - val_loss: 0.0513\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0106 - val_loss: 0.0533\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0104 - val_loss: 0.0539\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0103 - val_loss: 0.0556\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0102 - val_loss: 0.0585\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0101 - val_loss: 0.0549\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0099 - val_loss: 0.0576\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0588\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0098 - val_loss: 0.0603\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0098 - val_loss: 0.0579\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0098 - val_loss: 0.0594\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0097 - val_loss: 0.0613\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0097 - val_loss: 0.0626\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0097 - val_loss: 0.0613\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0097 - val_loss: 0.0614\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0096 - val_loss: 0.0634\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0096 - val_loss: 0.0610\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0096 - val_loss: 0.0634\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0095 - val_loss: 0.0637\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0095 - val_loss: 0.0643\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0095 - val_loss: 0.0633\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0095 - val_loss: 0.0651\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0095 - val_loss: 0.0609\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0095 - val_loss: 0.0658\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.0660\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0094 - val_loss: 0.0650\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0094 - val_loss: 0.0648\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0094 - val_loss: 0.0631\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.0656\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0094 - val_loss: 0.0638\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.0624\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0094 - val_loss: 0.0646\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0094 - val_loss: 0.0670\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0093 - val_loss: 0.0651\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0093 - val_loss: 0.0638\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0094 - val_loss: 0.0649\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0093 - val_loss: 0.0664\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0093 - val_loss: 0.0658\n",
      "8/8 [==============================] - 0s 29ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 66ms/step - loss: 0.1608 - val_loss: 0.1302\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1372 - val_loss: 0.1085\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1279 - val_loss: 0.0965\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1196 - val_loss: 0.0911\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1105 - val_loss: 0.0879\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1052 - val_loss: 0.0802\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1026 - val_loss: 0.0797\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0969 - val_loss: 0.0749\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0903 - val_loss: 0.0736\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0908 - val_loss: 0.0737\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0874 - val_loss: 0.0849\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0833 - val_loss: 0.0722\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0800 - val_loss: 0.0694\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0758 - val_loss: 0.0756\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0798 - val_loss: 0.0752\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0751 - val_loss: 0.1006\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0863 - val_loss: 0.0710\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0740 - val_loss: 0.0690\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0722 - val_loss: 0.0690\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0700 - val_loss: 0.0784\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0681 - val_loss: 0.0655\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0699 - val_loss: 0.0831\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0640 - val_loss: 0.0722\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0642 - val_loss: 0.0669\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0617 - val_loss: 0.0809\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0636 - val_loss: 0.0753\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0576 - val_loss: 0.0780\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0608 - val_loss: 0.0704\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0548 - val_loss: 0.0903\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0530 - val_loss: 0.0837\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0520 - val_loss: 0.0913\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0518 - val_loss: 0.0796\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0518 - val_loss: 0.0756\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0515 - val_loss: 0.0934\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0470 - val_loss: 0.0727\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0465 - val_loss: 0.0868\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0474 - val_loss: 0.0639\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0444 - val_loss: 0.0801\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0424 - val_loss: 0.0807\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0437 - val_loss: 0.0734\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0429 - val_loss: 0.0856\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0399 - val_loss: 0.0766\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0387 - val_loss: 0.0779\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0395 - val_loss: 0.0838\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0415 - val_loss: 0.0838\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0373 - val_loss: 0.0917\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0373 - val_loss: 0.0930\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0365 - val_loss: 0.0796\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0355 - val_loss: 0.0782\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0369 - val_loss: 0.0846\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0350 - val_loss: 0.0815\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0350 - val_loss: 0.0841\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0408 - val_loss: 0.0632\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0363 - val_loss: 0.0800\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0336 - val_loss: 0.0819\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0343 - val_loss: 0.0758\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0366 - val_loss: 0.0701\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0350 - val_loss: 0.0859\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0332 - val_loss: 0.0667\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0317 - val_loss: 0.0708\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0311 - val_loss: 0.0739\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0316 - val_loss: 0.0756\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0312 - val_loss: 0.0767\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0307 - val_loss: 0.0695\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0296 - val_loss: 0.0697\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0295 - val_loss: 0.0850\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0298 - val_loss: 0.0653\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0304 - val_loss: 0.0706\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0290 - val_loss: 0.0741\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0288 - val_loss: 0.0814\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0290 - val_loss: 0.0759\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0292 - val_loss: 0.0704\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0288 - val_loss: 0.0733\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0280 - val_loss: 0.0727\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0276 - val_loss: 0.0671\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0275 - val_loss: 0.0707\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0285 - val_loss: 0.0800\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0267 - val_loss: 0.0846\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0292 - val_loss: 0.0725\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0304 - val_loss: 0.0831\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0288 - val_loss: 0.0760\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0263 - val_loss: 0.0713\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0255 - val_loss: 0.0695\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0257 - val_loss: 0.0815\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0254 - val_loss: 0.0807\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0255 - val_loss: 0.0694\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0250 - val_loss: 0.0727\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0244 - val_loss: 0.0632\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0241 - val_loss: 0.0655\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0244 - val_loss: 0.0761\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0249 - val_loss: 0.0672\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0244 - val_loss: 0.0687\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0246 - val_loss: 0.0676\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0246 - val_loss: 0.0667\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0239 - val_loss: 0.0801\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0230 - val_loss: 0.0790\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0243 - val_loss: 0.0675\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0226 - val_loss: 0.0673\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0232 - val_loss: 0.0615\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 4s 66ms/step - loss: 0.1640 - val_loss: 0.1423\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1544 - val_loss: 0.1185\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1400 - val_loss: 0.0991\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1201 - val_loss: 0.0904\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1130 - val_loss: 0.0930\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1069 - val_loss: 0.0826\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1072 - val_loss: 0.0902\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1035 - val_loss: 0.0858\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1005 - val_loss: 0.0803\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0975 - val_loss: 0.0833\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0951 - val_loss: 0.0765\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0949 - val_loss: 0.0786\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0922 - val_loss: 0.0777\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0887 - val_loss: 0.0705\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0845 - val_loss: 0.0664\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0794 - val_loss: 0.0585\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0780 - val_loss: 0.0549\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0744 - val_loss: 0.0559\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0694 - val_loss: 0.0578\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0741 - val_loss: 0.0531\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0715 - val_loss: 0.0553\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0656 - val_loss: 0.0588\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0635 - val_loss: 0.0552\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0617 - val_loss: 0.0574\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0583 - val_loss: 0.0523\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0565 - val_loss: 0.0574\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0555 - val_loss: 0.0615\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0604 - val_loss: 0.0600\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0550 - val_loss: 0.0566\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0529 - val_loss: 0.0585\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0532 - val_loss: 0.0584\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0500 - val_loss: 0.0587\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0565 - val_loss: 0.0623\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0572 - val_loss: 0.0609\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0490 - val_loss: 0.0600\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0505 - val_loss: 0.0630\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0488 - val_loss: 0.0643\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0447 - val_loss: 0.0737\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0445 - val_loss: 0.0650\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0436 - val_loss: 0.0652\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0437 - val_loss: 0.0645\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0420 - val_loss: 0.0593\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0419 - val_loss: 0.0657\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0434 - val_loss: 0.0635\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0433 - val_loss: 0.0541\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0418 - val_loss: 0.0673\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0435 - val_loss: 0.0606\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0444 - val_loss: 0.0560\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0405 - val_loss: 0.0627\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0386 - val_loss: 0.0630\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0384 - val_loss: 0.0602\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0372 - val_loss: 0.0639\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0365 - val_loss: 0.0657\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0369 - val_loss: 0.0701\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0361 - val_loss: 0.0657\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0354 - val_loss: 0.0555\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0510 - val_loss: 0.0583\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0410 - val_loss: 0.0605\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0381 - val_loss: 0.0565\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0369 - val_loss: 0.0607\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0348 - val_loss: 0.0628\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0342 - val_loss: 0.0579\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0364 - val_loss: 0.0611\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0433 - val_loss: 0.0632\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0357 - val_loss: 0.0596\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0351 - val_loss: 0.0731\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0348 - val_loss: 0.0615\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0335 - val_loss: 0.0602\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0360 - val_loss: 0.0640\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0401 - val_loss: 0.0622\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0330 - val_loss: 0.0566\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0316 - val_loss: 0.0603\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0305 - val_loss: 0.0576\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0306 - val_loss: 0.0553\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0305 - val_loss: 0.0569\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0298 - val_loss: 0.0563\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0293 - val_loss: 0.0613\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0318 - val_loss: 0.0629\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0308 - val_loss: 0.0551\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0288 - val_loss: 0.0509\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0289 - val_loss: 0.0468\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0277 - val_loss: 0.0520\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0288 - val_loss: 0.0526\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0292 - val_loss: 0.0544\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0287 - val_loss: 0.0548\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0285 - val_loss: 0.0603\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0282 - val_loss: 0.0484\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0288 - val_loss: 0.0525\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0287 - val_loss: 0.0460\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0324 - val_loss: 0.0398\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0284 - val_loss: 0.0509\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0274 - val_loss: 0.0495\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0262 - val_loss: 0.0526\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0256 - val_loss: 0.0551\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0262 - val_loss: 0.0429\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0258 - val_loss: 0.0525\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0253 - val_loss: 0.0515\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0263 - val_loss: 0.0466\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0257 - val_loss: 0.0465\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0244 - val_loss: 0.0486\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0271 - val_loss: 0.0403\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0310 - val_loss: 0.0601\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0259 - val_loss: 0.0527\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0241 - val_loss: 0.0495\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0241 - val_loss: 0.0471\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0245 - val_loss: 0.0462\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0238 - val_loss: 0.0510\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0234 - val_loss: 0.0480\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0233 - val_loss: 0.0459\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0250 - val_loss: 0.0490\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0248 - val_loss: 0.0480\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0331 - val_loss: 0.0636\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0308 - val_loss: 0.0524\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0266 - val_loss: 0.0485\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0246 - val_loss: 0.0424\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0231 - val_loss: 0.0401\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0228 - val_loss: 0.0453\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0223 - val_loss: 0.0505\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0217 - val_loss: 0.0466\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0212 - val_loss: 0.0433\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0213 - val_loss: 0.0492\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0211 - val_loss: 0.0489\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0213 - val_loss: 0.0434\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0212 - val_loss: 0.0434\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0209 - val_loss: 0.0447\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0209 - val_loss: 0.0457\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0212 - val_loss: 0.0480\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0210 - val_loss: 0.0474\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0213 - val_loss: 0.0470\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0207 - val_loss: 0.0472\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0201 - val_loss: 0.0462\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0205 - val_loss: 0.0505\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0216 - val_loss: 0.0449\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0209 - val_loss: 0.0466\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0206 - val_loss: 0.0475\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0200 - val_loss: 0.0470\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0197 - val_loss: 0.0497\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0196 - val_loss: 0.0480\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0206 - val_loss: 0.0449\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0194 - val_loss: 0.0445\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0204 - val_loss: 0.0481\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0188 - val_loss: 0.0472\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0189 - val_loss: 0.0434\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0189 - val_loss: 0.0477\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0188 - val_loss: 0.0473\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0191 - val_loss: 0.0462\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0188 - val_loss: 0.0491\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0183 - val_loss: 0.0461\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0186 - val_loss: 0.0524\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0180 - val_loss: 0.0475\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0188 - val_loss: 0.0514\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0184 - val_loss: 0.0438\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0189 - val_loss: 0.0482\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0194 - val_loss: 0.0489\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0180 - val_loss: 0.0487\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0176 - val_loss: 0.0468\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0182 - val_loss: 0.0507\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0179 - val_loss: 0.0505\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0175 - val_loss: 0.0479\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0178 - val_loss: 0.0446\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0179 - val_loss: 0.0482\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0182 - val_loss: 0.0494\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0187 - val_loss: 0.0431\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0177 - val_loss: 0.0523\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0166 - val_loss: 0.0477\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0164 - val_loss: 0.0479\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0164 - val_loss: 0.0493\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0165 - val_loss: 0.0482\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0164 - val_loss: 0.0487\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0166 - val_loss: 0.0496\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0168 - val_loss: 0.0507\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0165 - val_loss: 0.0522\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0167 - val_loss: 0.0471\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0163 - val_loss: 0.0446\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0160 - val_loss: 0.0496\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0159 - val_loss: 0.0449\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0156 - val_loss: 0.0482\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0161 - val_loss: 0.0472\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0165 - val_loss: 0.0485\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0166 - val_loss: 0.0445\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0162 - val_loss: 0.0476\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0160 - val_loss: 0.0460\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0160 - val_loss: 0.0487\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0160 - val_loss: 0.0502\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0306 - val_loss: 0.0572\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0377 - val_loss: 0.0389\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0239 - val_loss: 0.0456\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0190 - val_loss: 0.0440\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0178 - val_loss: 0.0471\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0176 - val_loss: 0.0442\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0161 - val_loss: 0.0463\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0161 - val_loss: 0.0525\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0159 - val_loss: 0.0461\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0156 - val_loss: 0.0472\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0153 - val_loss: 0.0492\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0151 - val_loss: 0.0486\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0149 - val_loss: 0.0472\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0149 - val_loss: 0.0474\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0152 - val_loss: 0.0480\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0151 - val_loss: 0.0462\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 4s 67ms/step - loss: 0.1622 - val_loss: 0.1307\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1525 - val_loss: 0.1202\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1268 - val_loss: 0.1081\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1194 - val_loss: 0.0986\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1119 - val_loss: 0.0993\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1066 - val_loss: 0.1052\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1019 - val_loss: 0.1132\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0997 - val_loss: 0.1095\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0967 - val_loss: 0.1282\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0946 - val_loss: 0.1162\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0870 - val_loss: 0.1821\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0829 - val_loss: 0.1702\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0809 - val_loss: 0.1605\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0787 - val_loss: 0.1856\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0751 - val_loss: 0.1771\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0741 - val_loss: 0.1514\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0775 - val_loss: 0.1754\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0713 - val_loss: 0.1371\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0716 - val_loss: 0.1942\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0723 - val_loss: 0.1448\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0718 - val_loss: 0.1770\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0750 - val_loss: 0.1131\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0690 - val_loss: 0.0979\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0642 - val_loss: 0.1151\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0616 - val_loss: 0.1229\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0631 - val_loss: 0.0667\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0602 - val_loss: 0.1272\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0602 - val_loss: 0.0880\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0550 - val_loss: 0.0981\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0553 - val_loss: 0.0927\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0523 - val_loss: 0.0606\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0516 - val_loss: 0.0669\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0514 - val_loss: 0.0605\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0513 - val_loss: 0.0677\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0481 - val_loss: 0.0600\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0472 - val_loss: 0.0589\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0473 - val_loss: 0.0585\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0443 - val_loss: 0.0601\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0454 - val_loss: 0.0894\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0498 - val_loss: 0.0548\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0443 - val_loss: 0.0573\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0426 - val_loss: 0.0630\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0406 - val_loss: 0.0787\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0395 - val_loss: 0.0774\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0411 - val_loss: 0.0840\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0443 - val_loss: 0.0843\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0407 - val_loss: 0.0601\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0386 - val_loss: 0.0738\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0387 - val_loss: 0.0643\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0372 - val_loss: 0.0748\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0368 - val_loss: 0.0735\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0468 - val_loss: 0.0627\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0415 - val_loss: 0.0586\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0402 - val_loss: 0.0573\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0433 - val_loss: 0.0545\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0381 - val_loss: 0.0556\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0352 - val_loss: 0.0651\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0345 - val_loss: 0.0587\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0359 - val_loss: 0.0643\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.0570\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0336 - val_loss: 0.0674\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0340 - val_loss: 0.0566\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0327 - val_loss: 0.0650\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0340 - val_loss: 0.0737\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0313 - val_loss: 0.0668\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0307 - val_loss: 0.0755\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0307 - val_loss: 0.0681\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0302 - val_loss: 0.0760\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0296 - val_loss: 0.0673\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0324 - val_loss: 0.0858\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0305 - val_loss: 0.0664\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0312 - val_loss: 0.0648\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0298 - val_loss: 0.0580\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0291 - val_loss: 0.0664\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0299 - val_loss: 0.0575\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0279 - val_loss: 0.0692\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0281 - val_loss: 0.0629\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0286 - val_loss: 0.0503\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0269 - val_loss: 0.0677\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0270 - val_loss: 0.0532\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0282 - val_loss: 0.0516\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0269 - val_loss: 0.0578\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0260 - val_loss: 0.0617\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0264 - val_loss: 0.0628\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0277 - val_loss: 0.0581\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0283 - val_loss: 0.0500\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0278 - val_loss: 0.0504\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0255 - val_loss: 0.0626\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0261 - val_loss: 0.0620\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0280 - val_loss: 0.0738\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0250 - val_loss: 0.0646\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0242 - val_loss: 0.0618\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0244 - val_loss: 0.0679\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0245 - val_loss: 0.0622\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0237 - val_loss: 0.0551\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0237 - val_loss: 0.0539\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0244 - val_loss: 0.0627\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0254 - val_loss: 0.0644\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0235 - val_loss: 0.0665\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0231 - val_loss: 0.0727\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0233 - val_loss: 0.0860\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0224 - val_loss: 0.0694\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0227 - val_loss: 0.0827\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0223 - val_loss: 0.0695\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0226 - val_loss: 0.0758\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0223 - val_loss: 0.0967\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0229 - val_loss: 0.1091\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0226 - val_loss: 0.0701\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0222 - val_loss: 0.0692\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0238 - val_loss: 0.0683\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0214 - val_loss: 0.0663\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0213 - val_loss: 0.0675\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0223 - val_loss: 0.0710\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0235 - val_loss: 0.0661\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0286 - val_loss: 0.0734\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0248 - val_loss: 0.0666\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0215 - val_loss: 0.0695\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0216 - val_loss: 0.0687\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0207 - val_loss: 0.0814\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0202 - val_loss: 0.0728\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0209 - val_loss: 0.0974\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0215 - val_loss: 0.0750\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0215 - val_loss: 0.0761\n",
      "Epoch 124/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0196 - val_loss: 0.0772\n",
      "Epoch 125/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0192 - val_loss: 0.0873\n",
      "Epoch 126/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0195 - val_loss: 0.0745\n",
      "Epoch 127/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0190 - val_loss: 0.0757\n",
      "Epoch 128/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0188 - val_loss: 0.0816\n",
      "Epoch 129/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0189 - val_loss: 0.0837\n",
      "Epoch 130/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0192 - val_loss: 0.0774\n",
      "Epoch 131/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0186 - val_loss: 0.0669\n",
      "Epoch 132/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0185 - val_loss: 0.0777\n",
      "Epoch 133/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0182 - val_loss: 0.0780\n",
      "Epoch 134/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0185 - val_loss: 0.0689\n",
      "Epoch 135/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0184 - val_loss: 0.0890\n",
      "Epoch 136/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0200 - val_loss: 0.0687\n",
      "Epoch 137/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0190 - val_loss: 0.0739\n",
      "Epoch 138/500\n",
      "46/46 [==============================] - 3s 66ms/step - loss: 0.0179 - val_loss: 0.0895\n",
      "Epoch 139/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0179 - val_loss: 0.0802\n",
      "Epoch 140/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0182 - val_loss: 0.0752\n",
      "Epoch 141/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0182 - val_loss: 0.0713\n",
      "Epoch 142/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0179 - val_loss: 0.0893\n",
      "Epoch 143/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0215 - val_loss: 0.0593\n",
      "Epoch 144/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0206 - val_loss: 0.0631\n",
      "Epoch 145/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0181 - val_loss: 0.0720\n",
      "Epoch 146/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0173 - val_loss: 0.0747\n",
      "Epoch 147/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0180 - val_loss: 0.0652\n",
      "Epoch 148/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0184 - val_loss: 0.0618\n",
      "Epoch 149/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0177 - val_loss: 0.0653\n",
      "Epoch 150/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0169 - val_loss: 0.0750\n",
      "Epoch 151/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0170 - val_loss: 0.0746\n",
      "Epoch 152/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0177 - val_loss: 0.0752\n",
      "Epoch 153/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0171 - val_loss: 0.0826\n",
      "Epoch 154/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0168 - val_loss: 0.0650\n",
      "Epoch 155/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0167 - val_loss: 0.0724\n",
      "Epoch 156/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0166 - val_loss: 0.0951\n",
      "Epoch 157/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0164 - val_loss: 0.0924\n",
      "Epoch 158/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0163 - val_loss: 0.0773\n",
      "Epoch 159/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0160 - val_loss: 0.0765\n",
      "Epoch 160/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0162 - val_loss: 0.0807\n",
      "Epoch 161/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0161 - val_loss: 0.0800\n",
      "Epoch 162/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0160 - val_loss: 0.0914\n",
      "Epoch 163/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0184 - val_loss: 0.0795\n",
      "Epoch 164/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0165 - val_loss: 0.0641\n",
      "Epoch 165/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0173 - val_loss: 0.0840\n",
      "Epoch 166/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0168 - val_loss: 0.0913\n",
      "Epoch 167/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0159 - val_loss: 0.0742\n",
      "Epoch 168/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0155 - val_loss: 0.0698\n",
      "Epoch 169/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0154 - val_loss: 0.0828\n",
      "Epoch 170/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0156 - val_loss: 0.0785\n",
      "Epoch 171/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0156 - val_loss: 0.0727\n",
      "Epoch 172/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0153 - val_loss: 0.0806\n",
      "Epoch 173/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0153 - val_loss: 0.0820\n",
      "Epoch 174/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0152 - val_loss: 0.0833\n",
      "Epoch 175/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0151 - val_loss: 0.0733\n",
      "Epoch 176/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0152 - val_loss: 0.0774\n",
      "Epoch 177/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0152 - val_loss: 0.0753\n",
      "Epoch 178/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0159 - val_loss: 0.0719\n",
      "Epoch 179/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0155 - val_loss: 0.0638\n",
      "Epoch 180/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0162 - val_loss: 0.0892\n",
      "Epoch 181/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0342 - val_loss: 0.0576\n",
      "Epoch 182/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0237 - val_loss: 0.0752\n",
      "Epoch 183/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0179 - val_loss: 0.0760\n",
      "Epoch 184/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0160 - val_loss: 0.0885\n",
      "Epoch 185/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0159 - val_loss: 0.0772\n",
      "Epoch 186/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0152 - val_loss: 0.0796\n",
      "Epoch 187/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0148 - val_loss: 0.0702\n",
      "Epoch 188/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0146 - val_loss: 0.0721\n",
      "Epoch 189/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0146 - val_loss: 0.0738\n",
      "Epoch 190/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0145 - val_loss: 0.0865\n",
      "Epoch 191/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0145 - val_loss: 0.0787\n",
      "Epoch 192/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0144 - val_loss: 0.0786\n",
      "Epoch 193/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0146 - val_loss: 0.0955\n",
      "Epoch 194/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0145 - val_loss: 0.0804\n",
      "Epoch 195/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0143 - val_loss: 0.0870\n",
      "Epoch 196/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0142 - val_loss: 0.0944\n",
      "Epoch 197/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0141 - val_loss: 0.0849\n",
      "Epoch 198/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0141 - val_loss: 0.0942\n",
      "Epoch 199/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0140 - val_loss: 0.1040\n",
      "Epoch 200/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0143 - val_loss: 0.0815\n",
      "Epoch 201/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0142 - val_loss: 0.0804\n",
      "Epoch 202/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0140 - val_loss: 0.0804\n",
      "Epoch 203/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0144 - val_loss: 0.0921\n",
      "Epoch 204/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0141 - val_loss: 0.0763\n",
      "Epoch 205/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0138 - val_loss: 0.0730\n",
      "Epoch 206/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0138 - val_loss: 0.0794\n",
      "Epoch 207/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0139 - val_loss: 0.0820\n",
      "Epoch 208/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0137 - val_loss: 0.1049\n",
      "Epoch 209/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0136 - val_loss: 0.0809\n",
      "Epoch 210/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0137 - val_loss: 0.0813\n",
      "Epoch 211/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0137 - val_loss: 0.0895\n",
      "Epoch 212/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0137 - val_loss: 0.0794\n",
      "Epoch 213/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0137 - val_loss: 0.0861\n",
      "Epoch 214/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0140 - val_loss: 0.0865\n",
      "Epoch 215/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0144 - val_loss: 0.1042\n",
      "Epoch 216/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0137 - val_loss: 0.0810\n",
      "Epoch 217/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0135 - val_loss: 0.0971\n",
      "Epoch 218/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0133 - val_loss: 0.0901\n",
      "Epoch 219/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0134 - val_loss: 0.0882\n",
      "Epoch 220/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0138 - val_loss: 0.0859\n",
      "Epoch 221/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0140 - val_loss: 0.1016\n",
      "Epoch 222/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0144 - val_loss: 0.0684\n",
      "Epoch 223/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0136 - val_loss: 0.0968\n",
      "Epoch 224/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0141 - val_loss: 0.0824\n",
      "Epoch 225/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0155 - val_loss: 0.0889\n",
      "Epoch 226/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0146 - val_loss: 0.0904\n",
      "Epoch 227/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0140 - val_loss: 0.0807\n",
      "Epoch 228/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0130 - val_loss: 0.0828\n",
      "Epoch 229/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0130 - val_loss: 0.0774\n",
      "Epoch 230/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0129 - val_loss: 0.0740\n",
      "Epoch 231/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0128 - val_loss: 0.0808\n",
      "Epoch 232/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0128 - val_loss: 0.0893\n",
      "Epoch 233/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0129 - val_loss: 0.0961\n",
      "Epoch 234/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0131 - val_loss: 0.1102\n",
      "Epoch 235/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0132 - val_loss: 0.0838\n",
      "Epoch 236/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0128 - val_loss: 0.0786\n",
      "Epoch 237/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0132 - val_loss: 0.0835\n",
      "Epoch 238/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0133 - val_loss: 0.1061\n",
      "Epoch 239/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0135 - val_loss: 0.0837\n",
      "Epoch 240/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0135 - val_loss: 0.0790\n",
      "Epoch 241/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0146 - val_loss: 0.0673\n",
      "Epoch 242/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0129 - val_loss: 0.0895\n",
      "Epoch 243/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0129 - val_loss: 0.0776\n",
      "Epoch 244/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0128 - val_loss: 0.0893\n",
      "Epoch 245/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0127 - val_loss: 0.0752\n",
      "Epoch 246/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0125 - val_loss: 0.0728\n",
      "Epoch 247/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0123 - val_loss: 0.0820\n",
      "Epoch 248/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0126 - val_loss: 0.0895\n",
      "Epoch 249/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0124 - val_loss: 0.0791\n",
      "Epoch 250/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0124 - val_loss: 0.0835\n",
      "Epoch 251/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0125 - val_loss: 0.1008\n",
      "Epoch 252/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0125 - val_loss: 0.0879\n",
      "Epoch 253/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0124 - val_loss: 0.0883\n",
      "Epoch 254/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0123 - val_loss: 0.0925\n",
      "Epoch 255/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0124 - val_loss: 0.0838\n",
      "Epoch 256/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0122 - val_loss: 0.0821\n",
      "Epoch 257/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0122 - val_loss: 0.1039\n",
      "Epoch 258/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0128 - val_loss: 0.0773\n",
      "Epoch 259/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0124 - val_loss: 0.0690\n",
      "Epoch 260/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0122 - val_loss: 0.0721\n",
      "Epoch 261/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0123 - val_loss: 0.0947\n",
      "Epoch 262/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0131 - val_loss: 0.1031\n",
      "Epoch 263/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0180 - val_loss: 0.0565\n",
      "Epoch 264/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0197 - val_loss: 0.0593\n",
      "Epoch 265/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0151 - val_loss: 0.0673\n",
      "Epoch 266/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0127 - val_loss: 0.0802\n",
      "Epoch 267/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0121 - val_loss: 0.0780\n",
      "Epoch 268/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0120 - val_loss: 0.0861\n",
      "Epoch 269/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0120 - val_loss: 0.0741\n",
      "Epoch 270/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0119 - val_loss: 0.0832\n",
      "Epoch 271/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0120 - val_loss: 0.0847\n",
      "Epoch 272/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0118 - val_loss: 0.0820\n",
      "Epoch 273/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0117 - val_loss: 0.0853\n",
      "Epoch 274/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0117 - val_loss: 0.0792\n",
      "Epoch 275/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0118 - val_loss: 0.0805\n",
      "Epoch 276/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0118 - val_loss: 0.0910\n",
      "Epoch 277/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0119 - val_loss: 0.0822\n",
      "Epoch 278/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0117 - val_loss: 0.0870\n",
      "Epoch 279/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0115 - val_loss: 0.0780\n",
      "Epoch 280/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0115 - val_loss: 0.0786\n",
      "Epoch 281/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0115 - val_loss: 0.0820\n",
      "Epoch 282/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0115 - val_loss: 0.0890\n",
      "Epoch 283/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0115 - val_loss: 0.0663\n",
      "Epoch 284/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0116 - val_loss: 0.0690\n",
      "Epoch 285/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0115 - val_loss: 0.0770\n",
      "Epoch 286/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0115 - val_loss: 0.0869\n",
      "Epoch 287/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0115 - val_loss: 0.0969\n",
      "Epoch 288/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0116 - val_loss: 0.0827\n",
      "Epoch 289/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0115 - val_loss: 0.0751\n",
      "Epoch 290/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0119 - val_loss: 0.0669\n",
      "Epoch 291/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0117 - val_loss: 0.0926\n",
      "Epoch 292/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0116 - val_loss: 0.0868\n",
      "Epoch 293/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0113 - val_loss: 0.1184\n",
      "Epoch 294/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0114 - val_loss: 0.0905\n",
      "Epoch 295/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0128 - val_loss: 0.0666\n",
      "Epoch 296/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0390 - val_loss: 0.0592\n",
      "Epoch 297/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0255 - val_loss: 0.0611\n",
      "Epoch 298/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0179 - val_loss: 0.0651\n",
      "Epoch 299/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0153 - val_loss: 0.0821\n",
      "Epoch 300/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0145 - val_loss: 0.1532\n",
      "Epoch 301/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0135 - val_loss: 0.0685\n",
      "Epoch 302/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0126 - val_loss: 0.0756\n",
      "Epoch 303/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0122 - val_loss: 0.0751\n",
      "Epoch 304/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0121 - val_loss: 0.0973\n",
      "Epoch 305/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0117 - val_loss: 0.0917\n",
      "Epoch 306/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0117 - val_loss: 0.0876\n",
      "Epoch 307/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0116 - val_loss: 0.0824\n",
      "Epoch 308/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0114 - val_loss: 0.0891\n",
      "Epoch 309/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0114 - val_loss: 0.0887\n",
      "Epoch 310/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0114 - val_loss: 0.1005\n",
      "Epoch 311/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0114 - val_loss: 0.0913\n",
      "Epoch 312/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0113 - val_loss: 0.0853\n",
      "Epoch 313/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0112 - val_loss: 0.0959\n",
      "Epoch 314/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0112 - val_loss: 0.0972\n",
      "Epoch 315/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0112 - val_loss: 0.0919\n",
      "Epoch 316/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0111 - val_loss: 0.0911\n",
      "Epoch 317/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0111 - val_loss: 0.0938\n",
      "Epoch 318/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0111 - val_loss: 0.1029\n",
      "Epoch 319/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0919\n",
      "Epoch 320/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0110 - val_loss: 0.1184\n",
      "Epoch 321/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0110 - val_loss: 0.1007\n",
      "Epoch 322/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0110 - val_loss: 0.0905\n",
      "Epoch 323/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0109 - val_loss: 0.0883\n",
      "Epoch 324/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0109 - val_loss: 0.0878\n",
      "Epoch 325/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0110 - val_loss: 0.0874\n",
      "Epoch 326/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0109 - val_loss: 0.0877\n",
      "Epoch 327/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0109 - val_loss: 0.0953\n",
      "Epoch 328/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0109 - val_loss: 0.0789\n",
      "Epoch 329/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0110 - val_loss: 0.0792\n",
      "Epoch 330/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0111 - val_loss: 0.0891\n",
      "Epoch 331/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0108 - val_loss: 0.0913\n",
      "Epoch 332/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0108 - val_loss: 0.0794\n",
      "Epoch 333/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0109 - val_loss: 0.0892\n",
      "Epoch 334/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0914\n",
      "Epoch 335/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0109 - val_loss: 0.0984\n",
      "Epoch 336/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0107 - val_loss: 0.0876\n",
      "Epoch 337/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0108 - val_loss: 0.0831\n",
      "Epoch 338/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0109 - val_loss: 0.1109\n",
      "Epoch 339/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0107 - val_loss: 0.1100\n",
      "Epoch 340/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0108 - val_loss: 0.0919\n",
      "Epoch 341/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0109 - val_loss: 0.1037\n",
      "Epoch 342/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0106 - val_loss: 0.0911\n",
      "Epoch 343/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0106 - val_loss: 0.0844\n",
      "Epoch 344/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0109 - val_loss: 0.0855\n",
      "Epoch 345/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0112 - val_loss: 0.0901\n",
      "Epoch 346/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0492 - val_loss: 0.0518\n",
      "Epoch 347/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0370 - val_loss: 0.0473\n",
      "Epoch 348/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0233 - val_loss: 0.0690\n",
      "Epoch 349/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0188 - val_loss: 0.1171\n",
      "Epoch 350/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0166 - val_loss: 0.0651\n",
      "Epoch 351/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0175 - val_loss: 0.0633\n",
      "Epoch 352/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0134 - val_loss: 0.0877\n",
      "Epoch 353/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0127 - val_loss: 0.0784\n",
      "Epoch 354/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0123 - val_loss: 0.1029\n",
      "Epoch 355/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0119 - val_loss: 0.0910\n",
      "Epoch 356/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0117 - val_loss: 0.0963\n",
      "Epoch 357/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0116 - val_loss: 0.0932\n",
      "Epoch 358/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0114 - val_loss: 0.0892\n",
      "Epoch 359/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0113 - val_loss: 0.0950\n",
      "Epoch 360/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0111 - val_loss: 0.0817\n",
      "Epoch 361/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0111 - val_loss: 0.0872\n",
      "Epoch 362/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0111 - val_loss: 0.0940\n",
      "Epoch 363/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0877\n",
      "Epoch 364/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0109 - val_loss: 0.0806\n",
      "Epoch 365/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0108 - val_loss: 0.0814\n",
      "Epoch 366/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0107 - val_loss: 0.0849\n",
      "Epoch 367/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0109 - val_loss: 0.0836\n",
      "Epoch 368/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0107 - val_loss: 0.0895\n",
      "Epoch 369/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0106 - val_loss: 0.0901\n",
      "Epoch 370/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0106 - val_loss: 0.0830\n",
      "Epoch 371/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0107 - val_loss: 0.0927\n",
      "Epoch 372/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0106 - val_loss: 0.0886\n",
      "Epoch 373/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0105 - val_loss: 0.0839\n",
      "Epoch 374/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0106 - val_loss: 0.0895\n",
      "Epoch 375/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0105 - val_loss: 0.0965\n",
      "Epoch 376/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0105 - val_loss: 0.0918\n",
      "Epoch 377/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0104 - val_loss: 0.0958\n",
      "Epoch 378/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0104 - val_loss: 0.0875\n",
      "Epoch 379/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0104 - val_loss: 0.0948\n",
      "Epoch 380/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0104 - val_loss: 0.0890\n",
      "Epoch 381/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0104 - val_loss: 0.0898\n",
      "Epoch 382/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0103 - val_loss: 0.0944\n",
      "Epoch 383/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0104 - val_loss: 0.0842\n",
      "Epoch 384/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0103 - val_loss: 0.0911\n",
      "Epoch 385/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0104 - val_loss: 0.1022\n",
      "Epoch 386/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0105 - val_loss: 0.1033\n",
      "Epoch 387/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0116 - val_loss: 0.0590\n",
      "Epoch 388/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0149 - val_loss: 0.0667\n",
      "Epoch 389/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0120 - val_loss: 0.0769\n",
      "Epoch 390/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0108 - val_loss: 0.0740\n",
      "Epoch 391/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0104 - val_loss: 0.0735\n",
      "Epoch 392/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0102 - val_loss: 0.0778\n",
      "Epoch 393/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0102 - val_loss: 0.0800\n",
      "Epoch 394/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0102 - val_loss: 0.0813\n",
      "Epoch 395/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0102 - val_loss: 0.0793\n",
      "Epoch 396/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0101 - val_loss: 0.0800\n",
      "Epoch 397/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0101 - val_loss: 0.0825\n",
      "Epoch 398/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0100 - val_loss: 0.0871\n",
      "Epoch 399/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0101 - val_loss: 0.0821\n",
      "Epoch 400/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0101 - val_loss: 0.0840\n",
      "Epoch 401/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0101 - val_loss: 0.0796\n",
      "Epoch 402/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0101 - val_loss: 0.0903\n",
      "Epoch 403/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0845\n",
      "Epoch 404/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0101 - val_loss: 0.0805\n",
      "Epoch 405/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0101 - val_loss: 0.0824\n",
      "Epoch 406/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0101 - val_loss: 0.0842\n",
      "Epoch 407/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0100 - val_loss: 0.0839\n",
      "Epoch 408/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0100 - val_loss: 0.0846\n",
      "Epoch 409/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0099 - val_loss: 0.0804\n",
      "Epoch 410/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0099 - val_loss: 0.0941\n",
      "Epoch 411/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0099 - val_loss: 0.0820\n",
      "Epoch 412/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0100 - val_loss: 0.0898\n",
      "Epoch 413/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0100 - val_loss: 0.0882\n",
      "Epoch 414/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0851\n",
      "Epoch 415/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0840\n",
      "Epoch 416/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0098 - val_loss: 0.1003\n",
      "Epoch 417/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0989\n",
      "Epoch 418/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0834\n",
      "Epoch 419/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0098 - val_loss: 0.0890\n",
      "Epoch 420/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0099 - val_loss: 0.0815\n",
      "Epoch 421/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0098 - val_loss: 0.0901\n",
      "Epoch 422/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0098 - val_loss: 0.0908\n",
      "Epoch 423/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0101 - val_loss: 0.0922\n",
      "Epoch 424/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0106 - val_loss: 0.0862\n",
      "Epoch 425/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0104 - val_loss: 0.0788\n",
      "Epoch 426/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0100 - val_loss: 0.0846\n",
      "Epoch 427/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0098 - val_loss: 0.1056\n",
      "Epoch 428/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0100 - val_loss: 0.0850\n",
      "Epoch 429/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0098 - val_loss: 0.0845\n",
      "Epoch 430/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0098 - val_loss: 0.0817\n",
      "Epoch 431/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0100 - val_loss: 0.0835\n",
      "Epoch 432/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0098 - val_loss: 0.0845\n",
      "Epoch 433/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0097 - val_loss: 0.0846\n",
      "Epoch 434/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0097 - val_loss: 0.0931\n",
      "Epoch 435/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0097 - val_loss: 0.0875\n",
      "Epoch 436/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0096 - val_loss: 0.0884\n",
      "Epoch 437/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0097 - val_loss: 0.0903\n",
      "Epoch 438/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0098 - val_loss: 0.0816\n",
      "Epoch 439/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0097 - val_loss: 0.0883\n",
      "Epoch 440/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0099 - val_loss: 0.0836\n",
      "Epoch 441/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0096 - val_loss: 0.0944\n",
      "Epoch 442/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0097 - val_loss: 0.0869\n",
      "Epoch 443/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0097 - val_loss: 0.0949\n",
      "Epoch 444/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0096 - val_loss: 0.0885\n",
      "Epoch 445/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0096 - val_loss: 0.0991\n",
      "Epoch 446/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0096 - val_loss: 0.1075\n",
      "Epoch 447/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0097 - val_loss: 0.1015\n",
      "Epoch 448/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0096 - val_loss: 0.0792\n",
      "Epoch 449/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0097 - val_loss: 0.0992\n",
      "Epoch 450/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0097 - val_loss: 0.1078\n",
      "Epoch 451/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0099 - val_loss: 0.0831\n",
      "Epoch 452/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0099 - val_loss: 0.0985\n",
      "Epoch 453/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0969\n",
      "Epoch 454/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0100 - val_loss: 0.0907\n",
      "Epoch 455/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0098 - val_loss: 0.1011\n",
      "Epoch 456/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0095 - val_loss: 0.1027\n",
      "Epoch 457/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0094 - val_loss: 0.0992\n",
      "Epoch 458/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0095 - val_loss: 0.0900\n",
      "Epoch 459/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0096 - val_loss: 0.0864\n",
      "Epoch 460/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0095 - val_loss: 0.0908\n",
      "Epoch 461/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.0907\n",
      "Epoch 462/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.1143\n",
      "Epoch 463/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0094 - val_loss: 0.0919\n",
      "Epoch 464/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0093 - val_loss: 0.0849\n",
      "Epoch 465/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0093 - val_loss: 0.0944\n",
      "Epoch 466/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0093 - val_loss: 0.1036\n",
      "Epoch 467/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.0881\n",
      "Epoch 468/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0094 - val_loss: 0.0893\n",
      "Epoch 469/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0093 - val_loss: 0.0999\n",
      "Epoch 470/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0093 - val_loss: 0.0839\n",
      "Epoch 471/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0093 - val_loss: 0.0919\n",
      "Epoch 472/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0093 - val_loss: 0.0994\n",
      "Epoch 473/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0101 - val_loss: 0.0963\n",
      "Epoch 474/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0104 - val_loss: 0.0669\n",
      "Epoch 475/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0310 - val_loss: 0.1130\n",
      "Epoch 476/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0380 - val_loss: 0.0371\n",
      "Epoch 477/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0265 - val_loss: 0.0515\n",
      "Epoch 478/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0168 - val_loss: 0.0510\n",
      "Epoch 479/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0133 - val_loss: 0.0588\n",
      "Epoch 480/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0120 - val_loss: 0.0692\n",
      "Epoch 481/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0110 - val_loss: 0.0669\n",
      "Epoch 482/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0106 - val_loss: 0.0682\n",
      "Epoch 483/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0103 - val_loss: 0.0699\n",
      "Epoch 484/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0102 - val_loss: 0.0724\n",
      "Epoch 485/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0100 - val_loss: 0.0712\n",
      "Epoch 486/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0099 - val_loss: 0.0698\n",
      "Epoch 487/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0098 - val_loss: 0.0723\n",
      "Epoch 488/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0097 - val_loss: 0.0736\n",
      "Epoch 489/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0097 - val_loss: 0.0735\n",
      "Epoch 490/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0096 - val_loss: 0.0754\n",
      "Epoch 491/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0096 - val_loss: 0.0774\n",
      "Epoch 492/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0095 - val_loss: 0.0759\n",
      "Epoch 493/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0095 - val_loss: 0.0764\n",
      "Epoch 494/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0094 - val_loss: 0.0747\n",
      "Epoch 495/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.0774\n",
      "Epoch 496/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0094 - val_loss: 0.0753\n",
      "Epoch 497/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0093 - val_loss: 0.0773\n",
      "Epoch 498/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0093 - val_loss: 0.0786\n",
      "Epoch 499/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0092 - val_loss: 0.0778\n",
      "Epoch 500/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0093 - val_loss: 0.0828\n",
      "8/8 [==============================] - 0s 27ms/step\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 4s 67ms/step - loss: 0.1615 - val_loss: 0.1359\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.1509 - val_loss: 0.1344\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1231 - val_loss: 0.1194\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1063 - val_loss: 0.1354\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1036 - val_loss: 0.1229\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0987 - val_loss: 0.1184\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0958 - val_loss: 0.1205\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0968 - val_loss: 0.1273\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0937 - val_loss: 0.1357\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0846 - val_loss: 0.1047\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0807 - val_loss: 0.1214\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0822 - val_loss: 0.1013\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0777 - val_loss: 0.0861\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0765 - val_loss: 0.0986\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0745 - val_loss: 0.1041\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0734 - val_loss: 0.0951\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0712 - val_loss: 0.0758\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0693 - val_loss: 0.0817\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0746 - val_loss: 0.0856\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0677 - val_loss: 0.0811\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0642 - val_loss: 0.0688\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0621 - val_loss: 0.0663\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0615 - val_loss: 0.0676\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0649 - val_loss: 0.0904\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0631 - val_loss: 0.0575\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0633 - val_loss: 0.0576\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0582 - val_loss: 0.0540\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0558 - val_loss: 0.0539\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0528 - val_loss: 0.0557\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0549 - val_loss: 0.0556\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0558 - val_loss: 0.0482\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0525 - val_loss: 0.0513\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0525 - val_loss: 0.0506\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0479 - val_loss: 0.0510\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0478 - val_loss: 0.0493\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0469 - val_loss: 0.0572\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0556 - val_loss: 0.0543\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0500 - val_loss: 0.0529\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0452 - val_loss: 0.0580\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0448 - val_loss: 0.0586\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0427 - val_loss: 0.0536\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0410 - val_loss: 0.0541\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0419 - val_loss: 0.0606\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0407 - val_loss: 0.0592\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0411 - val_loss: 0.0676\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0385 - val_loss: 0.0644\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0388 - val_loss: 0.0692\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0413 - val_loss: 0.0678\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0382 - val_loss: 0.0618\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0363 - val_loss: 0.0800\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0355 - val_loss: 0.0841\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0405 - val_loss: 0.0603\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0373 - val_loss: 0.0718\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0354 - val_loss: 0.0671\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0351 - val_loss: 0.0532\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0347 - val_loss: 0.0793\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0328 - val_loss: 0.0621\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0356 - val_loss: 0.0682\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0335 - val_loss: 0.0780\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0329 - val_loss: 0.0683\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0333 - val_loss: 0.0707\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0324 - val_loss: 0.0674\n",
      "Epoch 63/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0317 - val_loss: 0.0749\n",
      "Epoch 64/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0309 - val_loss: 0.0699\n",
      "Epoch 65/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0308 - val_loss: 0.0714\n",
      "Epoch 66/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0305 - val_loss: 0.0640\n",
      "Epoch 67/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0307 - val_loss: 0.0573\n",
      "Epoch 68/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0344 - val_loss: 0.0578\n",
      "Epoch 69/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0336 - val_loss: 0.0585\n",
      "Epoch 70/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0310 - val_loss: 0.0583\n",
      "Epoch 71/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0299 - val_loss: 0.0606\n",
      "Epoch 72/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0294 - val_loss: 0.0573\n",
      "Epoch 73/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0297 - val_loss: 0.0613\n",
      "Epoch 74/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0290 - val_loss: 0.0583\n",
      "Epoch 75/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0295 - val_loss: 0.0527\n",
      "Epoch 76/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0280 - val_loss: 0.0570\n",
      "Epoch 77/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0290 - val_loss: 0.0607\n",
      "Epoch 78/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0280 - val_loss: 0.0583\n",
      "Epoch 79/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0268 - val_loss: 0.0626\n",
      "Epoch 80/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0273 - val_loss: 0.0551\n",
      "Epoch 81/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0271 - val_loss: 0.0535\n",
      "Epoch 82/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0282 - val_loss: 0.0560\n",
      "Epoch 83/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0261 - val_loss: 0.0658\n",
      "Epoch 84/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0266 - val_loss: 0.0560\n",
      "Epoch 85/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0266 - val_loss: 0.0571\n",
      "Epoch 86/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0263 - val_loss: 0.0531\n",
      "Epoch 87/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0257 - val_loss: 0.0669\n",
      "Epoch 88/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0260 - val_loss: 0.0541\n",
      "Epoch 89/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0260 - val_loss: 0.0603\n",
      "Epoch 90/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0246 - val_loss: 0.0633\n",
      "Epoch 91/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0254 - val_loss: 0.0575\n",
      "Epoch 92/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0249 - val_loss: 0.0587\n",
      "Epoch 93/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0236 - val_loss: 0.0543\n",
      "Epoch 94/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0242 - val_loss: 0.0549\n",
      "Epoch 95/100\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0245 - val_loss: 0.0551\n",
      "Epoch 96/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0241 - val_loss: 0.0541\n",
      "Epoch 97/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0246 - val_loss: 0.0741\n",
      "Epoch 98/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0248 - val_loss: 0.0447\n",
      "Epoch 99/100\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0245 - val_loss: 0.0514\n",
      "Epoch 100/100\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0229 - val_loss: 0.0573\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 4s 70ms/step - loss: 0.1644 - val_loss: 0.1517\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1560 - val_loss: 0.1385\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1362 - val_loss: 0.1082\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1189 - val_loss: 0.1178\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1134 - val_loss: 0.1087\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.1164 - val_loss: 0.1020\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1070 - val_loss: 0.0987\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1061 - val_loss: 0.0904\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1058 - val_loss: 0.0927\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0996 - val_loss: 0.0905\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.1039 - val_loss: 0.0879\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0975 - val_loss: 0.0960\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0964 - val_loss: 0.0955\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0953 - val_loss: 0.0934\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0952 - val_loss: 0.1107\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0939 - val_loss: 0.0906\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0929 - val_loss: 0.1290\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0943 - val_loss: 0.0981\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0896 - val_loss: 0.1012\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0885 - val_loss: 0.0947\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0873 - val_loss: 0.0911\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0850 - val_loss: 0.1131\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0842 - val_loss: 0.1089\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0818 - val_loss: 0.0876\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0808 - val_loss: 0.0908\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0746 - val_loss: 0.0880\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0776 - val_loss: 0.0748\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0708 - val_loss: 0.0838\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0737 - val_loss: 0.0649\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0697 - val_loss: 0.0647\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0710 - val_loss: 0.0616\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0770 - val_loss: 0.0711\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0674 - val_loss: 0.0629\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0638 - val_loss: 0.0577\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0616 - val_loss: 0.0593\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0599 - val_loss: 0.0569\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0601 - val_loss: 0.0559\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0605 - val_loss: 0.0571\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0569 - val_loss: 0.0575\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0570 - val_loss: 0.0627\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0568 - val_loss: 0.0593\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0549 - val_loss: 0.0651\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0522 - val_loss: 0.0630\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0525 - val_loss: 0.0627\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0530 - val_loss: 0.0639\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0499 - val_loss: 0.0647\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0476 - val_loss: 0.0649\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0513 - val_loss: 0.0654\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0478 - val_loss: 0.0597\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0470 - val_loss: 0.0717\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0465 - val_loss: 0.0631\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0453 - val_loss: 0.0673\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0468 - val_loss: 0.0677\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0446 - val_loss: 0.0818\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0426 - val_loss: 0.0788\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0470 - val_loss: 0.0660\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0410 - val_loss: 0.0715\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0421 - val_loss: 0.0685\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0423 - val_loss: 0.0661\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0399 - val_loss: 0.0694\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0423 - val_loss: 0.0711\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0402 - val_loss: 0.0643\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0390 - val_loss: 0.0738\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0383 - val_loss: 0.0802\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0373 - val_loss: 0.0737\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0377 - val_loss: 0.0733\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0424 - val_loss: 0.0770\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0380 - val_loss: 0.0743\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0378 - val_loss: 0.0629\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0367 - val_loss: 0.0594\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0360 - val_loss: 0.0603\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0367 - val_loss: 0.0600\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0361 - val_loss: 0.0651\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0343 - val_loss: 0.0712\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0341 - val_loss: 0.0679\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0348 - val_loss: 0.0585\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0345 - val_loss: 0.0603\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0315 - val_loss: 0.0622\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0326 - val_loss: 0.0674\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0318 - val_loss: 0.0626\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0317 - val_loss: 0.0717\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0316 - val_loss: 0.0626\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0316 - val_loss: 0.0716\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0306 - val_loss: 0.0640\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0324 - val_loss: 0.0586\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0313 - val_loss: 0.0647\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0300 - val_loss: 0.0609\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0310 - val_loss: 0.0622\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0296 - val_loss: 0.0688\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0305 - val_loss: 0.0636\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0292 - val_loss: 0.0627\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0297 - val_loss: 0.0632\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0299 - val_loss: 0.0599\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0293 - val_loss: 0.0559\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0300 - val_loss: 0.0858\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0302 - val_loss: 0.0596\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0272 - val_loss: 0.0662\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0271 - val_loss: 0.0571\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0269 - val_loss: 0.0612\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0281 - val_loss: 0.0682\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0284 - val_loss: 0.0591\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0306 - val_loss: 0.0737\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0296 - val_loss: 0.0640\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0264 - val_loss: 0.0700\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0264 - val_loss: 0.0629\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0260 - val_loss: 0.0698\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0261 - val_loss: 0.0644\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0250 - val_loss: 0.0649\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0251 - val_loss: 0.0798\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0294 - val_loss: 0.0592\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0254 - val_loss: 0.0574\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0245 - val_loss: 0.0570\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0239 - val_loss: 0.0611\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0250 - val_loss: 0.0556\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0243 - val_loss: 0.0621\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0241 - val_loss: 0.0653\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0237 - val_loss: 0.0636\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0240 - val_loss: 0.0578\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0238 - val_loss: 0.0569\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0233 - val_loss: 0.0580\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0225 - val_loss: 0.0573\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0230 - val_loss: 0.0572\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0225 - val_loss: 0.0650\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0222 - val_loss: 0.0612\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0222 - val_loss: 0.0631\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0222 - val_loss: 0.0570\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0233 - val_loss: 0.0662\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0229 - val_loss: 0.0615\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0224 - val_loss: 0.0588\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0217 - val_loss: 0.0650\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0211 - val_loss: 0.0581\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0209 - val_loss: 0.0587\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0206 - val_loss: 0.0646\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0208 - val_loss: 0.0656\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0206 - val_loss: 0.0554\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0211 - val_loss: 0.0534\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0205 - val_loss: 0.0556\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0225 - val_loss: 0.0603\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0213 - val_loss: 0.0592\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0203 - val_loss: 0.0572\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0201 - val_loss: 0.0574\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0200 - val_loss: 0.0598\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0212 - val_loss: 0.0651\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0250 - val_loss: 0.0475\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0251 - val_loss: 0.0499\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0208 - val_loss: 0.0587\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0192 - val_loss: 0.0564\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0192 - val_loss: 0.0657\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0194 - val_loss: 0.0573\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0191 - val_loss: 0.0528\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0189 - val_loss: 0.0620\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0187 - val_loss: 0.0615\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0186 - val_loss: 0.0614\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0182 - val_loss: 0.0547\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0186 - val_loss: 0.0551\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0181 - val_loss: 0.0564\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0183 - val_loss: 0.0565\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0182 - val_loss: 0.0602\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0178 - val_loss: 0.0577\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0176 - val_loss: 0.0633\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0180 - val_loss: 0.0618\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0178 - val_loss: 0.0608\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0178 - val_loss: 0.0642\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0177 - val_loss: 0.0646\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0174 - val_loss: 0.0625\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0171 - val_loss: 0.0603\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0174 - val_loss: 0.0565\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0178 - val_loss: 0.0648\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0411 - val_loss: 0.0578\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0363 - val_loss: 0.0675\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0274 - val_loss: 0.0628\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0257 - val_loss: 0.0653\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0225 - val_loss: 0.0609\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0200 - val_loss: 0.0653\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0188 - val_loss: 0.0601\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0181 - val_loss: 0.0637\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0174 - val_loss: 0.0626\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0173 - val_loss: 0.0672\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0173 - val_loss: 0.0651\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0168 - val_loss: 0.0632\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0168 - val_loss: 0.0659\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0166 - val_loss: 0.0662\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0164 - val_loss: 0.0590\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0164 - val_loss: 0.0669\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0165 - val_loss: 0.0600\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0165 - val_loss: 0.0673\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0178 - val_loss: 0.0595\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0163 - val_loss: 0.0678\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0159 - val_loss: 0.0623\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0160 - val_loss: 0.0567\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0158 - val_loss: 0.0604\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0158 - val_loss: 0.0616\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0158 - val_loss: 0.0696\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0160 - val_loss: 0.0640\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0155 - val_loss: 0.0616\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0154 - val_loss: 0.0602\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0155 - val_loss: 0.0661\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0155 - val_loss: 0.0645\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0155 - val_loss: 0.0601\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0155 - val_loss: 0.0649\n",
      "8/8 [==============================] - 1s 44ms/step\n",
      "Epoch 1/500\n",
      "46/46 [==============================] - 4s 67ms/step - loss: 0.1606 - val_loss: 0.1391\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1473 - val_loss: 0.1052\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1280 - val_loss: 0.0930\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.1148 - val_loss: 0.0875\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1092 - val_loss: 0.0883\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1021 - val_loss: 0.0841\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.1017 - val_loss: 0.0847\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0970 - val_loss: 0.0840\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0972 - val_loss: 0.0864\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0939 - val_loss: 0.0819\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0918 - val_loss: 0.0761\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0876 - val_loss: 0.0759\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0840 - val_loss: 0.0728\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0833 - val_loss: 0.0677\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0769 - val_loss: 0.0640\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0781 - val_loss: 0.0638\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0766 - val_loss: 0.0621\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0719 - val_loss: 0.0607\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0693 - val_loss: 0.0591\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0697 - val_loss: 0.0612\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0665 - val_loss: 0.0555\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0655 - val_loss: 0.0572\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0635 - val_loss: 0.0546\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0651 - val_loss: 0.0542\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0631 - val_loss: 0.0589\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0609 - val_loss: 0.0596\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0610 - val_loss: 0.0514\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0585 - val_loss: 0.0535\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0569 - val_loss: 0.0606\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0575 - val_loss: 0.0803\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0569 - val_loss: 0.0746\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0532 - val_loss: 0.1164\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0558 - val_loss: 0.0671\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0515 - val_loss: 0.0987\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0525 - val_loss: 0.0905\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0543 - val_loss: 0.0564\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0504 - val_loss: 0.0805\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0470 - val_loss: 0.1114\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0468 - val_loss: 0.1151\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0471 - val_loss: 0.1340\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0476 - val_loss: 0.1495\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0456 - val_loss: 0.1672\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0448 - val_loss: 0.1188\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0517 - val_loss: 0.1430\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0455 - val_loss: 0.1240\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0427 - val_loss: 0.1502\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0424 - val_loss: 0.1176\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0404 - val_loss: 0.1377\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0400 - val_loss: 0.0890\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0411 - val_loss: 0.1376\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0407 - val_loss: 0.0731\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0388 - val_loss: 0.1231\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0383 - val_loss: 0.1283\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0374 - val_loss: 0.1166\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0366 - val_loss: 0.0802\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0380 - val_loss: 0.0952\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0369 - val_loss: 0.1189\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0355 - val_loss: 0.1320\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0351 - val_loss: 0.1050\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0360 - val_loss: 0.1397\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0364 - val_loss: 0.0903\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0340 - val_loss: 0.1192\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0355 - val_loss: 0.0832\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0327 - val_loss: 0.1003\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0317 - val_loss: 0.1106\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0334 - val_loss: 0.1425\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0319 - val_loss: 0.0706\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0319 - val_loss: 0.0752\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0317 - val_loss: 0.1330\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0313 - val_loss: 0.1054\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0319 - val_loss: 0.0970\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0318 - val_loss: 0.0819\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0308 - val_loss: 0.1222\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0297 - val_loss: 0.1075\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0301 - val_loss: 0.0796\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0296 - val_loss: 0.0794\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0295 - val_loss: 0.0870\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0310 - val_loss: 0.0949\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0294 - val_loss: 0.1031\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0309 - val_loss: 0.0737\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0292 - val_loss: 0.0862\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0277 - val_loss: 0.0752\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0279 - val_loss: 0.0887\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0279 - val_loss: 0.1001\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0287 - val_loss: 0.0818\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0287 - val_loss: 0.1015\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0268 - val_loss: 0.0965\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0279 - val_loss: 0.1125\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0280 - val_loss: 0.0934\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0267 - val_loss: 0.1180\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0263 - val_loss: 0.0913\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0256 - val_loss: 0.0845\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0260 - val_loss: 0.0969\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0281 - val_loss: 0.0717\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0269 - val_loss: 0.0832\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0264 - val_loss: 0.0794\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0253 - val_loss: 0.0835\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - 3s 65ms/step - loss: 0.0248 - val_loss: 0.0768\n",
      "Epoch 99/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0243 - val_loss: 0.0839\n",
      "Epoch 100/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0238 - val_loss: 0.1181\n",
      "Epoch 101/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0239 - val_loss: 0.0974\n",
      "Epoch 102/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0244 - val_loss: 0.0880\n",
      "Epoch 103/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0249 - val_loss: 0.0721\n",
      "Epoch 104/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0237 - val_loss: 0.0756\n",
      "Epoch 105/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0233 - val_loss: 0.0904\n",
      "Epoch 106/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0354 - val_loss: 0.1505\n",
      "Epoch 107/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0298 - val_loss: 0.1050\n",
      "Epoch 108/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0254 - val_loss: 0.0664\n",
      "Epoch 109/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0244 - val_loss: 0.1087\n",
      "Epoch 110/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0238 - val_loss: 0.0791\n",
      "Epoch 111/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0225 - val_loss: 0.0827\n",
      "Epoch 112/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0228 - val_loss: 0.0860\n",
      "Epoch 113/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0229 - val_loss: 0.0644\n",
      "Epoch 114/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0225 - val_loss: 0.0780\n",
      "Epoch 115/500\n",
      "46/46 [==============================] - 3s 61ms/step - loss: 0.0218 - val_loss: 0.0719\n",
      "Epoch 116/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0222 - val_loss: 0.0775\n",
      "Epoch 117/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0218 - val_loss: 0.0852\n",
      "Epoch 118/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0214 - val_loss: 0.1036\n",
      "Epoch 119/500\n",
      "46/46 [==============================] - 3s 64ms/step - loss: 0.0212 - val_loss: 0.0729\n",
      "Epoch 120/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0211 - val_loss: 0.0855\n",
      "Epoch 121/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0207 - val_loss: 0.0841\n",
      "Epoch 122/500\n",
      "46/46 [==============================] - 3s 63ms/step - loss: 0.0230 - val_loss: 0.0773\n",
      "Epoch 123/500\n",
      "46/46 [==============================] - 3s 62ms/step - loss: 0.0220 - val_loss: 0.0939\n",
      "Epoch 124/500\n",
      "21/46 [============>.................] - ETA: 1s - loss: 0.0232"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "def train_and_evaluate_model(model, train_data, valid_data, NUM_EPOCHS):\n",
    "    history = model.fit(train_data,\n",
    "                        validation_data=valid_data,\n",
    "                        # callbacks=callbacks_list,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        verbose=1)\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_thresholded = y_pred > 0.5\n",
    "\n",
    "    y_true_f = y_test.astype('float32')\n",
    "    y_pred_f = y_pred_thresholded.astype('float32')\n",
    "\n",
    "    y_true_flatten = y_true_f.reshape(-1)\n",
    "    y_pred_flatten = y_pred_f.reshape(-1)\n",
    "\n",
    "    jaccard = jaccard_score(y_true_flatten, y_pred_flatten)\n",
    "    return jaccard\n",
    "\n",
    "def hyperparameter_search(train_data, valid_data):\n",
    "    optimizer_options = [tf.keras.optimizers.Adam, tf.keras.optimizers.Nadam, tf.keras.optimizers.SGD, tf.keras.optimizers.RMSprop]\n",
    "    loss_options = ['binary_crossentropy', 'binary_focal_crossentropy', bce_dice_loss, jacc_coef]\n",
    "    learning_rates = [1e-4, 1e-3, 1e-2]\n",
    "    batch_sizes = [8, 16, 32]\n",
    "    num_epochs_list = [100, 200, 500]\n",
    "\n",
    "    best_jaccard = 0.0\n",
    "    best_hyperparameters = {}\n",
    "\n",
    "    for optimizer_class, loss_option, lr, batch_size, num_epochs in product(optimizer_options, loss_options, learning_rates, batch_sizes, num_epochs_list):\n",
    "        optimizer = optimizer_class(learning_rate=lr)\n",
    "        loss = loss_option\n",
    "\n",
    "        model_name = f'CustomModel_optimizer_{optimizer_class.__name__}_loss_{loss}_lr_{lr}_batch_{batch_size}_epochs_{num_epochs}'\n",
    "        \n",
    "        # model = create_model(model_name='CXNet', IMG_HEIGHT=256 , IMG_WIDTH=256, IMG_CHANNELS=3)\n",
    "        model = create_model(model_name='unet')\n",
    "        # model = create_model(model_name='rs_net')\n",
    "        # model = create_model(model_name='cloud_net')\n",
    "        model = compile_model(model, optimizer=optimizer, metrics=jacc_coef, loss=loss)\n",
    "\n",
    "        history = train_and_evaluate_model(model, train_data, valid_data, num_epochs)\n",
    "\n",
    "        jaccard = evaluate_model(model, X_test, y_test)\n",
    "        if jaccard > best_jaccard:\n",
    "            best_jaccard = jaccard\n",
    "            best_hyperparameters = {\n",
    "                'name': model_name,\n",
    "                'optimizer': optimizer_class.__name__,\n",
    "                'loss': loss,\n",
    "                'learning_rate': lr,\n",
    "                'batch_size': batch_size,\n",
    "                'num_epochs': num_epochs\n",
    "            }\n",
    "\n",
    "    return best_hyperparameters, best_jaccard\n",
    "\n",
    "# Call the hyperparameter search function\n",
    "best_hyperparameters, best_jaccard = hyperparameter_search(train, valid)\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n",
    "print(\"Best Jaccard Score:\", best_jaccard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n",
    "print(\"Best Jaccard Score:\", best_jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 21:16:18.142927: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/46 [==>...........................] - ETA: 20s - loss: 0.8107 - binary_crossentropy: 0.7032 - jaccard_coef_loss: 8.2776 - jaccard_coef: 0.1879 - jaccard_coef_thresholded: 0.2128 - accuracy: 3.0200e-06 - auc: 0.5688WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2573s vs `on_train_batch_end` time: 0.3030s). Check your callbacks.\n",
      "46/46 [==============================] - 46s 631ms/step - loss: 0.6684 - binary_crossentropy: 0.6422 - jaccard_coef_loss: 5.9614 - jaccard_coef: 0.3306 - jaccard_coef_thresholded: 0.3946 - accuracy: 1.0325e-05 - auc: 0.7738 - val_loss: 0.9263 - val_binary_crossentropy: 0.4150 - val_jaccard_coef_loss: 9.8827 - val_jaccard_coef: 0.0729 - val_jaccard_coef_thresholded: 0.0799 - val_accuracy: 0.0000e+00 - val_auc: 0.6099\n",
      "Epoch 2/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.4903 - binary_crossentropy: 0.6477 - jaccard_coef_loss: 3.2547 - jaccard_coef: 0.5088 - jaccard_coef_thresholded: 0.5319 - accuracy: 2.3635e-06 - auc: 0.8667 - val_loss: 0.9428 - val_binary_crossentropy: 0.3246 - val_jaccard_coef_loss: 8.3478 - val_jaccard_coef: 0.0561 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.6393\n",
      "Epoch 3/500\n",
      "46/46 [==============================] - 28s 608ms/step - loss: 0.4741 - binary_crossentropy: 0.6830 - jaccard_coef_loss: 2.9907 - jaccard_coef: 0.5244 - jaccard_coef_thresholded: 0.5347 - accuracy: 8.0109e-05 - auc: 0.8683 - val_loss: 0.9474 - val_binary_crossentropy: 0.3144 - val_jaccard_coef_loss: 8.0143 - val_jaccard_coef: 0.0516 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.6668\n",
      "Epoch 4/500\n",
      "46/46 [==============================] - 28s 605ms/step - loss: 0.4200 - binary_crossentropy: 0.6061 - jaccard_coef_loss: 2.4993 - jaccard_coef: 0.5795 - jaccard_coef_thresholded: 0.5880 - accuracy: 1.7774e-04 - auc: 0.8810 - val_loss: 0.9667 - val_binary_crossentropy: 0.3302 - val_jaccard_coef_loss: 7.6425 - val_jaccard_coef: 0.0324 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.6386\n",
      "Epoch 5/500\n",
      "46/46 [==============================] - 28s 602ms/step - loss: 0.3972 - binary_crossentropy: 0.6255 - jaccard_coef_loss: 2.2692 - jaccard_coef: 0.6035 - jaccard_coef_thresholded: 0.6101 - accuracy: 3.4009e-04 - auc: 0.8799 - val_loss: 0.9919 - val_binary_crossentropy: 0.4676 - val_jaccard_coef_loss: 8.1404 - val_jaccard_coef: 0.0077 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.5699\n",
      "Epoch 6/500\n",
      "46/46 [==============================] - 27s 595ms/step - loss: 0.3699 - binary_crossentropy: 0.5232 - jaccard_coef_loss: 2.0460 - jaccard_coef: 0.6302 - jaccard_coef_thresholded: 0.6359 - accuracy: 4.0257e-04 - auc: 0.8984 - val_loss: 0.9914 - val_binary_crossentropy: 0.4588 - val_jaccard_coef_loss: 8.0977 - val_jaccard_coef: 0.0083 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.5896\n",
      "Epoch 7/500\n",
      "46/46 [==============================] - 28s 603ms/step - loss: 0.3512 - binary_crossentropy: 0.5227 - jaccard_coef_loss: 1.8825 - jaccard_coef: 0.6479 - jaccard_coef_thresholded: 0.6532 - accuracy: 3.4187e-04 - auc: 0.8991 - val_loss: 0.9970 - val_binary_crossentropy: 0.5722 - val_jaccard_coef_loss: 8.8799 - val_jaccard_coef: 0.0029 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.5331\n",
      "Epoch 8/500\n",
      "46/46 [==============================] - 28s 605ms/step - loss: 0.3772 - binary_crossentropy: 0.5623 - jaccard_coef_loss: 2.0040 - jaccard_coef: 0.6209 - jaccard_coef_thresholded: 0.6275 - accuracy: 0.0015 - auc: 0.8882 - val_loss: 0.9975 - val_binary_crossentropy: 0.5966 - val_jaccard_coef_loss: 9.0562 - val_jaccard_coef: 0.0023 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.5212\n",
      "Epoch 9/500\n",
      "46/46 [==============================] - 28s 602ms/step - loss: 0.3336 - binary_crossentropy: 0.5152 - jaccard_coef_loss: 1.7650 - jaccard_coef: 0.6664 - jaccard_coef_thresholded: 0.6716 - accuracy: 0.0018 - auc: 0.8996 - val_loss: 0.9986 - val_binary_crossentropy: 0.6560 - val_jaccard_coef_loss: 9.5406 - val_jaccard_coef: 0.0013 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.5034\n",
      "Epoch 10/500\n",
      "46/46 [==============================] - 27s 599ms/step - loss: 0.3376 - binary_crossentropy: 0.5163 - jaccard_coef_loss: 1.7714 - jaccard_coef: 0.6626 - jaccard_coef_thresholded: 0.6676 - accuracy: 6.9745e-04 - auc: 0.8993 - val_loss: 0.9992 - val_binary_crossentropy: 0.7154 - val_jaccard_coef_loss: 10.0284 - val_jaccard_coef: 7.4917e-04 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.5006\n",
      "Epoch 11/500\n",
      "46/46 [==============================] - 28s 603ms/step - loss: 0.3405 - binary_crossentropy: 0.5474 - jaccard_coef_loss: 1.7905 - jaccard_coef: 0.6593 - jaccard_coef_thresholded: 0.6629 - accuracy: 0.0018 - auc: 0.8966 - val_loss: 0.9990 - val_binary_crossentropy: 0.7054 - val_jaccard_coef_loss: 9.8772 - val_jaccard_coef: 9.5962e-04 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.5118\n",
      "Epoch 12/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.3302 - binary_crossentropy: 0.5275 - jaccard_coef_loss: 1.7398 - jaccard_coef: 0.6692 - jaccard_coef_thresholded: 0.6728 - accuracy: 0.0023 - auc: 0.9043 - val_loss: 0.9987 - val_binary_crossentropy: 0.7125 - val_jaccard_coef_loss: 9.8200 - val_jaccard_coef: 0.0012 - val_jaccard_coef_thresholded: 0.0828 - val_accuracy: 0.0000e+00 - val_auc: 0.5379\n",
      "Epoch 13/500\n",
      "46/46 [==============================] - 28s 605ms/step - loss: 0.3015 - binary_crossentropy: 0.4824 - jaccard_coef_loss: 1.5611 - jaccard_coef: 0.6977 - jaccard_coef_thresholded: 0.7015 - accuracy: 0.0022 - auc: 0.9100 - val_loss: 0.9352 - val_binary_crossentropy: 0.5389 - val_jaccard_coef_loss: 7.3294 - val_jaccard_coef: 0.0623 - val_jaccard_coef_thresholded: 0.1355 - val_accuracy: 0.0000e+00 - val_auc: 0.7122\n",
      "Epoch 14/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.2891 - binary_crossentropy: 0.4506 - jaccard_coef_loss: 1.4827 - jaccard_coef: 0.7093 - jaccard_coef_thresholded: 0.7134 - accuracy: 0.0028 - auc: 0.9186 - val_loss: 0.6940 - val_binary_crossentropy: 0.3843 - val_jaccard_coef_loss: 5.5009 - val_jaccard_coef: 0.2936 - val_jaccard_coef_thresholded: 0.3738 - val_accuracy: 0.0000e+00 - val_auc: 0.8136\n",
      "Epoch 15/500\n",
      "46/46 [==============================] - 28s 603ms/step - loss: 0.3295 - binary_crossentropy: 0.5489 - jaccard_coef_loss: 1.7015 - jaccard_coef: 0.6717 - jaccard_coef_thresholded: 0.6755 - accuracy: 0.0055 - auc: 0.8998 - val_loss: 0.6549 - val_binary_crossentropy: 0.4177 - val_jaccard_coef_loss: 5.0160 - val_jaccard_coef: 0.3317 - val_jaccard_coef_thresholded: 0.4054 - val_accuracy: 5.9605e-08 - val_auc: 0.8137\n",
      "Epoch 16/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.2973 - binary_crossentropy: 0.4544 - jaccard_coef_loss: 1.5142 - jaccard_coef: 0.7026 - jaccard_coef_thresholded: 0.7063 - accuracy: 0.0059 - auc: 0.9191 - val_loss: 0.6206 - val_binary_crossentropy: 0.3370 - val_jaccard_coef_loss: 4.9603 - val_jaccard_coef: 0.3693 - val_jaccard_coef_thresholded: 0.4537 - val_accuracy: 9.2983e-06 - val_auc: 0.8450\n",
      "Epoch 17/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.2977 - binary_crossentropy: 0.4638 - jaccard_coef_loss: 1.5059 - jaccard_coef: 0.7012 - jaccard_coef_thresholded: 0.7046 - accuracy: 0.0047 - auc: 0.9177 - val_loss: 0.6147 - val_binary_crossentropy: 0.4765 - val_jaccard_coef_loss: 5.1080 - val_jaccard_coef: 0.3755 - val_jaccard_coef_thresholded: 0.4461 - val_accuracy: 2.2221e-04 - val_auc: 0.8361\n",
      "Epoch 18/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.2884 - binary_crossentropy: 0.4694 - jaccard_coef_loss: 1.4777 - jaccard_coef: 0.7110 - jaccard_coef_thresholded: 0.7140 - accuracy: 0.0052 - auc: 0.9164 - val_loss: 0.5773 - val_binary_crossentropy: 0.3914 - val_jaccard_coef_loss: 4.3895 - val_jaccard_coef: 0.4144 - val_jaccard_coef_thresholded: 0.4551 - val_accuracy: 5.3889e-04 - val_auc: 0.8838\n",
      "Epoch 19/500\n",
      "46/46 [==============================] - 28s 607ms/step - loss: 0.2854 - binary_crossentropy: 0.4666 - jaccard_coef_loss: 1.4465 - jaccard_coef: 0.7148 - jaccard_coef_thresholded: 0.7181 - accuracy: 0.0055 - auc: 0.9186 - val_loss: 0.5787 - val_binary_crossentropy: 0.5679 - val_jaccard_coef_loss: 5.0137 - val_jaccard_coef: 0.4131 - val_jaccard_coef_thresholded: 0.4270 - val_accuracy: 0.0017 - val_auc: 0.8514\n",
      "Epoch 20/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.2688 - binary_crossentropy: 0.4484 - jaccard_coef_loss: 1.3561 - jaccard_coef: 0.7304 - jaccard_coef_thresholded: 0.7341 - accuracy: 0.0067 - auc: 0.9213 - val_loss: 0.5768 - val_binary_crossentropy: 0.6237 - val_jaccard_coef_loss: 5.0362 - val_jaccard_coef: 0.4155 - val_jaccard_coef_thresholded: 0.4285 - val_accuracy: 8.2642e-04 - val_auc: 0.8328\n",
      "Epoch 21/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.2625 - binary_crossentropy: 0.4230 - jaccard_coef_loss: 1.3106 - jaccard_coef: 0.7374 - jaccard_coef_thresholded: 0.7403 - accuracy: 0.0063 - auc: 0.9283 - val_loss: 0.5764 - val_binary_crossentropy: 0.9679 - val_jaccard_coef_loss: 5.7151 - val_jaccard_coef: 0.4158 - val_jaccard_coef_thresholded: 0.4235 - val_accuracy: 0.0077 - val_auc: 0.8709\n",
      "Epoch 22/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.2721 - binary_crossentropy: 0.4800 - jaccard_coef_loss: 1.3788 - jaccard_coef: 0.7270 - jaccard_coef_thresholded: 0.7297 - accuracy: 0.0114 - auc: 0.9161 - val_loss: 0.6116 - val_binary_crossentropy: 0.8143 - val_jaccard_coef_loss: 5.4291 - val_jaccard_coef: 0.3876 - val_jaccard_coef_thresholded: 0.3992 - val_accuracy: 0.0014 - val_auc: 0.8059\n",
      "Epoch 23/500\n",
      "46/46 [==============================] - 27s 598ms/step - loss: 0.2781 - binary_crossentropy: 0.4686 - jaccard_coef_loss: 1.3883 - jaccard_coef: 0.7222 - jaccard_coef_thresholded: 0.7252 - accuracy: 0.0104 - auc: 0.9158 - val_loss: 0.6122 - val_binary_crossentropy: 0.8053 - val_jaccard_coef_loss: 5.2708 - val_jaccard_coef: 0.3848 - val_jaccard_coef_thresholded: 0.3951 - val_accuracy: 0.0015 - val_auc: 0.8258\n",
      "Epoch 24/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.2398 - binary_crossentropy: 0.3765 - jaccard_coef_loss: 1.2009 - jaccard_coef: 0.7613 - jaccard_coef_thresholded: 0.7641 - accuracy: 0.0067 - auc: 0.9384 - val_loss: 0.5889 - val_binary_crossentropy: 0.6803 - val_jaccard_coef_loss: 5.2671 - val_jaccard_coef: 0.4030 - val_jaccard_coef_thresholded: 0.4248 - val_accuracy: 8.6927e-04 - val_auc: 0.8406\n",
      "Epoch 25/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.2567 - binary_crossentropy: 0.4239 - jaccard_coef_loss: 1.2878 - jaccard_coef: 0.7423 - jaccard_coef_thresholded: 0.7452 - accuracy: 0.0079 - auc: 0.9293 - val_loss: 0.5441 - val_binary_crossentropy: 1.1730 - val_jaccard_coef_loss: 4.7397 - val_jaccard_coef: 0.4429 - val_jaccard_coef_thresholded: 0.4556 - val_accuracy: 0.0088 - val_auc: 0.8611\n",
      "Epoch 26/500\n",
      "46/46 [==============================] - 27s 599ms/step - loss: 0.2595 - binary_crossentropy: 0.4567 - jaccard_coef_loss: 1.3075 - jaccard_coef: 0.7420 - jaccard_coef_thresholded: 0.7447 - accuracy: 0.0111 - auc: 0.9244 - val_loss: 0.6817 - val_binary_crossentropy: 0.8300 - val_jaccard_coef_loss: 5.3458 - val_jaccard_coef: 0.3142 - val_jaccard_coef_thresholded: 0.3645 - val_accuracy: 5.2780e-04 - val_auc: 0.7504\n",
      "Epoch 27/500\n",
      "46/46 [==============================] - 28s 600ms/step - loss: 0.2241 - binary_crossentropy: 0.3721 - jaccard_coef_loss: 1.1106 - jaccard_coef: 0.7750 - jaccard_coef_thresholded: 0.7778 - accuracy: 0.0098 - auc: 0.9394 - val_loss: 0.5540 - val_binary_crossentropy: 0.9068 - val_jaccard_coef_loss: 5.1460 - val_jaccard_coef: 0.4401 - val_jaccard_coef_thresholded: 0.4498 - val_accuracy: 0.0059 - val_auc: 0.8166\n",
      "Epoch 28/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.2541 - binary_crossentropy: 0.4554 - jaccard_coef_loss: 1.2741 - jaccard_coef: 0.7462 - jaccard_coef_thresholded: 0.7488 - accuracy: 0.0106 - auc: 0.9257 - val_loss: 0.6586 - val_binary_crossentropy: 0.7624 - val_jaccard_coef_loss: 4.7030 - val_jaccard_coef: 0.3271 - val_jaccard_coef_thresholded: 0.3996 - val_accuracy: 6.0916e-04 - val_auc: 0.7602\n",
      "Epoch 29/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.2739 - binary_crossentropy: 0.5501 - jaccard_coef_loss: 1.4456 - jaccard_coef: 0.7253 - jaccard_coef_thresholded: 0.7281 - accuracy: 0.0101 - auc: 0.9233 - val_loss: 0.5856 - val_binary_crossentropy: 0.5770 - val_jaccard_coef_loss: 4.3258 - val_jaccard_coef: 0.4067 - val_jaccard_coef_thresholded: 0.4342 - val_accuracy: 8.1831e-04 - val_auc: 0.8223\n",
      "Epoch 30/500\n",
      "46/46 [==============================] - 28s 600ms/step - loss: 0.2628 - binary_crossentropy: 0.4321 - jaccard_coef_loss: 1.3113 - jaccard_coef: 0.7362 - jaccard_coef_thresholded: 0.7392 - accuracy: 0.0131 - auc: 0.9275 - val_loss: 0.5724 - val_binary_crossentropy: 0.5326 - val_jaccard_coef_loss: 4.4435 - val_jaccard_coef: 0.4202 - val_jaccard_coef_thresholded: 0.4576 - val_accuracy: 0.0027 - val_auc: 0.8443\n",
      "Epoch 31/500\n",
      "46/46 [==============================] - 28s 602ms/step - loss: 0.2247 - binary_crossentropy: 0.3713 - jaccard_coef_loss: 1.0990 - jaccard_coef: 0.7752 - jaccard_coef_thresholded: 0.7778 - accuracy: 0.0152 - auc: 0.9405 - val_loss: 0.5721 - val_binary_crossentropy: 0.4248 - val_jaccard_coef_loss: 4.1547 - val_jaccard_coef: 0.4198 - val_jaccard_coef_thresholded: 0.4604 - val_accuracy: 0.0052 - val_auc: 0.8414\n",
      "Epoch 32/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.2228 - binary_crossentropy: 0.3751 - jaccard_coef_loss: 1.0920 - jaccard_coef: 0.7768 - jaccard_coef_thresholded: 0.7794 - accuracy: 0.0152 - auc: 0.9391 - val_loss: 0.5479 - val_binary_crossentropy: 0.6750 - val_jaccard_coef_loss: 5.0231 - val_jaccard_coef: 0.4484 - val_jaccard_coef_thresholded: 0.4535 - val_accuracy: 0.0100 - val_auc: 0.8665\n",
      "Epoch 33/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.2311 - binary_crossentropy: 0.4099 - jaccard_coef_loss: 1.1213 - jaccard_coef: 0.7672 - jaccard_coef_thresholded: 0.7700 - accuracy: 0.0177 - auc: 0.9332 - val_loss: 0.5370 - val_binary_crossentropy: 0.3899 - val_jaccard_coef_loss: 4.1279 - val_jaccard_coef: 0.4554 - val_jaccard_coef_thresholded: 0.4677 - val_accuracy: 0.0071 - val_auc: 0.8660\n",
      "Epoch 34/500\n",
      "46/46 [==============================] - 28s 600ms/step - loss: 0.2248 - binary_crossentropy: 0.3517 - jaccard_coef_loss: 1.0727 - jaccard_coef: 0.7742 - jaccard_coef_thresholded: 0.7769 - accuracy: 0.0145 - auc: 0.9432 - val_loss: 0.6022 - val_binary_crossentropy: 0.3801 - val_jaccard_coef_loss: 3.8308 - val_jaccard_coef: 0.3834 - val_jaccard_coef_thresholded: 0.4358 - val_accuracy: 0.0028 - val_auc: 0.8263\n",
      "Epoch 35/500\n",
      "46/46 [==============================] - 28s 602ms/step - loss: 0.2243 - binary_crossentropy: 0.3630 - jaccard_coef_loss: 1.0921 - jaccard_coef: 0.7741 - jaccard_coef_thresholded: 0.7771 - accuracy: 0.0205 - auc: 0.9418 - val_loss: 0.5567 - val_binary_crossentropy: 0.3697 - val_jaccard_coef_loss: 3.6039 - val_jaccard_coef: 0.4361 - val_jaccard_coef_thresholded: 0.4612 - val_accuracy: 0.0087 - val_auc: 0.8350\n",
      "Epoch 36/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.2196 - binary_crossentropy: 0.3607 - jaccard_coef_loss: 1.0705 - jaccard_coef: 0.7793 - jaccard_coef_thresholded: 0.7817 - accuracy: 0.0238 - auc: 0.9425 - val_loss: 0.6376 - val_binary_crossentropy: 0.5667 - val_jaccard_coef_loss: 4.1030 - val_jaccard_coef: 0.3493 - val_jaccard_coef_thresholded: 0.3993 - val_accuracy: 0.0033 - val_auc: 0.7816\n",
      "Epoch 37/500\n",
      "46/46 [==============================] - 28s 607ms/step - loss: 0.2143 - binary_crossentropy: 0.3585 - jaccard_coef_loss: 1.0296 - jaccard_coef: 0.7862 - jaccard_coef_thresholded: 0.7887 - accuracy: 0.0250 - auc: 0.9403 - val_loss: 0.5220 - val_binary_crossentropy: 0.4141 - val_jaccard_coef_loss: 3.8205 - val_jaccard_coef: 0.4632 - val_jaccard_coef_thresholded: 0.4824 - val_accuracy: 0.0125 - val_auc: 0.8623\n",
      "Epoch 38/500\n",
      "46/46 [==============================] - 28s 612ms/step - loss: 0.2130 - binary_crossentropy: 0.3527 - jaccard_coef_loss: 1.0349 - jaccard_coef: 0.7857 - jaccard_coef_thresholded: 0.7884 - accuracy: 0.0217 - auc: 0.9429 - val_loss: 0.5152 - val_binary_crossentropy: 0.7259 - val_jaccard_coef_loss: 4.3014 - val_jaccard_coef: 0.4720 - val_jaccard_coef_thresholded: 0.4811 - val_accuracy: 0.0188 - val_auc: 0.8551\n",
      "Epoch 39/500\n",
      "46/46 [==============================] - 28s 606ms/step - loss: 0.2154 - binary_crossentropy: 0.3703 - jaccard_coef_loss: 1.0434 - jaccard_coef: 0.7843 - jaccard_coef_thresholded: 0.7865 - accuracy: 0.0225 - auc: 0.9414 - val_loss: 0.4998 - val_binary_crossentropy: 0.3294 - val_jaccard_coef_loss: 3.6069 - val_jaccard_coef: 0.4891 - val_jaccard_coef_thresholded: 0.5023 - val_accuracy: 0.0055 - val_auc: 0.8787\n",
      "Epoch 40/500\n",
      "46/46 [==============================] - 28s 611ms/step - loss: 0.2144 - binary_crossentropy: 0.3544 - jaccard_coef_loss: 1.0299 - jaccard_coef: 0.7837 - jaccard_coef_thresholded: 0.7860 - accuracy: 0.0219 - auc: 0.9424 - val_loss: 0.6132 - val_binary_crossentropy: 0.4404 - val_jaccard_coef_loss: 3.9798 - val_jaccard_coef: 0.3766 - val_jaccard_coef_thresholded: 0.3948 - val_accuracy: 0.0011 - val_auc: 0.8042\n",
      "Epoch 41/500\n",
      "46/46 [==============================] - 28s 612ms/step - loss: 0.2071 - binary_crossentropy: 0.3514 - jaccard_coef_loss: 1.0080 - jaccard_coef: 0.7918 - jaccard_coef_thresholded: 0.7943 - accuracy: 0.0201 - auc: 0.9460 - val_loss: 0.5844 - val_binary_crossentropy: 1.3309 - val_jaccard_coef_loss: 4.7273 - val_jaccard_coef: 0.4056 - val_jaccard_coef_thresholded: 0.4251 - val_accuracy: 0.0060 - val_auc: 0.7899\n",
      "Epoch 42/500\n",
      "46/46 [==============================] - 28s 619ms/step - loss: 0.2588 - binary_crossentropy: 0.6401 - jaccard_coef_loss: 1.3874 - jaccard_coef: 0.7393 - jaccard_coef_thresholded: 0.7417 - accuracy: 0.0256 - auc: 0.9371 - val_loss: 0.5445 - val_binary_crossentropy: 1.1799 - val_jaccard_coef_loss: 5.0936 - val_jaccard_coef: 0.4495 - val_jaccard_coef_thresholded: 0.4600 - val_accuracy: 0.0135 - val_auc: 0.8393\n",
      "Epoch 43/500\n",
      "46/46 [==============================] - 29s 623ms/step - loss: 0.2243 - binary_crossentropy: 0.3756 - jaccard_coef_loss: 1.0781 - jaccard_coef: 0.7741 - jaccard_coef_thresholded: 0.7764 - accuracy: 0.0221 - auc: 0.9448 - val_loss: 0.5586 - val_binary_crossentropy: 1.0695 - val_jaccard_coef_loss: 5.6286 - val_jaccard_coef: 0.4426 - val_jaccard_coef_thresholded: 0.4484 - val_accuracy: 0.0146 - val_auc: 0.8791\n",
      "Epoch 44/500\n",
      "46/46 [==============================] - 28s 612ms/step - loss: 0.1854 - binary_crossentropy: 0.3202 - jaccard_coef_loss: 0.8975 - jaccard_coef: 0.8148 - jaccard_coef_thresholded: 0.8167 - accuracy: 0.0289 - auc: 0.9493 - val_loss: 0.5131 - val_binary_crossentropy: 0.8453 - val_jaccard_coef_loss: 4.8156 - val_jaccard_coef: 0.4740 - val_jaccard_coef_thresholded: 0.4771 - val_accuracy: 0.0068 - val_auc: 0.8421\n",
      "Epoch 45/500\n",
      "46/46 [==============================] - 28s 614ms/step - loss: 0.1896 - binary_crossentropy: 0.3107 - jaccard_coef_loss: 0.8956 - jaccard_coef: 0.8090 - jaccard_coef_thresholded: 0.8110 - accuracy: 0.0348 - auc: 0.9509 - val_loss: 0.5390 - val_binary_crossentropy: 0.7469 - val_jaccard_coef_loss: 4.6151 - val_jaccard_coef: 0.4501 - val_jaccard_coef_thresholded: 0.4604 - val_accuracy: 0.0047 - val_auc: 0.8275\n",
      "Epoch 46/500\n",
      "46/46 [==============================] - 28s 614ms/step - loss: 0.1915 - binary_crossentropy: 0.3224 - jaccard_coef_loss: 0.9192 - jaccard_coef: 0.8077 - jaccard_coef_thresholded: 0.8097 - accuracy: 0.0294 - auc: 0.9532 - val_loss: 0.5521 - val_binary_crossentropy: 0.6757 - val_jaccard_coef_loss: 4.3171 - val_jaccard_coef: 0.4358 - val_jaccard_coef_thresholded: 0.4482 - val_accuracy: 0.0111 - val_auc: 0.8130\n",
      "Epoch 47/500\n",
      "46/46 [==============================] - 28s 606ms/step - loss: 0.2022 - binary_crossentropy: 0.3808 - jaccard_coef_loss: 0.9784 - jaccard_coef: 0.7966 - jaccard_coef_thresholded: 0.7990 - accuracy: 0.0285 - auc: 0.9422 - val_loss: 0.5714 - val_binary_crossentropy: 0.4604 - val_jaccard_coef_loss: 3.9583 - val_jaccard_coef: 0.4231 - val_jaccard_coef_thresholded: 0.4349 - val_accuracy: 0.0045 - val_auc: 0.8293\n",
      "Epoch 48/500\n",
      "46/46 [==============================] - 28s 608ms/step - loss: 0.1892 - binary_crossentropy: 0.3255 - jaccard_coef_loss: 0.9067 - jaccard_coef: 0.8086 - jaccard_coef_thresholded: 0.8106 - accuracy: 0.0282 - auc: 0.9518 - val_loss: 0.7573 - val_binary_crossentropy: 0.6852 - val_jaccard_coef_loss: 4.6553 - val_jaccard_coef: 0.2318 - val_jaccard_coef_thresholded: 0.2800 - val_accuracy: 1.8948e-04 - val_auc: 0.7144\n",
      "Epoch 49/500\n",
      "46/46 [==============================] - 28s 605ms/step - loss: 0.2104 - binary_crossentropy: 0.3750 - jaccard_coef_loss: 1.0137 - jaccard_coef: 0.7900 - jaccard_coef_thresholded: 0.7922 - accuracy: 0.0265 - auc: 0.9473 - val_loss: 0.5166 - val_binary_crossentropy: 0.4748 - val_jaccard_coef_loss: 4.1468 - val_jaccard_coef: 0.4755 - val_jaccard_coef_thresholded: 0.4854 - val_accuracy: 0.0060 - val_auc: 0.8741\n",
      "Epoch 50/500\n",
      "46/46 [==============================] - 28s 603ms/step - loss: 0.1857 - binary_crossentropy: 0.3338 - jaccard_coef_loss: 0.8953 - jaccard_coef: 0.8132 - jaccard_coef_thresholded: 0.8155 - accuracy: 0.0291 - auc: 0.9498 - val_loss: 0.5882 - val_binary_crossentropy: 0.6475 - val_jaccard_coef_loss: 4.2525 - val_jaccard_coef: 0.4018 - val_jaccard_coef_thresholded: 0.4571 - val_accuracy: 0.0027 - val_auc: 0.7938\n",
      "Epoch 51/500\n",
      "46/46 [==============================] - 28s 608ms/step - loss: 0.1830 - binary_crossentropy: 0.3156 - jaccard_coef_loss: 0.8753 - jaccard_coef: 0.8165 - jaccard_coef_thresholded: 0.8184 - accuracy: 0.0411 - auc: 0.9517 - val_loss: 0.4983 - val_binary_crossentropy: 0.4683 - val_jaccard_coef_loss: 3.9281 - val_jaccard_coef: 0.4916 - val_jaccard_coef_thresholded: 0.4956 - val_accuracy: 0.0095 - val_auc: 0.8719\n",
      "Epoch 52/500\n",
      "46/46 [==============================] - 28s 602ms/step - loss: 0.1733 - binary_crossentropy: 0.3056 - jaccard_coef_loss: 0.8298 - jaccard_coef: 0.8257 - jaccard_coef_thresholded: 0.8274 - accuracy: 0.0382 - auc: 0.9561 - val_loss: 0.4828 - val_binary_crossentropy: 0.3680 - val_jaccard_coef_loss: 3.6923 - val_jaccard_coef: 0.4999 - val_jaccard_coef_thresholded: 0.5075 - val_accuracy: 0.0106 - val_auc: 0.8892\n",
      "Epoch 53/500\n",
      "46/46 [==============================] - 28s 605ms/step - loss: 0.1830 - binary_crossentropy: 0.3083 - jaccard_coef_loss: 0.8600 - jaccard_coef: 0.8153 - jaccard_coef_thresholded: 0.8174 - accuracy: 0.0394 - auc: 0.9549 - val_loss: 0.5446 - val_binary_crossentropy: 0.5021 - val_jaccard_coef_loss: 4.1985 - val_jaccard_coef: 0.4413 - val_jaccard_coef_thresholded: 0.4519 - val_accuracy: 0.0071 - val_auc: 0.8511\n",
      "Epoch 54/500\n",
      "46/46 [==============================] - 28s 602ms/step - loss: 0.1744 - binary_crossentropy: 0.3011 - jaccard_coef_loss: 0.8175 - jaccard_coef: 0.8243 - jaccard_coef_thresholded: 0.8264 - accuracy: 0.0375 - auc: 0.9584 - val_loss: 0.6002 - val_binary_crossentropy: 0.5051 - val_jaccard_coef_loss: 4.3308 - val_jaccard_coef: 0.3896 - val_jaccard_coef_thresholded: 0.3998 - val_accuracy: 0.0025 - val_auc: 0.7977\n",
      "Epoch 55/500\n",
      "46/46 [==============================] - 28s 608ms/step - loss: 0.1869 - binary_crossentropy: 0.3414 - jaccard_coef_loss: 0.9137 - jaccard_coef: 0.8131 - jaccard_coef_thresholded: 0.8149 - accuracy: 0.0420 - auc: 0.9495 - val_loss: 0.6152 - val_binary_crossentropy: 0.7424 - val_jaccard_coef_loss: 4.7771 - val_jaccard_coef: 0.3794 - val_jaccard_coef_thresholded: 0.3891 - val_accuracy: 0.0248 - val_auc: 0.8094\n",
      "Epoch 56/500\n",
      "46/46 [==============================] - 28s 607ms/step - loss: 0.1773 - binary_crossentropy: 0.3082 - jaccard_coef_loss: 0.8521 - jaccard_coef: 0.8219 - jaccard_coef_thresholded: 0.8238 - accuracy: 0.0545 - auc: 0.9570 - val_loss: 0.5590 - val_binary_crossentropy: 0.7474 - val_jaccard_coef_loss: 4.7654 - val_jaccard_coef: 0.4386 - val_jaccard_coef_thresholded: 0.4477 - val_accuracy: 0.0367 - val_auc: 0.8534\n",
      "Epoch 57/500\n",
      "46/46 [==============================] - 28s 609ms/step - loss: 0.2020 - binary_crossentropy: 0.3370 - jaccard_coef_loss: 0.9402 - jaccard_coef: 0.7968 - jaccard_coef_thresholded: 0.7986 - accuracy: 0.0402 - auc: 0.9515 - val_loss: 0.5245 - val_binary_crossentropy: 0.5520 - val_jaccard_coef_loss: 4.4497 - val_jaccard_coef: 0.4712 - val_jaccard_coef_thresholded: 0.4758 - val_accuracy: 0.0215 - val_auc: 0.8862\n",
      "Epoch 58/500\n",
      "46/46 [==============================] - 28s 612ms/step - loss: 0.1980 - binary_crossentropy: 0.3371 - jaccard_coef_loss: 0.9165 - jaccard_coef: 0.8005 - jaccard_coef_thresholded: 0.8026 - accuracy: 0.0377 - auc: 0.9523 - val_loss: 0.5893 - val_binary_crossentropy: 0.6973 - val_jaccard_coef_loss: 4.6855 - val_jaccard_coef: 0.4109 - val_jaccard_coef_thresholded: 0.4198 - val_accuracy: 0.0174 - val_auc: 0.8159\n",
      "Epoch 59/500\n",
      "46/46 [==============================] - 28s 606ms/step - loss: 0.1788 - binary_crossentropy: 0.3099 - jaccard_coef_loss: 0.8318 - jaccard_coef: 0.8212 - jaccard_coef_thresholded: 0.8231 - accuracy: 0.0469 - auc: 0.9552 - val_loss: 0.5770 - val_binary_crossentropy: 0.6038 - val_jaccard_coef_loss: 4.5507 - val_jaccard_coef: 0.4156 - val_jaccard_coef_thresholded: 0.4248 - val_accuracy: 0.0096 - val_auc: 0.8138\n",
      "Epoch 60/500\n",
      "46/46 [==============================] - 28s 605ms/step - loss: 0.1699 - binary_crossentropy: 0.2982 - jaccard_coef_loss: 0.7979 - jaccard_coef: 0.8280 - jaccard_coef_thresholded: 0.8298 - accuracy: 0.0440 - auc: 0.9588 - val_loss: 0.5347 - val_binary_crossentropy: 0.4081 - val_jaccard_coef_loss: 3.7882 - val_jaccard_coef: 0.4614 - val_jaccard_coef_thresholded: 0.4710 - val_accuracy: 0.0215 - val_auc: 0.8552\n",
      "Epoch 61/500\n",
      "46/46 [==============================] - 28s 611ms/step - loss: 0.1701 - binary_crossentropy: 0.2972 - jaccard_coef_loss: 0.7882 - jaccard_coef: 0.8282 - jaccard_coef_thresholded: 0.8301 - accuracy: 0.0402 - auc: 0.9587 - val_loss: 0.5183 - val_binary_crossentropy: 0.4062 - val_jaccard_coef_loss: 3.7990 - val_jaccard_coef: 0.4757 - val_jaccard_coef_thresholded: 0.4866 - val_accuracy: 0.0274 - val_auc: 0.8711\n",
      "Epoch 62/500\n",
      "46/46 [==============================] - 28s 613ms/step - loss: 0.1751 - binary_crossentropy: 0.2962 - jaccard_coef_loss: 0.8147 - jaccard_coef: 0.8243 - jaccard_coef_thresholded: 0.8259 - accuracy: 0.0496 - auc: 0.9585 - val_loss: 0.4899 - val_binary_crossentropy: 0.4924 - val_jaccard_coef_loss: 4.0348 - val_jaccard_coef: 0.4994 - val_jaccard_coef_thresholded: 0.5096 - val_accuracy: 0.0264 - val_auc: 0.8849\n",
      "Epoch 63/500\n",
      "46/46 [==============================] - 28s 612ms/step - loss: 0.1670 - binary_crossentropy: 0.2734 - jaccard_coef_loss: 0.7715 - jaccard_coef: 0.8327 - jaccard_coef_thresholded: 0.8346 - accuracy: 0.0468 - auc: 0.9614 - val_loss: 0.4982 - val_binary_crossentropy: 0.3575 - val_jaccard_coef_loss: 3.4105 - val_jaccard_coef: 0.4931 - val_jaccard_coef_thresholded: 0.5034 - val_accuracy: 0.0176 - val_auc: 0.8612\n",
      "Epoch 64/500\n",
      "46/46 [==============================] - 28s 615ms/step - loss: 0.1805 - binary_crossentropy: 0.3187 - jaccard_coef_loss: 0.8409 - jaccard_coef: 0.8187 - jaccard_coef_thresholded: 0.8201 - accuracy: 0.0552 - auc: 0.9559 - val_loss: 0.5236 - val_binary_crossentropy: 0.4009 - val_jaccard_coef_loss: 3.7203 - val_jaccard_coef: 0.4712 - val_jaccard_coef_thresholded: 0.4815 - val_accuracy: 0.0129 - val_auc: 0.8569\n",
      "Epoch 65/500\n",
      "46/46 [==============================] - 28s 606ms/step - loss: 0.1787 - binary_crossentropy: 0.3046 - jaccard_coef_loss: 0.8247 - jaccard_coef: 0.8187 - jaccard_coef_thresholded: 0.8207 - accuracy: 0.0433 - auc: 0.9564 - val_loss: 0.5660 - val_binary_crossentropy: 0.4962 - val_jaccard_coef_loss: 3.9940 - val_jaccard_coef: 0.4301 - val_jaccard_coef_thresholded: 0.4393 - val_accuracy: 0.0149 - val_auc: 0.8255\n",
      "Epoch 66/500\n",
      "46/46 [==============================] - 28s 605ms/step - loss: 0.1620 - binary_crossentropy: 0.2818 - jaccard_coef_loss: 0.7451 - jaccard_coef: 0.8384 - jaccard_coef_thresholded: 0.8399 - accuracy: 0.0443 - auc: 0.9621 - val_loss: 0.5367 - val_binary_crossentropy: 0.4064 - val_jaccard_coef_loss: 3.6753 - val_jaccard_coef: 0.4636 - val_jaccard_coef_thresholded: 0.4736 - val_accuracy: 0.0179 - val_auc: 0.8539\n",
      "Epoch 67/500\n",
      "46/46 [==============================] - 28s 609ms/step - loss: 0.1565 - binary_crossentropy: 0.3040 - jaccard_coef_loss: 0.7398 - jaccard_coef: 0.8425 - jaccard_coef_thresholded: 0.8437 - accuracy: 0.0593 - auc: 0.9604 - val_loss: 0.5363 - val_binary_crossentropy: 0.4350 - val_jaccard_coef_loss: 3.7414 - val_jaccard_coef: 0.4579 - val_jaccard_coef_thresholded: 0.4676 - val_accuracy: 0.0145 - val_auc: 0.8454\n",
      "Epoch 68/500\n",
      "46/46 [==============================] - 28s 607ms/step - loss: 0.1661 - binary_crossentropy: 0.2800 - jaccard_coef_loss: 0.7561 - jaccard_coef: 0.8336 - jaccard_coef_thresholded: 0.8353 - accuracy: 0.0484 - auc: 0.9603 - val_loss: 0.5248 - val_binary_crossentropy: 0.4359 - val_jaccard_coef_loss: 3.9331 - val_jaccard_coef: 0.4717 - val_jaccard_coef_thresholded: 0.4785 - val_accuracy: 0.0126 - val_auc: 0.8612\n",
      "Epoch 69/500\n",
      "46/46 [==============================] - 28s 609ms/step - loss: 0.1497 - binary_crossentropy: 0.2586 - jaccard_coef_loss: 0.7049 - jaccard_coef: 0.8498 - jaccard_coef_thresholded: 0.8515 - accuracy: 0.0529 - auc: 0.9639 - val_loss: 0.5416 - val_binary_crossentropy: 0.5164 - val_jaccard_coef_loss: 3.9456 - val_jaccard_coef: 0.4517 - val_jaccard_coef_thresholded: 0.4607 - val_accuracy: 0.0154 - val_auc: 0.8351\n",
      "Epoch 70/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.1539 - binary_crossentropy: 0.2698 - jaccard_coef_loss: 0.7047 - jaccard_coef: 0.8461 - jaccard_coef_thresholded: 0.8476 - accuracy: 0.0563 - auc: 0.9631 - val_loss: 0.5241 - val_binary_crossentropy: 0.5227 - val_jaccard_coef_loss: 4.1627 - val_jaccard_coef: 0.4711 - val_jaccard_coef_thresholded: 0.4800 - val_accuracy: 0.0224 - val_auc: 0.8699\n",
      "Epoch 71/500\n",
      "46/46 [==============================] - 28s 603ms/step - loss: 0.1478 - binary_crossentropy: 0.2632 - jaccard_coef_loss: 0.6758 - jaccard_coef: 0.8511 - jaccard_coef_thresholded: 0.8525 - accuracy: 0.0536 - auc: 0.9638 - val_loss: 0.5524 - val_binary_crossentropy: 0.4019 - val_jaccard_coef_loss: 3.5902 - val_jaccard_coef: 0.4338 - val_jaccard_coef_thresholded: 0.4445 - val_accuracy: 0.0087 - val_auc: 0.8344\n",
      "Epoch 72/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.1419 - binary_crossentropy: 0.2586 - jaccard_coef_loss: 0.6585 - jaccard_coef: 0.8573 - jaccard_coef_thresholded: 0.8585 - accuracy: 0.0577 - auc: 0.9663 - val_loss: 0.5077 - val_binary_crossentropy: 0.3830 - val_jaccard_coef_loss: 3.4453 - val_jaccard_coef: 0.4822 - val_jaccard_coef_thresholded: 0.4911 - val_accuracy: 0.0159 - val_auc: 0.8579\n",
      "Epoch 73/500\n",
      "46/46 [==============================] - 28s 603ms/step - loss: 0.1445 - binary_crossentropy: 0.2568 - jaccard_coef_loss: 0.6689 - jaccard_coef: 0.8541 - jaccard_coef_thresholded: 0.8557 - accuracy: 0.0576 - auc: 0.9650 - val_loss: 0.5934 - val_binary_crossentropy: 0.4730 - val_jaccard_coef_loss: 3.8075 - val_jaccard_coef: 0.3952 - val_jaccard_coef_thresholded: 0.4116 - val_accuracy: 0.0082 - val_auc: 0.8119\n",
      "Epoch 74/500\n",
      "46/46 [==============================] - 28s 606ms/step - loss: 0.1493 - binary_crossentropy: 0.2629 - jaccard_coef_loss: 0.6907 - jaccard_coef: 0.8496 - jaccard_coef_thresholded: 0.8511 - accuracy: 0.0561 - auc: 0.9658 - val_loss: 0.5456 - val_binary_crossentropy: 0.4893 - val_jaccard_coef_loss: 3.9846 - val_jaccard_coef: 0.4505 - val_jaccard_coef_thresholded: 0.4594 - val_accuracy: 0.0323 - val_auc: 0.8603\n",
      "Epoch 75/500\n",
      "46/46 [==============================] - 28s 606ms/step - loss: 0.1451 - binary_crossentropy: 0.2605 - jaccard_coef_loss: 0.6656 - jaccard_coef: 0.8538 - jaccard_coef_thresholded: 0.8552 - accuracy: 0.0563 - auc: 0.9643 - val_loss: 0.5621 - val_binary_crossentropy: 0.6482 - val_jaccard_coef_loss: 4.0660 - val_jaccard_coef: 0.4243 - val_jaccard_coef_thresholded: 0.4346 - val_accuracy: 0.0128 - val_auc: 0.8102\n",
      "Epoch 76/500\n",
      "46/46 [==============================] - 28s 603ms/step - loss: 0.1519 - binary_crossentropy: 0.2799 - jaccard_coef_loss: 0.7067 - jaccard_coef: 0.8467 - jaccard_coef_thresholded: 0.8481 - accuracy: 0.0677 - auc: 0.9621 - val_loss: 0.4937 - val_binary_crossentropy: 0.4151 - val_jaccard_coef_loss: 3.6873 - val_jaccard_coef: 0.4960 - val_jaccard_coef_thresholded: 0.4979 - val_accuracy: 0.0181 - val_auc: 0.8624\n",
      "Epoch 77/500\n",
      "46/46 [==============================] - 27s 598ms/step - loss: 0.1472 - binary_crossentropy: 0.2706 - jaccard_coef_loss: 0.6861 - jaccard_coef: 0.8516 - jaccard_coef_thresholded: 0.8530 - accuracy: 0.0594 - auc: 0.9645 - val_loss: 0.4833 - val_binary_crossentropy: 0.3313 - val_jaccard_coef_loss: 3.4875 - val_jaccard_coef: 0.5122 - val_jaccard_coef_thresholded: 0.5156 - val_accuracy: 0.0223 - val_auc: 0.9032\n",
      "Epoch 78/500\n",
      "46/46 [==============================] - 28s 605ms/step - loss: 0.1465 - binary_crossentropy: 0.2648 - jaccard_coef_loss: 0.6777 - jaccard_coef: 0.8530 - jaccard_coef_thresholded: 0.8543 - accuracy: 0.0642 - auc: 0.9642 - val_loss: 0.5229 - val_binary_crossentropy: 0.5302 - val_jaccard_coef_loss: 3.8155 - val_jaccard_coef: 0.4676 - val_jaccard_coef_thresholded: 0.4772 - val_accuracy: 0.0201 - val_auc: 0.8476\n",
      "Epoch 79/500\n",
      "46/46 [==============================] - 28s 603ms/step - loss: 0.1479 - binary_crossentropy: 0.2711 - jaccard_coef_loss: 0.6921 - jaccard_coef: 0.8471 - jaccard_coef_thresholded: 0.8487 - accuracy: 0.0613 - auc: 0.9639 - val_loss: 0.4996 - val_binary_crossentropy: 0.4799 - val_jaccard_coef_loss: 4.0465 - val_jaccard_coef: 0.4948 - val_jaccard_coef_thresholded: 0.5025 - val_accuracy: 0.0193 - val_auc: 0.8915\n",
      "Epoch 80/500\n",
      "46/46 [==============================] - 28s 606ms/step - loss: 0.1363 - binary_crossentropy: 0.2481 - jaccard_coef_loss: 0.6305 - jaccard_coef: 0.8625 - jaccard_coef_thresholded: 0.8638 - accuracy: 0.0602 - auc: 0.9676 - val_loss: 0.4933 - val_binary_crossentropy: 0.5337 - val_jaccard_coef_loss: 3.8518 - val_jaccard_coef: 0.4988 - val_jaccard_coef_thresholded: 0.5014 - val_accuracy: 0.0210 - val_auc: 0.8653\n",
      "Epoch 81/500\n",
      "46/46 [==============================] - 28s 602ms/step - loss: 0.1350 - binary_crossentropy: 0.2395 - jaccard_coef_loss: 0.6281 - jaccard_coef: 0.8638 - jaccard_coef_thresholded: 0.8651 - accuracy: 0.0683 - auc: 0.9685 - val_loss: 0.5390 - val_binary_crossentropy: 0.4914 - val_jaccard_coef_loss: 3.4889 - val_jaccard_coef: 0.4524 - val_jaccard_coef_thresholded: 0.4683 - val_accuracy: 0.0191 - val_auc: 0.8368\n",
      "Epoch 82/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.1515 - binary_crossentropy: 0.2633 - jaccard_coef_loss: 0.6903 - jaccard_coef: 0.8481 - jaccard_coef_thresholded: 0.8495 - accuracy: 0.0655 - auc: 0.9642 - val_loss: 0.5388 - val_binary_crossentropy: 0.3556 - val_jaccard_coef_loss: 3.3565 - val_jaccard_coef: 0.4452 - val_jaccard_coef_thresholded: 0.4687 - val_accuracy: 0.0089 - val_auc: 0.8561\n",
      "Epoch 83/500\n",
      "46/46 [==============================] - 28s 600ms/step - loss: 0.1458 - binary_crossentropy: 0.2628 - jaccard_coef_loss: 0.6720 - jaccard_coef: 0.8527 - jaccard_coef_thresholded: 0.8539 - accuracy: 0.0692 - auc: 0.9659 - val_loss: 0.5543 - val_binary_crossentropy: 0.3950 - val_jaccard_coef_loss: 3.3931 - val_jaccard_coef: 0.4408 - val_jaccard_coef_thresholded: 0.4783 - val_accuracy: 0.0167 - val_auc: 0.8553\n",
      "Epoch 84/500\n",
      "46/46 [==============================] - 27s 599ms/step - loss: 0.1545 - binary_crossentropy: 0.2801 - jaccard_coef_loss: 0.7054 - jaccard_coef: 0.8439 - jaccard_coef_thresholded: 0.8450 - accuracy: 0.0704 - auc: 0.9630 - val_loss: 0.5509 - val_binary_crossentropy: 0.5935 - val_jaccard_coef_loss: 4.0560 - val_jaccard_coef: 0.4510 - val_jaccard_coef_thresholded: 0.4528 - val_accuracy: 0.0231 - val_auc: 0.8342\n",
      "Epoch 85/500\n",
      "46/46 [==============================] - 27s 595ms/step - loss: 0.1411 - binary_crossentropy: 0.2586 - jaccard_coef_loss: 0.6613 - jaccard_coef: 0.8578 - jaccard_coef_thresholded: 0.8589 - accuracy: 0.0710 - auc: 0.9654 - val_loss: 0.5606 - val_binary_crossentropy: 0.4432 - val_jaccard_coef_loss: 3.6340 - val_jaccard_coef: 0.4322 - val_jaccard_coef_thresholded: 0.4392 - val_accuracy: 0.0175 - val_auc: 0.8354\n",
      "Epoch 86/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.1338 - binary_crossentropy: 0.2351 - jaccard_coef_loss: 0.6173 - jaccard_coef: 0.8653 - jaccard_coef_thresholded: 0.8666 - accuracy: 0.0702 - auc: 0.9693 - val_loss: 0.5534 - val_binary_crossentropy: 0.8803 - val_jaccard_coef_loss: 4.7708 - val_jaccard_coef: 0.4434 - val_jaccard_coef_thresholded: 0.4473 - val_accuracy: 0.0326 - val_auc: 0.8466\n",
      "Epoch 87/500\n",
      "46/46 [==============================] - 28s 602ms/step - loss: 0.1366 - binary_crossentropy: 0.2472 - jaccard_coef_loss: 0.6275 - jaccard_coef: 0.8619 - jaccard_coef_thresholded: 0.8632 - accuracy: 0.0737 - auc: 0.9676 - val_loss: 0.6695 - val_binary_crossentropy: 0.6651 - val_jaccard_coef_loss: 3.9619 - val_jaccard_coef: 0.3209 - val_jaccard_coef_thresholded: 0.3440 - val_accuracy: 0.0040 - val_auc: 0.7467\n",
      "Epoch 88/500\n",
      "46/46 [==============================] - 28s 600ms/step - loss: 0.1390 - binary_crossentropy: 0.2476 - jaccard_coef_loss: 0.6358 - jaccard_coef: 0.8599 - jaccard_coef_thresholded: 0.8611 - accuracy: 0.0724 - auc: 0.9666 - val_loss: 0.5058 - val_binary_crossentropy: 0.3658 - val_jaccard_coef_loss: 3.4602 - val_jaccard_coef: 0.4906 - val_jaccard_coef_thresholded: 0.4950 - val_accuracy: 0.0231 - val_auc: 0.8745\n",
      "Epoch 89/500\n",
      "46/46 [==============================] - 27s 599ms/step - loss: 0.1286 - binary_crossentropy: 0.2236 - jaccard_coef_loss: 0.5898 - jaccard_coef: 0.8686 - jaccard_coef_thresholded: 0.8699 - accuracy: 0.0725 - auc: 0.9714 - val_loss: 0.5166 - val_binary_crossentropy: 0.3817 - val_jaccard_coef_loss: 3.5638 - val_jaccard_coef: 0.4731 - val_jaccard_coef_thresholded: 0.4782 - val_accuracy: 0.0213 - val_auc: 0.8632\n",
      "Epoch 90/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.1346 - binary_crossentropy: 0.2413 - jaccard_coef_loss: 0.6174 - jaccard_coef: 0.8644 - jaccard_coef_thresholded: 0.8657 - accuracy: 0.0697 - auc: 0.9689 - val_loss: 0.5152 - val_binary_crossentropy: 0.4173 - val_jaccard_coef_loss: 3.7652 - val_jaccard_coef: 0.4737 - val_jaccard_coef_thresholded: 0.4811 - val_accuracy: 0.0180 - val_auc: 0.8595\n",
      "Epoch 91/500\n",
      "46/46 [==============================] - 27s 600ms/step - loss: 0.1322 - binary_crossentropy: 0.2290 - jaccard_coef_loss: 0.6011 - jaccard_coef: 0.8668 - jaccard_coef_thresholded: 0.8681 - accuracy: 0.0621 - auc: 0.9702 - val_loss: 0.4903 - val_binary_crossentropy: 0.3752 - val_jaccard_coef_loss: 3.7370 - val_jaccard_coef: 0.5033 - val_jaccard_coef_thresholded: 0.5050 - val_accuracy: 0.0184 - val_auc: 0.8946\n",
      "Epoch 92/500\n",
      "46/46 [==============================] - 28s 603ms/step - loss: 0.1350 - binary_crossentropy: 0.2520 - jaccard_coef_loss: 0.6308 - jaccard_coef: 0.8640 - jaccard_coef_thresholded: 0.8651 - accuracy: 0.0765 - auc: 0.9665 - val_loss: 0.5135 - val_binary_crossentropy: 0.4542 - val_jaccard_coef_loss: 3.9155 - val_jaccard_coef: 0.4755 - val_jaccard_coef_thresholded: 0.4812 - val_accuracy: 0.0174 - val_auc: 0.8756\n",
      "Epoch 93/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.1341 - binary_crossentropy: 0.2384 - jaccard_coef_loss: 0.6100 - jaccard_coef: 0.8653 - jaccard_coef_thresholded: 0.8665 - accuracy: 0.0668 - auc: 0.9694 - val_loss: 0.4868 - val_binary_crossentropy: 0.3722 - val_jaccard_coef_loss: 3.4447 - val_jaccard_coef: 0.5039 - val_jaccard_coef_thresholded: 0.5096 - val_accuracy: 0.0191 - val_auc: 0.8703\n",
      "Epoch 94/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.1308 - binary_crossentropy: 0.2339 - jaccard_coef_loss: 0.5943 - jaccard_coef: 0.8676 - jaccard_coef_thresholded: 0.8688 - accuracy: 0.0656 - auc: 0.9703 - val_loss: 0.4894 - val_binary_crossentropy: 0.4199 - val_jaccard_coef_loss: 3.6024 - val_jaccard_coef: 0.5006 - val_jaccard_coef_thresholded: 0.5068 - val_accuracy: 0.0317 - val_auc: 0.8708\n",
      "Epoch 95/500\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.1189 - binary_crossentropy: 0.2148 - jaccard_coef_loss: 0.5481 - jaccard_coef: 0.8800 - jaccard_coef_thresholded: 0.8812 - accuracy: 0.0751 - auc: 0.9735 - val_loss: 0.4530 - val_binary_crossentropy: 0.2958 - val_jaccard_coef_loss: 3.2769 - val_jaccard_coef: 0.5420 - val_jaccard_coef_thresholded: 0.5456 - val_accuracy: 0.0310 - val_auc: 0.9137\n",
      "Epoch 96/500\n",
      "46/46 [==============================] - 28s 601ms/step - loss: 0.1295 - binary_crossentropy: 0.2335 - jaccard_coef_loss: 0.5930 - jaccard_coef: 0.8690 - jaccard_coef_thresholded: 0.8700 - accuracy: 0.0730 - auc: 0.9712 - val_loss: 0.4822 - val_binary_crossentropy: 0.3434 - val_jaccard_coef_loss: 3.4692 - val_jaccard_coef: 0.5072 - val_jaccard_coef_thresholded: 0.5129 - val_accuracy: 0.0245 - val_auc: 0.8963\n",
      "Epoch 97/500\n",
      "46/46 [==============================] - 27s 598ms/step - loss: 0.1250 - binary_crossentropy: 0.2378 - jaccard_coef_loss: 0.5813 - jaccard_coef: 0.8741 - jaccard_coef_thresholded: 0.8751 - accuracy: 0.0844 - auc: 0.9692 - val_loss: 0.4871 - val_binary_crossentropy: 0.4112 - val_jaccard_coef_loss: 3.6997 - val_jaccard_coef: 0.5104 - val_jaccard_coef_thresholded: 0.5141 - val_accuracy: 0.0414 - val_auc: 0.9091\n",
      "Epoch 98/500\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1197 - binary_crossentropy: 0.2260 - jaccard_coef_loss: 0.5492 - jaccard_coef: 0.8795 - jaccard_coef_thresholded: 0.8804 - accuracy: 0.0823 - auc: 0.9717"
     ]
    }
   ],
   "source": [
    "# # Train the model and save the history\n",
    "# history = model.fit(train, \n",
    "#                     epochs=NUM_EPOCHS,\n",
    "#                     validation_data=valid,\n",
    "#                     callbacks=callbacks_list,\n",
    "#                     verbose=1)\n",
    "\n",
    "# Train the model and save the history. Generator version\n",
    "history = model.fit(train,\n",
    "                    validation_data=valid,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=callbacks_list,\n",
    "                    epochs=NUM_EPOCHS, \n",
    "                    verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "plt.plot(time_callback.times, label=f'Total training time: {sum(time_callback.times):.2f} secs')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title(f'Training time for each epoch for {model_name}')\n",
    "plt.legend()  # This displays the legend\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(log_dir, 'training_time.png'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plot the training and validation metric and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title(f'Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(log_dir, 'loss.png'))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "# plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig(os.path.join(log_dir, 'acc.png'))\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "acc = history.history['auc']\n",
    "val_acc = history.history['val_auc']\n",
    "plt.plot(epochs, acc, 'y', label='Training auc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation auc')\n",
    "plt.title('Training and validation AUC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC score')\n",
    "plt.legend()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(log_dir, 'auc.png'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# auc = history.history['iou']\n",
    "# val_auc = history.history['val_iou']\n",
    "\n",
    "# plt.plot(epochs, auc, 'y', label='Training IOU')\n",
    "# plt.plot(epochs, val_auc, 'r', label='Validation IOU')\n",
    "# plt.title('Training and validation Intersection Over Union')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('IOU')\n",
    "# plt.legend()\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig(os.path.join(log_dir, 'score.png'))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 256, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape\n",
    "# np.expand_dims(y_test, -1)\n",
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_image_names = sorted(os.listdir(Path(sparcs_test_dir, \"images_p/\")))\n",
    "\n",
    "# # Define number of steps\n",
    "# steps = math.ceil(len(test_image_names) / batch_size)\n",
    "\n",
    "# y_pred=model.predict(test, steps=steps) # NOTE: Using the test generator you need to reset it after avery predict\n",
    "# y_pred_thresholded = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 113ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "y_pred_thresholded = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Score (IoU):  0.6539238330164421\n"
     ]
    }
   ],
   "source": [
    "# Cast to float types\n",
    "y_true_f = y_test.astype('float32')\n",
    "y_pred_f = y_pred_thresholded.astype('float32')\n",
    "\n",
    "y_true_flatten = y_true_f.reshape(-1)\n",
    "y_pred_flatten = y_pred_f.reshape(-1)\n",
    "\n",
    "# Jaccard Score\n",
    "jaccard = jaccard_score(y_true_flatten, y_pred_flatten)\n",
    "print(\"Jaccard Score (IoU): \", jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = y_test.flatten()\n",
    "y_score = y_pred.flatten()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(log_dir, 'roc.png'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(Path(saved_models_path, model_name))\n",
    "\n",
    "# Save training history\n",
    "with open(Path(train_hystory_path, f'{model_name}.json'), 'w') as f:\n",
    "    json.dump(history.history, f)\n",
    "\n",
    "# model.save('rsnet_adam_100.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metrics = {'jaccard_coef': jaccard_coef, 'jaccard_coef_thresholded':jaccard_coef_thresholded, 'jaccard_coef_loss':jaccard_coef_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(Path(saved_models_path, model_name))\n",
    "# new_model = tf.keras.models.load_model(Path(saved_models_path, model_name), custom_objects=custom_metrics)\n",
    "\n",
    "# Load training history\n",
    "with open(Path(train_hystory_path, f'{model_name}.json'), 'r') as f:\n",
    "    loaded_history = json.load(f)\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = new_model\n",
    "history = loaded_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = history['auc']\n",
    "val_auc = history['val_auc']\n",
    "\n",
    "plt.plot(epochs, auc, 'y', label='Training AUC')\n",
    "plt.plot(epochs, val_auc, 'r', label='Validation AUC')\n",
    "plt.title('Training and validation AUC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load weights / Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 08:44:45.711794: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-28 08:44:46.187560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22309 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:81:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(latest)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Re-evaluate the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m loss, metric \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestored model, IOU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:3160\u001b[0m, in \u001b[0;36mModel._assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_compile_was_called\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3155\u001b[0m   \u001b[38;5;66;03m# Checks whether `compile` has been called. If it has been called,\u001b[39;00m\n\u001b[1;32m   3156\u001b[0m   \u001b[38;5;66;03m# then the optimizer is set. This is different from whether the\u001b[39;00m\n\u001b[1;32m   3157\u001b[0m   \u001b[38;5;66;03m# model is compiled\u001b[39;00m\n\u001b[1;32m   3158\u001b[0m   \u001b[38;5;66;03m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[39;00m\n\u001b[1;32m   3159\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_compiled:\n\u001b[0;32m-> 3160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must compile your model before \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3161\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining/testing. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3162\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse `model.compile(optimizer, loss)`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "ckpt_path = str(checkpoint_path / \"UNet_SPARCS_epochs300_batch16\" / \"cp-{epoch:04d}.ckpt\")\n",
    "ckpt_dir = os.path.dirname(ckpt_path)\n",
    "\n",
    "# Upload the last checkpoint\n",
    "latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "\n",
    "# Create a new model instance\n",
    "model = create_model(model_name='unet')\n",
    "\n",
    "# Load the previously saved weights\n",
    "model.load_weights(latest)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, metric = model.evaluate(test, verbose=2)\n",
    "print(f\"Restored model, IOU: {metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the last checkpoint\n",
    "latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "\n",
    "# Create a new model instance\n",
    "model = get_model(model_name='unet')\n",
    "\n",
    "# Load the previously saved weights\n",
    "model.load_weights(latest)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, metric = model.evaluate(test, verbose=2)\n",
    "print(f\"Restored model, AUC: {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_sparcs():\n",
    "    # Create two instances of the test generator\n",
    "    test_gen_pred = get_SPARCS_generator('test', batch_size=batch_size, only_rgb=train_with_RGB)\n",
    "\n",
    "    test_image_names = sorted(os.listdir(Path(sparcs_test_dir, \"images_p/\")))\n",
    "\n",
    "    # Define number of steps\n",
    "    steps = math.ceil(len(test_image_names) / batch_size)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred_sparcs = model.predict(test_gen_pred, steps=steps)\n",
    "\n",
    "    # Collect true labels\n",
    "    y_true_sparcs = get_SPARCS('test', only_rgb=train_with_RGB)[1]\n",
    "\n",
    "    print(y_true_sparcs.shape, y_pred_sparcs.shape)\n",
    "\n",
    "    return y_true_sparcs, y_pred_sparcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true, y_pred = load_sparcs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.unique(y_true), np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = iou(y_true, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def resize_image(image, new_dim=(1024, 1024)):\n",
    "#     return cv2.resize(image, new_dim, interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "def resize_image(image, new_dim=(1024, 1024, 3)):\n",
    "    resized_image = resize(image, new_dim, mode='edge', preserve_range=True, anti_aliasing=False, anti_aliasing_sigma=None, order=0)\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = resize_image(images, new_dim=((8, 1024, 1024, 3)))\n",
    "masks = resize_image(masks, new_dim=((8, 1024, 1024, 1)))\n",
    "\n",
    "images.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = resize_image(get_SPARCS('test', full_image=True, binary=True)[0], new_dim=((8, 1024, 1024, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X)\n",
    "preds = (preds > 0.5).astype(np.uint8)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(images), 3, figsize=(10, 25))\n",
    "\n",
    "for i, im in enumerate(images):\n",
    "    # inputs, targets = test_list[idx]\n",
    "    ax[i, 0].imshow(im)\n",
    "    ax[i, 0].set_title('RGB Image')\n",
    "    ax[i, 1].imshow(masks[i], cmap='gray')\n",
    "    ax[i, 1].set_title('True mask')\n",
    "    ax[i, 2].imshow(preds[i], cmap='gray')\n",
    "    ax[i, 2].set_title('Predicted mask')\n",
    "    \n",
    "# Add a title for the entire plot\n",
    "plt.suptitle('Cloud Detection Results on SPARCS')\n",
    "\n",
    "# Adjust the spacing between subplots and from the top of the figure\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# # Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(images), 2, figsize=(10, 30))\n",
    "\n",
    "for i, im in enumerate(images):\n",
    "    # Remove singleton dimensions from preds[i]\n",
    "    mask = np.squeeze(preds[i])\n",
    "    \n",
    "    # Create an RGB mask where the masked region is light blue\n",
    "    rgb_mask = np.zeros((*mask.shape, 3))\n",
    "    rgb_mask[mask == 1] = [0, 0, 1]  # Light blue color for the mask\n",
    "    \n",
    "    ax[i, 0].imshow(im)\n",
    "    ax[i, 0].set_title('RGB Image')\n",
    "    \n",
    "    ax[i, 1].imshow(im)\n",
    "    ax[i, 1].imshow(rgb_mask, alpha=0.3)  # Adjust alpha for desired transparency\n",
    "    ax[i, 1].set_title('RGB Image with mask')\n",
    "    \n",
    "# Add a title for the entire plot\n",
    "plt.suptitle('Cloud masking on SPARCS')\n",
    "\n",
    "# Adjust the spacing between subplots and from the top of the figure\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S2_images, S2_masks = get_S2('test', only_rgb=True)\n",
    "X = get_S2('test')[0]\n",
    "y_pred_S2 = model.predict(X)\n",
    "y_pred_binary_S2 = (y_pred_S2 > 0.1).astype(np.uint8)\n",
    "y_pred_binary_S2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed_value)\n",
    "idxs = np.random.randint(len(S2_images), size=10)\n",
    "\n",
    "fig, ax = plt.subplots(len(idxs), 3, figsize=(10, 20))\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    ax[i, 0].imshow(S2_images[idx])\n",
    "    ax[i, 0].set_title('RGB Image')\n",
    "    ax[i, 1].imshow(S2_masks[idx], cmap='gray')\n",
    "    ax[i, 1].set_title('True mask')\n",
    "    ax[i, 2].imshow(y_pred_binary_S2[idx], cmap='gray')\n",
    "    ax[i, 2].set_title('Predicted mask')\n",
    "\n",
    "# Add a title for the entire plot\n",
    "plt.suptitle('Cloud Detection Results on Sentinel-2')\n",
    "\n",
    "# Adjust the spacing between subplots and from the top of the figure\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# # Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(idxs), 2, figsize=(10, 30))\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    # Remove singleton dimensions from preds[i]\n",
    "    mask = np.squeeze(y_pred_binary_S2[idx])\n",
    "    \n",
    "    # Create an RGB mask where the masked region is light blue\n",
    "    rgb_mask = np.zeros((*mask.shape, 3))\n",
    "    rgb_mask[mask == 1] = [0, 0, 1]  # Light blue color for the mask\n",
    "    \n",
    "    ax[i, 0].imshow(S2_images[idx])\n",
    "    ax[i, 0].set_title('RGB Image')\n",
    "    \n",
    "    ax[i, 1].imshow(S2_images[idx])\n",
    "    ax[i, 1].imshow(rgb_mask, alpha=0.3)  # Adjust alpha for desired transparency\n",
    "    ax[i, 1].set_title('RGB Image with mask')\n",
    "    \n",
    "# Add a title for the entire plot\n",
    "plt.suptitle('Cloud masking on Sentinel-2')\n",
    "\n",
    "# Adjust the spacing between subplots and from the top of the figure\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = tiff.imread('../Data/L8_Biome8/test/masks/LC81490122013218LGN00.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biome_images, biome_masks = get_biome8('test', only_rgb=True, full_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biome_images.shape, biome_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 32\n",
    "total_images = len(os.listdir(biome_test_dir / \"images/\"))  # assuming your images are in this directory\n",
    "\n",
    "# Calculate the number of steps\n",
    "steps = math.ceil(total_images / batch_size)\n",
    "\n",
    "# Create the generator\n",
    "biome_generator = get_biome8_generator('test', batch_size=batch_size, full_image=True)\n",
    "\n",
    "# Use the predict method directly on the generator\n",
    "biome_preds = model.predict(biome_generator, steps=steps)\n",
    "\n",
    "# Apply threshold\n",
    "biome_preds = (biome_preds > 0.7).astype(np.uint8)\n",
    "\n",
    "print(biome_preds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(biome_images), 3, figsize=(10, 40))\n",
    "\n",
    "for i, im in enumerate(biome_images):\n",
    "    ax[i, 0].imshow(im)\n",
    "    ax[i, 0].set_title('RGB Image')\n",
    "    ax[i, 1].imshow(biome_masks[i], cmap='gray')\n",
    "    ax[i, 1].set_title('True mask')\n",
    "    ax[i, 2].imshow(biome_preds[i], cmap='gray')\n",
    "    ax[i, 2].set_title('Predicted mask')\n",
    "    \n",
    "# Add a title for the entire plot\n",
    "plt.suptitle('Cloud Detection Results on Biome')\n",
    "\n",
    "# Adjust the spacing between subplots and from the top of the figure\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# # Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(biome_images), 2, figsize=(10, 30))\n",
    "\n",
    "for i, im in enumerate(biome_images):\n",
    "    # Remove singleton dimensions from preds[i]\n",
    "    mask = np.squeeze(biome_preds[i])\n",
    "    \n",
    "    # Create an RGB mask where the masked region is light blue\n",
    "    rgb_mask = np.zeros((*mask.shape, 3))\n",
    "    rgb_mask[mask == 1] = [0, 0, 1]  # Light blue color for the mask\n",
    "    \n",
    "    ax[i, 0].imshow(im)\n",
    "    ax[i, 0].set_title('RGB Image')\n",
    "    \n",
    "    ax[i, 1].imshow(im)\n",
    "    ax[i, 1].imshow(rgb_mask, alpha=0.3)  # Adjust alpha for desired transparency\n",
    "    ax[i, 1].set_title('RGB Image with mask')\n",
    "    \n",
    "# Add a title for the entire plot\n",
    "plt.suptitle('Cloud masking on Biome')\n",
    "\n",
    "# Adjust the spacing between subplots and from the top of the figure\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
